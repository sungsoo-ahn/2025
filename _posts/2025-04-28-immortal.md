---
layout: distill
title: Making Models Immortal - How the Resilience of the Turritopsis Dohrnii Jellyfish Can Inspire Continuous Learning in AI
description: Turritopsis dohrnii, or the immortal jellyfish, can revert back to an earlier life stage so that it can renew itself and survive through challenging conditions. This inspires the question, can AI systems be designed with similar resilience? Imagine a model that rather than growing outdated could continuously adapt, regenerate, and retain knowledge without costly retraining.
date: 2025-04-28
future: true
htmlwidgets: true
hidden: false

# Anonymize when submitting
authors:
  - name: Anonymous

# authors:
#   - name: Almodels Einstein
#     url: "https://en.wikipedia.org/wiki/Almodels_Einstein"
#     affiliations:
#       name: IAS, Princeton


# must be the exact same name as your blogpost
bibliography: 2025-04-28-immortal.bib  

# Add a table of contents to your post.
#   - make sure that TOC names match the actual section names
#     for hyperlinks within the post to work correctly. 
#   - please use this format rather than manually creating a markdown table of contents.
toc:
  - name: Background
  - name: External Memory 
  - name: Adaptive Learning Rate
  - name: Episodic Memory
  - name: Elastic Weight Consolidation (EWC)
  - name: Progressive Neural Architecture Search (PNAS)
  - name: Multi-Model Ensemble and Gating
  - name: Closing Remarks

# Below is an example of injecting additional post-specific styles.
# This is used in the 'Layouts' section of this post.
# If you use this post as a template, delete this _styles block.
_styles: >
  .fake-img {
    background: #bbb;
    border: 1px solid rgba(0, 0, 0, 0.1);
    box-shadow: 0 0px 4px rgba(0, 0, 0, 0.1);
    margin-bottom: 12px;
  }
  .fake-img p {
    font-family: monospace;
    color: white;
    text-align: left;
    margin: 12px 0;
    text-align: center;
    font-size: 16px;
  }
---

## Background

The "immortal jellyfish" or *Turritopsis dohrnii* is unique because it can revert to an earlier stage of its life cycle at times of stress, allowing it to restart its life indefinitely <d-cite key="Turritop">. 

This regenerative process inspired the "immortal model," which is a type of machine learning approach, designed to learn constantly, adapt, and preserve knowledge across evolving tasks. By incorporating components that mirror the jellyfish's life-cycle reversion, it can even undergo adaptation without "forgetting" important information-a characteristic that metaphorically captures the very spirit of the jellyfish's resilience.


## External Memory 

*Inspiration*: The jellyfish "rejuvenates" by regenerating its entire cellular structure. Similarly, the **External Memory** provides the model with a persistent, accessible memory, allowing it to "recall" and leverage previous experiences during new learning processes. This memory, represented as a trainable matrix $$ \mathbf{M} $$, enables the model to store and retrieve information in a way that stabilizes learning over time.

The memory mechanism enables retention of long-term knowledge, essential for lifelong learning. Given an input query $$ \mathbf{q} $$, the memory produces a weighted output that acts as "retrieved" knowledge:

$$ \mathbf{r} = \text{softmax}(\mathbf{q} \cdot \mathbf{M}^T) \cdot \mathbf{M} $$

This helps the model incorporate historical knowledge, similar to how the immortal jellyfish retains core life cycle knowledge as it "resets."

{% highlight python %}
import torch
import torch.nn as nn

class ExternalMemory(nn.Module):
    def __init__(self, memory_size, memory_vector_dim):
        super().__init__()
        self.memory_size = memory_size
        self.memory_vector_dim = memory_vector_dim
        self.memory = nn.Parameter(torch.randn(memory_size, memory_vector_dim))

    def forward(self, query):
        attention = torch.matmul(query, self.memory.t())
        attention_weights = torch.softmax(attention, dim=-1)
        read_memory = torch.matmul(attention_weights, self.memory)
        return read_memory
{% endhighlight %}

## Adaptive Learning Rate

*Inspiration*: The jellyfish adjusts to environmental stressors, gradually regenerating at a molecular level. In parallel, the **Adaptive Learning Rate Optimizer** modifies the model's learning rate, gradually slowing the rate of new learning to protect past knowledge Zhong et al.<d-cite key="Adaptive"/>.

By adjusting the learning rate for each parameter dynamically, the optimizer ensures that new learning does not overwrite essential knowledge. For each gradient $$ g $$, the learning rate $$ \alpha $$ adapts as:

$$ \alpha_{\text{new}} = \frac{\alpha}{1 + 0.1 \times \text{step}} $$

This continuous adjustment process parallels the jellyfish’s slow and controlled regenerative process, ensuring stability in evolving environments.

{% highlight python %}
import torch
import torch.nn as nn
import torch.optim as optim

def __init__(self, parameters, lr):
    self.opt = optim.Adam(parameters, lr=lr)
    self.lr = lr

def step(self):
    for group in self.opt.param_groups:
        for p in group['params']:
            if p.grad is None:
                continue
            state = self.opt.state[p]
            if 'step' not in state:
                state['step'] = 0
            state['step'] += 1
            step = state['step']
            group['lr'] = self.lr / (1 + 0.1 * step)
    self.opt.step()

def zero_grad(self):
    self.opt.zero_grad()
{% endhighlight %}

## Episodic Memory

*Inspiration*: Just as the jellyfish stores core information in cellular structures to "reboot," **Episodic Memory** Das et al.<d-cite key="das2024larimarlargelanguagemodels"> stores representative samples from past experiences. The model "rehearses" these samples to reinforce learning, thus retaining information essential to its core functionality.

The episodic memory buffer retains select examples from past tasks. By periodically revisiting this stored data, the model stabilizes its performance on old tasks, reducing the risk of catastrophic forgetting.

{% highlight python %}
import torch
import torch.nn as nn
import torch.optim as optim
import random

class EpisodicMemory:
    def __init__(self, capacity):
        self.capacity = capacity
        self.memory = []

    def add_example(self, example):
        if len(self.memory) >= self.capacity:
            self.memory.pop(0)
        self.memory.append(example)

    def get_batch(self, batch_size):
        return random.sample(self.memory, min(batch_size, len(self.memory)))
{% endhighlight %}

## Elastic Weight Consolidation (EWC)

*Inspiration*: The jellyfish conserves essential cellular structures during reversion. **Elastic Weight Consolidation (EWC)** Kirkpatrick et al.<d-cite key="Kirkpatrick_2017">, likewise, helps the model "consolidate" key weights, protecting parameters critical to past knowledge.

EWC regularizes parameter updates, penalizing changes to weights critical for previous tasks. The EWC loss term $$ L_{\text{EWC}} $$ for each parameter $$ \theta_i $$ balances old and new learning:

$$ L_{\text{EWC}} = \sum_i \frac{\lambda}{2} F_i (\theta_i - \theta_i^{*})^2 $$

This penalty helps the model conserve critical knowledge, akin to the jellyfish’s protection of essential cellular information.

{% highlight python %}
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F

class EWC:
    def __init__(self, model, dataloader, importance=1000):
        self.model = model.module  # Access the original model
        self.dataloader = dataloader
        self.importance = importance
        self.params = {n: p for n, p in self.model.named_parameters() if p.requires_grad}
        self._means = {}
        self._precision_matrices = self._diag_fisher()

    def _diag_fisher(self):
        precision_matrices = {}
        for n, p in self.params.items():
            precision_matrices[n] = p.clone().detach().fill_(0)

        self.model.eval()
        for batch in tqdm(self.dataloader, desc='Calculating Fisher Information', leave=False):
            self.model.zero_grad()
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            labels = batch['labels'].to(device)
            output = self.model(input_ids, attention_mask)
            loss = F.cross_entropy(output, labels)
            loss.backward()

            for n, p in self.model.named_parameters():
                precision_matrices[n].data += p.grad.data ** 2 / len(self.dataloader)

        precision_matrices = {n: p for n, p in precision_matrices.items()}
        return precision_matrices
{% endhighlight %}

## Progressive Neural Architecture Search (PNAS)

*Inspiration*: Just as the jellyfish’s life cycle flexibility allows it to "regrow" for new environments, **Progressive Neural Architecture Search (PNAS)** Rusu et al.<d-cite key="rusu2022progressiveneuralnetworks"> enables the model to adjust its structure dynamically, adding layers based on task complexity.

PNAS dynamically constructs new layers for the model in response to complex tasks. Using a controller (an LSTM), PNAS explores various architecture configurations, adding layers based on task-specific needs. The decision process relies on action probabilities $$ p $$, determining whether to add or exclude a layer:

$$ p = \text{sigmoid}(\text{LSTM}(x)) $$

This adaptive flexibility parallels the jellyfish's life cycle reversion, optimizing model structure as required by new tasks.

{% highlight python %}
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F

class PNASController(nn.Module):
    def __init__(self, input_size, hidden_size, num_layers):
        super().__init__()
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers)
        self.fc = nn.Linear(hidden_size, 1)

    def forward(self, x, hidden=None):
        output, hidden = self.lstm(x, hidden)
        scores = self.fc(output)
        return scores

class PNAS:
    def __init__(self, model, controller, max_layers=5):
        self.model = model
        self.controller = controller
        self.max_layers = max_layers

    def search_step(self):
        actions = []
        log_probs = []
        for _ in range(self.max_layers):
            action, hidden = self.controller(torch.randn(1, 1, self.controller.lstm.input_size))
            action_prob = torch.sigmoid(action)
            m = torch.distributions.Bernoulli(action_prob)
            action = m.sample()
            log_prob = m.log_prob(action)
            actions.append(action.item())
            log_probs.append(log_prob)

        return actions, torch.stack(log_probs).sum()

    def update_model(self, actions):
        new_layers = []
        for action in actions:
            if action == 1:
                new_layers.append(nn.Linear(768, 768))
                new_layers.append(nn.ReLU())
        self.model.classifier = nn.Sequential(
            *new_layers,
            nn.Linear(768, self.model.num_labels)
        )
{% endhighlight %}

## Multi-Model Ensemble and Gating

*Inspiration*: The jellyfish operates through a synergy of its biological systems to adapt. The **Multi-Model Ensemble** mirrors this by combining multiple specialized "experts," dynamically activating them to maximize adaptability to diverse inputs Chen et al.<d-cite key="chen2022understandingmixtureexpertsdeep">.

In a multi-expert ensemble, each model (expert) specializes in certain tasks. A **Gating Network** dynamically selects the most relevant expert(s) based on the input:

$$ \text{gate_outputs} = \text{softmax}(\mathbf{W} \mathbf{x}) $$

The ensemble’s adaptability mimics the jellyfish's coordinated regenerative processes, allowing the model to leverage specialized knowledge for diverse tasks.

{% highlight python %}
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F

class GatingNetwork(nn.Module):
    def __init__(self, input_size, num_experts):
        super().__init__()
        self.fc = nn.Linear(input_size, num_experts)

    def forward(self, x):
        return F.softmax(self.fc(x), dim=-1)

class MultiModelEnsemble(nn.Module):
    def __init__(self, num_experts, num_labels):
        super().__init__()
        self.experts = nn.ModuleList([ImmortalDistilBERT(num_labels) for _ in range(num_experts)])
        self.gate = GatingNetwork(768, num_experts)

    def forward(self, input_ids, attention_mask):
        expert_outputs = [expert(input_ids, attention_mask) for expert in self.experts]
        expert_outputs = torch.stack(expert_outputs)

        cls_token_output = self.experts[0].distilbert(input_ids, attention_mask).last_hidden_state[:, 0]
        gate_outputs = self.gate(cls_token_output)

        gate_outputs = gate_outputs.unsqueeze(1)
        expert_outputs = expert_outputs.permute(1, 0, 2)

        weighted_outputs = (gate_outputs * expert_outputs).sum(dim=1)

        return weighted_outputs
{% endhighlight %}

## Closing Remarks

The immortal jellyfish rejuvenates itself to survive environmental challenges. The "immortal model" mimics this resilience by combining external memory, adaptive learning rates, episodic memory, EWC, PNAS, and multi-model ensembles. Together, these features enable the model to continuously learn, adapt, and retain knowledge across dynamic tasks. This adaptability is vital for real-world applications where models must persistently learn and retain crucial knowledge without starting from scratch, embodying the enduring, regenerative spirit of the immortal jellyfish.
