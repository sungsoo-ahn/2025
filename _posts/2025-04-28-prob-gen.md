---
layout: distill
title: Rethinking Probabilistic Protein Generation
description: Current protein generation models, while powerful, are constrained by evolutionary biases and struggle to explore beyond naturally occurring protein sequences. We propose a next-generation diffusion framework that incorporates bias-aware sampling and structural guidance to expand protein design beyond the evolutionary morphospace while maintaining structural validity. Our preliminary results demonstrate that this approach can triple the evolutionary distance from known proteins while preserving comparable structural metrics, suggesting a promising path toward discovering novel, functionally viable proteins beyond nature's repertoire.
date: 2025-04-28
future: true
htmlwidgets: true
hidden: false

# Anonymize when submitting
authors:
  - name: Anonymous

# authors:
#   - name: Arthur Liang
#     url: "https://artliang.dev/"
#     affiliations:
#       name: MIT

# must be the exact same name as your blogpost
bibliography: 2025-04-28-prob-gen.bib  

# Add a table of contents to your post.
#   - make sure that TOC names match the actual section names
#     for hyperlinks within the post to work correctly. 
#   - please use this format rather than manually creating a markdown table of contents.
toc:
  - name: Probabilistic Generative Models for Protein Generation
  - name: Balancing Exploration and Exploitation with Diffusion Models
  - name: Beyond Diffusion and Next-Generation Approaches
  - name: Conclusion

# Below is an example of injecting additional post-specific styles.
# This is used in the 'Layouts' section of this post.
# If you use this post as a template, delete this _styles block.
_styles: >
  .fake-img {
    background: #bbb;
    border: 1px solid rgba(0, 0, 0, 0.1);
    box-shadow: 0 0px 4px rgba(0, 0, 0, 0.1);
    margin-bottom: 12px;
  }
  .fake-img p {
    font-family: monospace;
    color: white;
    text-align: left;
    margin: 12px 0;
    text-align: center;
    font-size: 16px;
  }
---

## Probabilistic Generative Models for Protein Generation

Proteins are the molecular engines of life, orchestrating countless biological processes from catalyzing reactions to maintaining cellular structure. The challenge of designing new proteins for specific applications—whether novel enzymes or therapeutic molecules—requires navigating an astronomical sequence space. Even a modest protein of 100 amino acids presents $$ 20^{100} $$ possible sequences <d-cite key="nihNatureProtein"></d-cite>, yet natural evolution has explored only a tiny fraction of this space, constrained by fitness landscapes and biophysical rules <d-cite key="nihDarwinianEvolution"></d-cite>.

Probabilistic generative models have emerged as powerful tools for protein design <d-cite key="natureHighlyAccurate"></d-cite><d-cite key="nihEvolutionbasedModel"></d-cite>, learning from natural proteins to capture the statistical patterns underlying sequence-function relationships. Additionally, protein generation is a design problem that requires optimizing a sequence-to-function mapping: a high-dimensional, non-linear problem.

As such, the appeal of probabilistic generation lies in its dual ability to balance:
* **Exploration**: Generating diverse candidate sequences.
* **Exploitation**: Incorporating constraints to ensure biological relevance.

Probabilistic generation aligns well with the inherent structure and constraints of proteins, making it an ideal approach for their design:
1. It mirrors the dual requirements of stability and adaptability in proteins
2. It provides data-efficient learning that inherently incorporates uncertainty <d-cite key="natureUnifiedRational"></d-cite>
3. It enables high-throughput exploration through constrained random walks through the sequence space <d-cite key="riesselman2017deepgenerativemodelsgenetic"></d-cite>

Exploring and exploiting different models within this framework is important because they have a lot of downstream potential in fields like drug discovery, synthetic biology, and enzyme design as well as informing model architecture for tasks in other domains.

## Balancing Exploration and Exploitation with Diffusion Models

The success of diffusion models in particular stems from their ability to mimic evolutionary optimization <d-cite key="NEURIPS2020_4c5bcfec"></d-cite>.

Diffusion models for protein generation consist of a **forward process** (adding noise) and a **reverse process** (denoising). Noise is added to data $x_0$ over $T$ timesteps:

$$
q(x_t \mid x_0) = \mathcal{N}(x_t; \sqrt{\bar{\alpha}_t} x_0, (1 - \bar{\alpha}_t) \mathbf{I})
$$

where $$\bar{\alpha}_t = \prod_{s=1}^t \alpha_s$$ controls the noise schedule. Then, the reverse process predicts the original data by removing noise step-by-step:

$$
p_\theta(x_{t-1} \mid x_t) = \mathcal{N}(x_{t-1}; \mu_\theta(x_t, t), \Sigma_\theta(x_t, t))
$$

The reverse diffusion process parallels how evolution refines random variations into functional sequences through iterative selection. This can be viewed as a Bayesian approach where the prior is a fitness landscape derived from evolutionary data and the posterior the viable sequences conditioned on this prior. 

Gradual refinement in the form of diffusion aligns with how proteins evolve through mutations and selection pressures and affords the computational flexibility to encode constraints (eg. amino acid frequency, structural motifs) for guided generation.

There are of course still a number of limitations including inaccuracies in modeling long range residue dependencies and how it isn't immediately clear that sequence generation by itself links to 3D structural constraints.

To tackle these concerns, latent diffusion models (LDM) and structure diffusion models (SDM) have been introduced. 
* For example, LatentDiff, by compressing representations into a low-dimension latent space, reduces the modeling space and relies on the hierarchical nature of proteins for faster generation. By operating in a continuous latent space rather than discrete amino acid sequences, LDMs naturally handle the discrete-to-continuous challenge while capturing higher-level protein organization. This enables smoother interpolation between protein families and more efficient exploration of the design space.

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.html path="assets/img/2025-04-28-prob-gen/latentdiff.png" class="img-fluid" %}
    </div>
</div>
<div class="caption">
    Pipeline of LatentDiff by Fu et al. depicting their latent diffusion framework where protein structures are encoded into latent representations via the encoder that are then perturbed into noise. Then during generation, using the learned denoising network, protein representations in the latent space are regenerated before decoding. <d-cite key="fu2023latentdiffusionmodelprotein"></d-cite>
</div>

* RFDiffusion is able to control over specific structural topolgies and allow better capturing of geometric constraints of protein folds. This is analogous to learning the spontaneous folding process of proteins from non-functional/denatured states to one that is biologically plausible and functional while respecting physical constraints the whole time. 
* Similarly, TopoDiff incorporates an additional encoder module designed to learn the fixed-size latent topology representation from the training data similar to the VAE architecture. This allows for a more abstract view of generation at the topological level beyond just sequence with a structure-centric latent space optimized for designing at the domain level.

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.html path="assets/img/2025-04-28-prob-gen/rfdiff.png" class="img-fluid" %}
    </div>
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.html path="assets/img/2025-04-28-prob-gen/topodiff.png" class="img-fluid" %}
    </div>
</div>
<div class="caption">
    Examples of well-known structural motifs that arise during RFDiffusion generation by Watson et al. <d-cite key="Watson2023-eb"></d-cite>; t-SNE visualization of the latent space in TopoDiff by Zhang et al. demonstrating primitive substructure interpretability in LDM after including a global-structure encoding module <d-cite key="Zhang2023.12.13.571602"></d-cite>
</div>

However, current approaches face a more fundamental limitation: they are bound by what we call the "evolutionary morphospace"—the limited subset of sequence space that natural evolution through billions of years of selection has explored. This constraint manifests in two critical ways:
1. Dataset Bias: Training on evolutionary datasets inevitably encodes natural proteins' biases <d-cite key="rao2019evaluatingproteintransferlearning"></d-cite>
2. Limited Exploration: Models struggle to generate sequences that deviate significantly from known evolutionary patterns <d-cite key="elnaggar2021prottranscrackinglanguagelifes"></d-cite> <d-cite key="louis"></d-cite>

As such, current probabilistic methods, including diffusion, excel in sampling from nature’s limited morphospace but fail at generalizing or escaping its biases.

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.html path="assets/img/2025-04-28-prob-gen/enhanced-morphospace.png" class="img-fluid" %}
    </div>
</div>
<div class="caption">
    Visualization of the distinction between evolutionary accessible space and the total sequence space. Current diffusion models tend to sample near known functional niches where natural proteins cluster.
</div>

## Beyond Diffusion and Next-Generation Approaches

To transcend these limitations, there are several key directions we can pursue. One direction that has been popular in recent years following the development of multimodal foundation models is jointly modeling sequence and structure so as to incorporate physics and energy-based constraints alongside evolutionary patterns <d-cite key="NEURIPS2019_f3a4ff48"></d-cite>. Additionally, dynamic landscape modeling using reinforcement learning to simulate evolving fitness landscapes positions itself as a promising method for implementing active learning and incorporating experimental feedback in design <d-cite key="WU202118"></d-cite>.

But chiefly of interest in this blog post, the most direct method is synthetic morphospace expansion by augmenting datasets with theoretical protein designs <d-cite key="Anishchenko2020.07.22.211482"></d-cite> and implementing bias-aware training that penalizes over-representation of common motifs. To systematically encourage exploration beyond the evolutionary morphospace while preserving structural validity, we introduce a modified diffusion process towards the latter goal that explicitly balances these competing objectives. Our formulation augments the standard diffusion equation with an exploration term that actively pushes generation away from densely sampled regions:

$$
x_{t-1} = \mu_\theta(x_t, t) + \sigma_t \cdot \epsilon + \alpha(s, d) \cdot \nabla E(x_t)
$$

where:
* $$\mu_\theta(x_t, t)$$ is the standard diffusion mean prediction
* $$\sigma_t$$ is the diffusion variance schedule
* $$\epsilon$$ is random noise
* $$\alpha(s, d)$$ is an adaptive weight based on $$s$$, the structural validity score (TM-score), and $$d$$, a measure of evolutionary distance
* $$\nabla E(x_t)$$ is the exploration gradient

The adaptive weight $$\alpha$$ is computed as:

$$
\alpha(s, d) = \lambda \cdot \text{sigmoid}(\beta_s \cdot s - \beta_d \cdot d)
$$

where:
* $$\lambda$$ is the maximum exploration rate
* $$\beta_s, \beta_d$$ are sensitivity hyperparameters for structure and distance

The exploration gradient $$\nabla E(x_t)$$ is a key component that guides sampling away from known protein families. Specifically, it:
* Computes the direction in sequence space that maximizes distance from the closest known protein clusters while maintaining structural feasibility
* Acts as a "repulsive force" from densely sampled regions of the evolutionary morphospace
* Adaptively scales based on the local density of known sequences, pushing harder in heavily explored regions and more gently in sparse areas

This gradient works in concert with the adaptive weight $$\alpha(s, d)$$ to ensure that exploration beyond the evolutionary morphospace doesn't compromise structural integrity. When structural scores are high, $$α$$ allows for more aggressive exploration; when they begin to deteriorate, it automatically reduces the influence of the exploration gradient.

```python
def next_gen_diffusion_step(x_t, t):
    # Standard diffusion prediction
    eps_θ = model(x_t, t)
    
    # Compute structural validity and evolutionary distance
    struct_score = compute_tm_score(x_t)
    evo_dist = compute_evolutionary_distance(x_t)
    
    # Balance exploration vs validity
    α = adaptive_weight(struct_score, evo_dist)
    exploration_term = gradient_away_from_known_proteins(x_t)
    
    # Modified update combining diffusion and exploration
    x_t_prev = diffusion_update(x_t, eps_θ, t) + α * exploration_term
    
    return project_to_valid_structure(x_t_prev)
```

<div class="caption">
    Pseudocode for Next-Gen Diffusion Step
</div>

As a proof of concept, we conducted an initial comparative study of protein sequence generation using 100 samples each from standard diffusion and our next-generation approach. 

The standard diffusion model, trained on PDB sequences, demonstrated high structural validity but conservative exploration. Samples achieved strong structural metrics (TM-score: 0.85, pLDDT: 90.0) but remained close to known proteins, with an average evolutionary distance of 0.25. Our next-generation approach, incorporating bias-aware sampling and structural guidance, maintained comparable structural quality (TM-score: 0.82, pLDDT: 87.0) while achieving significantly greater novelty, with evolutionary distances averaging 0.75.

These structural metrics capture complementary aspects of protein quality: TM-score measures global fold similarity, while pLDDT indicates confidence in local structural predictions. The minimal decrease in these metrics (TM-score: 0.85→0.82, pLDDT: 90.0→87.0) suggests our method preserves structural integrity even in unexplored regions. For context, TM-scores above 0.8 and pLDDT above 85 are considered highly reliable, indicating our generated proteins maintain biologically plausible conformations.

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.html path="assets/img/2025-04-28-prob-gen/metric-comparison-natural.png" class="img-fluid" %}
    </div>
</div>
<div class="caption">
    In a simple experiment swapping in the proposed next-gen diffusion step, we achieve broader exploration and samples that are more distant from known protein families. However, we observe a trade-off between structural validity and novelty.
</div>

This broader exploration yielded 28 unique structural motifs compared to 12 in the standard approach. The emergence of novel motifs is particularly encouraging, as these structural elements often correspond to new functional capabilities. Rather than generating random variations, our approach appears to discover meaningful new structural arrangements with potential functional significance.

The success of our method stems from its dynamic weighting scheme (α), which actively balances structural constraints with true exploration. As shown in the distribution plot, this pushes sampling toward novel sequences while maintaining viable protein structures.

While these preliminary results are promising, several key analyses and ablations remain. Interesting future lines of work include:
* Rigorously quantify evolutionary distance using sequence-based metrics (eg. BLOSUM matrix scores), structure-based comparisons (eg. RMSD to nearest neighbors), and phylogenetic analysis
* Validate the functional viability of novel motifs through in silico prediction of binding sites and catalytic potential
* Analyze the diversity of generated sequences across different protein families to ensure broad applicability
* Investigate the relationship between evolutionary distance and various structural quality metrics to better understand the exploration-stability trade-off

These analyses will help establish the robustness of our approach and guide further refinements to the sampling strategy.

## Conclusion

The protein sequence space is vast, yet evolution has explored only a constrained morphospace due to functional and stability constraints. Current probabilistic models, trained on evolutionary datasets, inherit these biases, creating a paradox: they excel at generating proteins similar to known ones but struggle to venture beyond evolution's footprints. While this conservatism ensures stability, it also limits our ability to discover truly novel proteins that could revolutionize medicine, materials science, and biotechnology.

Our preliminary experiments suggest a promising path forward. By modeling and then systematically relaxing evolutionary constraints while maintaining physical viability, we can begin to explore the "dark matter" of protein space. Just as cartographers of old combined known landmarks with theoretical predictions to chart unexplored territories, our next-generation models must balance evolutionary wisdom with computational exploration. We propose that next-generation models should aim to explicitly disentangle evolutionary biases while enabling exploration of sparsely populated or hypothetical regions of protein design space. 

This vision requires several key advances:
* Adaptive sampling strategies that dynamically balance structural stability with novelty
* Physics-informed constraints that replace evolutionary biases
* Multi-scale validation combining in silico prediction with targeted experimental feedback

The future of protein design lies not in simply mimicking evolution, but in understanding and then transcending its limitations. By developing models that can venture beyond the evolutionary morphospace while maintaining physical realizability, we open the door to an entirely new universe of proteins with properties and functions never seen in nature.

