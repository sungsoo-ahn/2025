---
layout: distill
title: MixNAR
description: A blogpost about neural algorithmic reasoning, mixture of experts architectures and how trying to implement the first MoE reasoner gave insights about one of the most difficult algorithms to emulate.
date: 2025-04-28
future: true
htmlwidgets: true
hidden: false

# Anonymize when submitting
# authors:
#   - name: Anonymous

authors:
  - name: Anonymous

#  - name: Albert Einstein
#    url: "https://en.wikipedia.org/wiki/Albert_Einstein"
#    affiliations:
#      name: IAS, Princeton
#  - name: Boris Podolsky
#    url: "https://en.wikipedia.org/wiki/Boris_Podolsky"
#    affiliations:
#      name: IAS, Princeton
#  - name: Nathan Rosen
#    url: "https://en.wikipedia.org/wiki/Nathan_Rosen"
#    affiliations:
#      name: IAS, Princeton

# must be the exact same name as your blogpost
bibliography: 2025-04-28-modnar.bib  

# Add a table of contents to your post.
#   - make sure that TOC names match the actual section names
#     for hyperlinks within the post to work correctly. 
#   - please use this format rather than manually creating a markdown table of contents.
toc:
  - name: Before we start
  - name: NAR-101
    subsections:
    - name: Alignment
    - name: The CLRS-30 benchmark
    - name: Current SOTAs
  - name: Knuth-Morris-Pratt
    subsections:
    - name: Encountering it
    - name: Fighting KMP with the MoE sword
  - name: How much damage does the MoE sword do?
    subsections:
    - name: Reality check
    - name: So what are you waiting for?
  - name: Is the boss finally dead? (Outro)

# styles block
_styles: >
  .takeaway {
    border: 1px solid #333; /* Dark grey border */
    border-radius: 12px; /* Rounded corners */
    padding: 15px;
    background-color: #f5f5f5; /* Light background to contrast with the border */
    font-style: italic;
    font-weight: bold;
    font-size: 1.1em
  }
  .takeaway h2 {
    margin-top: 0;
  }




---

## Before we start

To save the reader their time, I will begin my writings by making clear one
important thing. _This is a blogpost and it will read as such._ Therefore, it
will not follow a traditional paper structure.<d-footnote> Starts with
introduction complemented with 3+ contributions, followed by related work,
methodology, experiments, conclusions.</d-footnote> Instead, I decided to lay
out my train of thought in a way that would be interesting from the perspective
of an NAR expert, but also from the perspective of an early career researcher
who would like to glimpse how a (self-proclaimed) NAR researcher thinks like.

However, I do understand that some might be newcomers to the topic. Therefore,
I would like to start this blogpost with...

# NAR-101

Consider how a programmer may solve a problem: They
would check what instruments they have in their toolkit -- books such
as _Introduction to Algorithms_ <d-cite key="clrs"></d-cite> (a.k.a. CLRS) or _Algorithms_ <d-cite
key="sedgewick2011algorithms"></d-cite>, choose the most suitable out of the
set and start matching the problem to the tool. Manually performing the last
part may involve rethinking the problem at hand, consulting their teammates or
_performing various forms of witchcraft_, such as plugging random pieces of
data or scalar functions of them as the input to the algorithm and seeing what
happens.  It is conjectured that this gap between theoretical algorithms and
their real-life execution largely stems from the _scalar bottleneck_ --
having to represent multitude of factors with a single scalar. <d-cite
key="narblueprint"></d-cite>

**N**eural **a**lgorithmic **r**easoning (**NAR**) focuses on training graph neural
networks (**GNNs**) to execute algorithms in a vectorial latent space<d-footnote>i.e.,
node features are 64-dimensional vectors</d-footnote> so we can overcome the
scalar bottleneck. When deploying an algorithm of interest to a problem we can
leave it to gradient descent instead of the programmer to find an appropriate
mapping. <d-cite key="deac2020xlvin"></d-cite><d-cite
key="tang2020towards"></d-cite><d-cite
key="he2022continuous"></d-cite><d-cite
key="mathys2023flood"></d-cite><d-cite
key="numeroso2023dar"></d-cite><d-cite key="panagiotaki2024naricp"></d-cite><d-cite key="estermann2024puzzles"></d-cite>

## Alignment 

Even from the earliest papers on NAR <d-cite
key="velickovic2020neural"></d-cite> it became obvious that training a _robust
neural reasoner, that can give correct outputs for inputs of any (even larger
ones than trained on) sizes_ is not an easy task. It requires _architectural
alignment_. Giving a precise definition is a lengthy task for a blogpost, but
some intuition never hurts:


{% include figure.html path="assets/img/2025-04-28-modnar/alignment.png" class="alignment" %}
<div class="caption">
    Figure source: Veličković <d-cite key="velickovic2023nargradient"></d-cite>
</div>

<blockquote>
    Neural networks extrapolate well if the algorithm can be separated into subfunctions and each of them is easily learnable by a corresponding neural submodule.
  
<br>
    - The core idea of Xu et al. <d-cite key="xu2020howneural"></d-cite>
</blockquote>

Or in other words, GNNs extrapolate very easily on the Bellman-Ford algorithm,
because:
- the intermediate algorithm values correspond to the latent features of the
  nodes.
- the multi-layer perceptron (**MLP**) that computes the messages can easily
  learn linear functions like addition.
- we can choose the GNN neighbourhood aggregation to match the operator
  ($\min$) of the algorithm.

This is just a simplified example and aligning neural models with other
algorithmic properties is an active (and tough) area of research. As I will
strive to keep the 101 just as much as you would need to understand **this**
blogpost, I refer the enthusiastic reader to another discussion <d-cite
key="velickovic2023nargradient"></d-cite> for more details and papers on NAR
alignment.

## The CLRS-30 Benchmark

The CLRS-30 benchmark <d-cite key="velivckovic2022clrs"></d-cite> includes more
than just 30 instances from the _Introduction to Algorithms_ textbook. It
provides a unifying framework for representing algorithmic problems to the
GNNs. Getting familiar with the API/pipeline requires some time to master, so
if you are looking for where to start from, [this EEML 2024 tutorial][eeml2024]
is a good first resource.

[eeml2024]: https://github.com/eemlcommunity/PracticalSessions2024/blob/main/1_reasoning/Reasoning_tutorial_solution.ipynb

However, I believe having a good grasp of the CLRS-30 lingo is sufficient (but also necessary) for
understanding **this** blogpost. Luckily being on the same page does not
require us studying all the codebase. We can get to the required level of
knowledge, again, through an example.


To start with, at a given timestep, all information about an algorithm, such as variables, is
stored as a set of `(Stage, Location, Datatype)` triplets.

{% highlight python linenos %}

class Stage:
  INPUT = 'input'
  OUTPUT = 'output'
  HINT = 'hint'

class Location:
  NODE = 'node'
  EDGE = 'edge'
  GRAPH = 'graph'

class Type:
  SCALAR = 'scalar'
  CATEGORICAL = 'categorical' # for variables with categorical featues
  MASK = 'mask' # binary features
  MASK_ONE = 'mask_one' # same as MASK, just one element is allowed to be 1
  # Make no mistake -- mask_one is still a binary, not a categorical feature
  POINTER = 'pointer'
  # ... some types omitted for brevity ...

{% endhighlight %}

`Location` (notion for node/edge/graph features), as well as the `Stage.INPUT`
and `Stage.OUTPUT` are self-explanatory. `Stage.HINT` is CLRS-specific and it
allows for modelling the trajectory of the algorithm -- at any timestep $\tau$
the NAR would take as input hints computed from the previous step $\tau-1$ and
learn to predict what the hints are for step $\tau$. `Type` dictates how
features should be processed, including what losses should we use for training.
Of all types, the only non-trivial one is `Type.POINTER`. E.g. the node pointer
for node $i$ behaves like an edge (yes, it is not a typo) from $i$ to any other
node $j$, with the restriction that there could be only **one** pointer outgoing from
node $i$.<d-footnote>Some NAR development frameworks, such as the one I used
here, put an even harder restriction that a pointer must be an edge in the
graph. This is usually done to maintain low memory usage -- $O(E)$ instead of
$O(V^2)$. For this project, however, $E=V^2$, so we did not get the benefits...
but I have a preference for PyTorch over JAX</d-footnote> When (re)encoded as
an input/hint node pointers become edge features.

How all the data for an algorithm looks like is described via specifications,
like the one below, corresponding to the Bellman-Ford algorithm.<d-footnote>If
the names of the variables are too cryptic for your taste, a pro advice from me
is to check the "Introduction to Algorithms". The authors of the CLRS benchmark
usually use the same variable names.</d-footnote>

{% highlight python linenos %}

SPECS = types.MappingProxyType({
    # ... many algorithms omitted for brevity ...
    'bellman_ford': {
        # just a tie-breaker, in case two solutions are identical
        'pos': (Stage.INPUT, Location.NODE, Type.SCALAR),
        # starting node
        's': (Stage.INPUT, Location.NODE, Type.MASK_ONE),
        # weight matrix (NxN)
        'A': (Stage.INPUT, Location.EDGE, Type.SCALAR),
        # adjacency matrix (NxN)
        'adj': (Stage.INPUT, Location.EDGE, Type.MASK),
        # predecessors array pi[i] holds the id of the
        # tip of the pointer from i
        # NOTE: it's an array of size N holding integers
        'pi': (Stage.OUTPUT, Location.NODE, Type.POINTER),
        # same as above, but for each timestep of algorithm's execution;
        # the default starting pi array consists of self-loops (pi[i]=i) 
        'pi_h': (Stage.HINT, Location.NODE, Type.POINTER),
        # shortest distances at a given timestep
        'd': (Stage.HINT, Location.NODE, Type.SCALAR),
        # reachable nodes at a given timestep
        'msk': (Stage.HINT, Location.NODE, Type.MASK),
    },
    # ... many algorithms omitted for brevity ...
})

{% endhighlight %}

Construction of encoders/decoders/losses/accuracies can be then automated based
on the specification of the algorithm, saving us from reimplementing 30
different classes for each algorithm. To be able to train a network, the only
thing left is a data sampling process and a concrete algorithm implementation.

## Current SOTAs

As you can see from the year of publication, this benchmark has been around for
a while. A lot of research has went into creating models that improve on
previous ones and so on and so forth. As a result, only for all but three (out
of thirty) algorithms no one has reported out-of-distribution (**OOD**) accuracy above 80%. <d-cite
key="xu2024recurrent"></d-cite> The most challenging of the trio and the one
I tried to tackle is...


# Knuth-Morris-Pratt

## Encountering it
{% include figure.html path="assets/img/2025-04-28-modnar/darksouls2.png"  class="img-fluid" %}

I will call it **KMP** for short. As implemented currently in CLRS-30,
the algorithm spans a bit less than 170 lines of code, dwarfing the 20 lines of
pseudocode of _Introduction to Algorithms_ (which I presume the reader is
familiar with). Yes, a lot of it is boilerplate, but I believe visualising
CLRS-30's implementation is worth the time.


The specification:
{% highlight python linenos %}
SPECS = types.MappingProxyType({
    # ... many algorithms omitted for brevity ...
    'kmp_matcher': {
        'string': (Stage.INPUT, Location.NODE, Type.MASK),
        'pos': (Stage.INPUT, Location.NODE, Type.SCALAR),
        'key': (Stage.INPUT, Location.NODE, Type.CATEGORICAL),
        'match': (Stage.OUTPUT, Location.NODE, Type.MASK_ONE),
        'pred_h': (Stage.HINT, Location.NODE, Type.POINTER),
        'pi': (Stage.HINT, Location.NODE, Type.POINTER),
        'is_reset': (Stage.HINT, Location.NODE, Type.MASK),
        'k': (Stage.HINT, Location.NODE, Type.MASK_ONE),
        'k_reset': (Stage.HINT, Location.GRAPH, Type.MASK),
        'q': (Stage.HINT, Location.NODE, Type.MASK_ONE),
        'q_reset': (Stage.HINT, Location.GRAPH, Type.MASK),
        's': (Stage.HINT, Location.NODE, Type.MASK_ONE),
        'i': (Stage.HINT, Location.NODE, Type.MASK_ONE),
        'phase': (Stage.HINT, Location.GRAPH, Type.MASK),
    }
    # ... many algorithms omitted for brevity ...
})
{% endhighlight %}
consist of a number of inputs and hints, which start in the following configuration (given to the model):


<div class="l-page">
{% include figure.html path="assets/img/2025-04-28-modnar/kmp_0.svg"  class="img-fluid" %}
<div class="caption">
    Step 0 (Phase 0) - initialisation of phase 0 and the algorithm
</div>
</div>

I will use green arrows for `MASK_ONE` indices like `i`, `s`, `k`, `q` and
purple for the output `match`.  The haystack nodes (gray squares) have
different colour from the needle (blue squares) -- this was the binary feature
of the `string` variable in the specification.  Pointers will be arrows
_between_ squares (black for `pred_h`; colour for `pi`). Note that a CLRS-30
convention/trick is to use `pred_h` for input only as it never changes. Part of
the pointers for `pi` (the ones on the haystack) are dotted, to emphasise the
fact, that they never change. The `is_reset` node mask<d-footnote>It helps
model the prefix function.  If this feature is set for the $i$-th position of
$needle$, then for the substring $needle_{:i}$ no suffix matches
a prefix</d-footnote> will be a little tickmark in bottom right corner if the
feature is 1 and nothing otherwise.  The `reset` graph masks are on the right
of the diagram. I will not visualise `phase` as it starts from 0 and changes
only once, but will keep it in the caption instead. Bear in mind that the model
will need to predict the phase as well.

You might notice there is no adjacency matrix here. We will assume the graph is
complete (everyone is connected to everyone). Hence, message passing occurs
between all pairs of nodes for this algorithm. 

KMP is a two phase algorithm. The aim of the first phase (Phase 0) of the
algorithm is to build the prefix function pointers `pi`. Here is how CLRS-30
execution state of every step of the first half of the algorithm.



<div class="l-page">
{% include figure.html path="assets/img/2025-04-28-modnar/kmp_1.svg"  class="img-fluid" %}
<div class="caption">
    Step 1 (Phase 0)
</div>
</div>

<div class="l-page">
{% include figure.html path="assets/img/2025-04-28-modnar/kmp_2.svg"  class="img-fluid" %}
<div class="caption">
    Step 2 (Phase 0)
</div>
</div>

<div class="l-page">
{% include figure.html path="assets/img/2025-04-28-modnar/kmp_3.svg"  class="img-fluid" %}
<div class="caption">
    Step 3 (Phase 0)
</div>
</div>

<div class="l-page">
{% include figure.html path="assets/img/2025-04-28-modnar/kmp_3p5.svg"  class="img-fluid" %}
<div class="caption">
    Step 4 (Phase 0)
</div>
</div>

You might notice several things:
- `k` did not move -- this was the case for this **toy** example and for a bit
  larger inputs (present in the training data) `k` may move.
- only 1 binary feature changed for step 4 -- this was again the case for this example.
- some of the `pi` pointers (the one on the haystack) are never touched -- this
  is a design choice of CLRS-30.<d-footnote>Otherwise one would need to figure out how to
  define the pointer array only for half of the nodes, which can make the code
  even messier.</d-footnote> Figuring out which pointers are subject to change
  is left to the GNN processor of the NAR.
- $i$ and $s$ did not move -- again this by design, they will in the second
  phase (Phase 1).

The next phase aims to use the built prefix function, i.e. the pointers `pi`,
in order to find the **start** position of the needle in the haystack. To
emphasise that in CLRS-30, this is not a different algorithm, I continue the
step count from 5.


<div class="l-page">
{% include figure.html path="assets/img/2025-04-28-modnar/kmp_5.svg"  class="img-fluid" %}
<div class="caption">
    Step 5 (Phase 1)
</div>
</div>

<div class="l-page">
{% include figure.html path="assets/img/2025-04-28-modnar/kmp_6.svg"  class="img-fluid" %}
<div class="caption">
    Step 6 (Phase 1)
</div>
</div>

<div class="l-page">
{% include figure.html path="assets/img/2025-04-28-modnar/kmp_7.svg"  class="img-fluid" %}
<div class="caption">
    Step 7 (Phase 1)
</div>
</div>

<div class="l-page">
{% include figure.html path="assets/img/2025-04-28-modnar/kmp_8.svg"  class="img-fluid" %}
<div class="caption">
    Step 8 (Phase 1)
</div>
</div>

<div class="l-page">
{% include figure.html path="assets/img/2025-04-28-modnar/kmp_9.svg"  class="img-fluid" %}
<div class="caption">
    Step 9 (Phase 1)
</div>
</div>

<div class="l-page">
{% include figure.html path="assets/img/2025-04-28-modnar/kmp_10.svg"  class="img-fluid" %}
<div class="caption">
    Step 10 (Phase 1)
</div>
</div>

<div class="l-page">
{% include figure.html path="assets/img/2025-04-28-modnar/kmp_11.svg"  class="img-fluid" %}
<div class="caption">
    Step 11 (Phase 1)
</div>
</div>

_Where `s` is, when a match occurs, this is where the `match` index will point to._

Allow me to draw your attention to the following observations:
- `k` again never moves -- again by design (k is never
  used in Phase 1, but we cannot delete variables in NAR).
- `k_reset` never changes -- again by design.
- `pi` pointers never change -- this time they are used as inputs.
- `is_reset` (the tick in the boxes) never changes -- see above.

It should also be fairly obvious that Phase 1 performs a different type of work --
moving indices around, and following previously computed pointers, instead of
changing the pointers. At each step it also uses a different set of variables as input
(e.g. the characters under indices `i` and `q` dictate how the `q` pointer
moves, not the ones under `k` and `q`) and changes a different set of variables
at output (pointers remain untouched). While the lack of persistency in
NAR<d-cite key="jain2024neural"></d-cite> (i.e. recalling information from many
steps ago) could be one possible explanation, I propose an alternative
hypothesis.


<div class="takeaway">
    It is currently not possible to switch GNN behaviour during a NAR execution
    based only on a single global hint feature.
</div>

<!-- Note that the above does not contradict the generalist NAR paper <d-cite -->
<!-- key="ibarz2022generalist"></d-cite>, since (to my best understanding) in the -->
<!-- generalist paper -->

## Fighting KMP with the MoE sword

There are a number of ways to implement the above behaviour for
NAR<d-footnote>Neural Attentive Circuits <d-cite
key="weiss2022neural"></d-cite> were initially under consideration, but they
were too hard to train, getting many vanishing gradients and a training loss
stuck in a local minima as a result</d-footnote>, but
I have found that a simpler solution worked better.  You might have guessed it from
the title, but my idea of achieving this switching and solving KMP
once and for all is mixture of experts (**MoE**).

In the land of LLMs, a MoE model like Mixtral <d-cite
key="jiang2024mixtral"></d-cite>, is one where the output for a given input $x$
is determined by the weighted sum of outputs of some expert networks. Experts
operate independently of each other.

{% include figure.html path="assets/img/2025-04-28-modnar/mixtral.png"  class="img-fluid" %}
<div class="caption">
    Figure source: Jiang et al. <d-cite key="jiang2024mixtral"></d-cite>
</div>

I will make this even more simplified by choosing to have only two GNN experts
(recall we had two phases of the algorithm) + a router. Both of the experts
will perform computations at each timestep (contrasted to the TopK approach of
Mixtral), carrying their own latent state, but the router will decide who to
trust.

Two extra adaptations are necessary, because in NAR we are (usually) dealing
with data on the graph domain where an "input" is a graph with algorithmic
information attached. Those are outlined below.

### Routing variables to experts

<div class="l-page">
{% include figure.html path="assets/img/2025-04-28-modnar/mod_encoding.svg"  class="img-fluid" %}
<div class="caption">
    Two experts choosing what inputs to attend to. "Var" represents either an input or a hint.
</div>
</div>

In the following paragraphs I will refer to something that is an input or
a hint with the collective term **variable**.

Deciding which input or hint to route to which expert is based on the latent
graph embedding of each expert, as shown in the above image. For each expert,
once a graph embedding is obtained, a linear projection with a sigmoid
activation gives the attention coefficient in $(0,1)$ representing the
importance of a variable to the expert. The graph embedding I used here is
based on PrediNet <d-cite key="shanahan2020anexplicitly"></d-cite>, since mean
pooling will provably not generalise <d-cite key="adam-day2024almost"></d-cite>
and max pooling gives sparse gradients.


In CLRS-30 embeddings are obtained by encoding all variables in latent space
and then summing their embeddings (node variables are **not** added to edge
variables, be careful). In our case, we will modify this process, by weighing each
variable by its respective coefficient before adding them together. Each expert
will also have its own encoders, as some variables may have different
meaning at different stages (e.g. `q` or `is_reset`)


### Combining experts' opinions

<div class="l-page-outset">
{% include figure.html path="assets/img/2025-04-28-modnar/mod_merge2.svg"  class="img-fluid" %}
<div class="caption">
    Combining the experts is based on the router (top; blue) embeddings
</div>
</div>

After the encodings take place, processing step follows. The router (top row in
the image above; blue features) and its experts (two bottom rows) operate
independently. Each expert has a different set of GNN weights. After the
message passing step, the router, based on the latent graph embedding, decides
how much each of the experts' latent states will contribute (modelling
a categorical distribution with `torch.nn.softmax`). The weighted sum of the
experts' latents is **added**<d-footnote>This creates a residual connection. If
the router's next latent space is set to the weighted sum of the two experts,
training gradients easily vanish (backpropagation through time+saturating
softmax).</d-footnote> to the router's latent embeddings (akin to Mixtral) and
the processing step is complete.<d-footnote>I have experimented with a setup
where the router also performs its own GNN message passing step, but found that
it gave no substantial difference</d-footnote>

### Decoding/Training/Processor is left unchanged

Just as the subtitle states, after the processing step, the decoding proceeds
as usual. The training also proceeds as with other NAR works, trying to
optimise prediction of the output as well as the hints. As the Generalist paper<d-cite key="ibarz2022generalist"></d-cite> suggests, no teacher forcing is
employed.

For a processor, I employed the gated MPNN (contrasted to the TripletMPNN) as
it is more cost-effective: when I ran a `gmpnn` processor on the official code
provided for CLRS-30 it was not much worse than a `triplet_gmpnn`, (10% vs 20%)
but it was faster. That would be useful, as I will be running two copies of it
for the MoE approach.

# How much damage does the MoE sword do?

Time to see how well the idea performs. I trained it 10K training samples of
sizes up to 20 tokens (needle+haystack) for 100 epochs taking the model with
lowest validation loss for testing. As standard, the test datasets are
out-of-distribution (80 tokens for needle+haystack). The first one is 100 samples, generated using the
CLRS-30 generators. The second one is the one that can be downloaded from
CLRS-30, however it is only available for the original KMP implementation.


## Reality check

First of all, the situation with KMP, albeit bad, it is not that terrible.
A number of methods, including the Generalist, which has been around for
a while, score about 20% accuracy. Therefore, I would take results below and
around 20-30% as "meh".

For the first result, I report the accuracy with both test datasets. The MoE
approach will be called **MixNAR**. **NARsup** is same as NAR but the latent
dimension is scaled up (hence sup) so that the parameter count matches MixNAR.

| Dataset | NAR | NARsup | MixNAR |
| :------------- |-------------:| -----:| -----:|
| self-generated | 33% | 30% | **77%**
| downloaded | 7% |   1% | **76%**

_That is anything but a "meh" result!_ Clearly, the same improvement in accuracy
cannot be achieved by simply increasing the latent state dimensionality. And
although the "official" downloaded dataset is potentially harder, MixNAR
mispredicted only 1 test instance more out of the 100.

For my own sanity, I decided to check two more seeds of MixNAR.<d-footnote>
Given that baseline approaches here are trivial and tested many times by other
researchers, and given no other papers reported high results, I spared myself
doing 4 more seeds for NAR and NARsup.</d-footnote> The average accuracy for
the 3 runs was $\approx$ **90%$\pm$10%** for both the self-generated and
downloaded test data.

## So what are you waiting for?

Perhaps, you are now wondering why I have not tested it on the remaining 29
algorithms and published the result in the next top-tier conference. The short
version is:

{% include figure.html path="assets/img/2025-04-28-modnar/endmeme.png"  class="img-fluid" %}

### Speed overhead

To start with, I do not have many GPUs lying around unused. Additionally, the
code is not currently engineered to parallelise the experts as done in
Mixtral, so I incur about 50-100% speed overhead. The training time of MixNAR
is about 12 hours (6 for NAR) and the test time is 32s (20s for
NAR).<d-footnote>Times were measured on: A100 for training; a mobile laptop
3080 for inference.</d-footnote> Combine this with the fact the parameter count
*at least* doubles, and the legendary reject-leaning Reviewer #2 is guaranteed.

### Poor hint prediction

In the table below I have used the worst performing (according to `match`
accuracy) seed for MixNAR. The other seeds mispredict the hints too, but the
table is already wide for $\pm$-s.

| Approach | `i` | `s` | `q` | `pi` | `phase` |
| :------------- |--------:| -----:| -----:| -----:| -----:|
| NAR | 84.97% | <span style="color:#EA7125">30.38%</span> |  86.88% | <span style="color:#EA7125">99.92%</span> | 99.53%
| MixNAR | <span style="color:#D6083B">25.04%</span> | 34.38% | <span style="color:#D6083B">31.82%</span> | 95.42% | 99.97%

Results for hints are first averaged across the time dimension and then
averaged across samples -- `phase` can still be high, even if the transition
step is mispredicted. The test dataset is still the official one, however
I have omitted `k`-s accuracy, as it is above 95% for both.

To save you the scrolling up, `i`, `s` and `q` were the key indices in the
second phase, which decide where the match happens. The MixNAR clearly solves
the matching problem, but for the second phase it surely does not adhere to the
KMP algorithm. While NAR models that give great results but do not use hints is not
unheard of <d-cite key="bevilacqua2023neural"></d-cite>, there is another, more
interesting observation. **The standard approach almost perfectly predicts the prefix function pointers `pi` and has reasonably high accuracy on the other hints, except `s`.**

### Unnecessary?

As a (self-proclaimed) NAR researcher, I immediately asked myself the question
-- "Where does it break?". A common way to answer this question in our domain
is to hardcode stuff, retrain and see what happens. The test data for those
"new" mini algorithms are of course self-generated and not downloaded from the
officially published data. While the generator was still CLRS-30, your mileage
may vary.

I started the investigation by only focusing on Phase 1, providing the correct
`pi`. The `match` accuracy was increased to around 60% (suggesting that
slightest imperfections in `pi` can affect final accuracy), but `s` was still
around 50% and in the ground truth algorithm `match` is decided on `s`.

So, can NAR just move those two indices in the context of KMP? The following
algorithm was designed to check it:

{% highlight python linenos %}

def move_i_s(T: _Array, P: _Array) -> _Out:

  chex.assert_rank([T, P], 1)
  probes = probing.initialize(SPECS['move_i_s'])

  T_pos = np.arange(T.shape[0])
  P_pos = np.arange(P.shape[0])


  pi = np.arange(P.shape[0])
  is_reset = np.zeros(P.shape[0])

  k = 0
  k_reset = 1
  is_reset[0] = 1

  # Cover the edge case where |P| = 1, and the first half is not executed.
  delta = 1 if P.shape[0] > 1 else 0


  probing.push(
      probes,
      specs.Stage.INPUT,
      next_probe={
          'pred_h_i': probing.strings_pred(T_pos, P_pos),
          'string':
              probing.strings_id(T_pos, P_pos),
          'pos':
              probing.strings_pos(T_pos, P_pos),
          'key':
              probing.array_cat(
                  np.concatenate([np.copy(T), np.copy(P)]), _ALPHABET_SIZE),
      })
  safe_s = 0
  s = 0
  probing.push(
      probes,
      specs.Stage.HINT,
      next_probe={
          's_h': probing.mask_one(s, T.shape[0] + P.shape[0]),
          'i_h': probing.mask_one(0, T.shape[0] + P.shape[0]),
      })
  for i in range(T.shape[0]):
    if i >= P.shape[0]:
      s += 1
    probing.push(
        probes,
        specs.Stage.HINT,
        next_probe={
            's_h': probing.mask_one(s, T.shape[0] + P.shape[0]),
            'i_h': probing.mask_one(i, T.shape[0] + P.shape[0]),
        })
  probing.push(
      probes,
      specs.Stage.OUTPUT,
      next_probe={
          's': probing.mask_one(s, T.shape[0] + P.shape[0]),
          'i': probing.mask_one(i, T.shape[0] + P.shape[0]),
      })

  # By convention, set probe to head of needle if no match is found
  probing.finalize(probes)

  return T.shape[0], probes

{% endhighlight %}

As simple as that, no prefix functions to compute or pointers to follow, just
start moving `s`, when `i` has done `len(P)` steps.

You guessed it correctly, it did not work! And that should not be surprising,
because for good OOD generalisation (e.g. one to lengths much greater than 80)
the neural network has to **count** the number of characters in the haystack.
Counting is not an easy task for neural networks.

So, what if we remove the `s` altogether and use the `i` pointer, which is easy
to predict, as it moves along the `pred_h` edges. Fundamentally, the KMP will not
be much different, regardless if we predict where the needle starts (before) or
we predict where it ends (now). The `match` accuracy, however, skyrocketed to
**82% for a simple gated MPNN**.

# Is the boss finally dead? (Outro)

Part of me wants to claim the trophy of defeating KMP. Another, the more scientific and pedagogical part, however, wants to summarise the key takeaways from this (already long) blogpost:

1. Studying the algorithm's implementation is a great way to develop intuition
   and devise new architectures.
1. Modularity is one way to get to high accuracy for KMP, but is not strictly
   necessary.
1. Hint OOD accuracy, combined with hardcoding/simplifying parts of the
   algorithm can tell us a lot about which parts NAR struggles with.
1. KMP accuracy can easily exceed 80% with a slightly altered definition.

I will leave it to the authors of CLRS-30 to decide if minimally changing the
definition is "legal" and to the reviewers to decide if mixture of NAR experts is a cool
idea.
