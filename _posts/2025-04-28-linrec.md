---
layout: distill
title: Linear Recursions for Everyone
description: Investigating linear RNNs such as Mamba, can be challenging because they are currently not efficiently expressible in PyTorch. We propose the abstraction of linear recursions to gain intuition for the computational structure of these emerging deep learning architectures. After deriving their parallel algorithm, we gradually build towards a simple template CUDA extension for PyTorch. We hope that making linear recursions accessible to a wider audience inspires further research on linear-time sequence mixing.
date: 2025-04-28
future: true
htmlwidgets: true
hidden: false

# Anonymize when submitting
authors:
  - name: Anonymous

# must be the exact same name as your blogpost
bibliography: 2025-04-28-linrec.bib  

# Add a table of contents to your post.
#   - make sure that TOC names match the actual section names
#     for hyperlinks within the post to work correctly. 
#   - please use this format rather than manually creating a markdown table of contents.
toc:
  - name: Introduction
    subsections:
    - name: Motivation
    - name: Mamba with a Linear Recursion
  - name: Part I - Algorithm
    subsections:
    - name: Parallel Calculation
    - name: Gradient Calculation
  - name: Part II - Implementation
    subsections:
    - name: Preparing a CUDA Extension for PyTorch
    - name: Reference Implementation Scan
    - name: Tile Implementation
    - name: Pipe Implementation
    - name: Reverse and Backward Implementation
    - name: Testing and Benchmarking
  - name: Discussion

styles: >
  .callout-block {
      width: 85%;
      margin: 25px auto;           
      padding: 15px 30px;          
      background-color: #1f2937;
      color: #f3f4f6;
      border-radius: 8px;    
  }
  .note {
    margin-bottom: 10px; 
    margin-top: 10px; 
    overflow: hidden; 
    color: #31708f; 
    background-color: #d9edf7; 
    border-color: #bce8f1; 
    padding: 15px; border: 1px solid transparent; 
    border-radius: 4px;
  }
  .blocktip {
    background: #bbb;
    border-left: 2px solid var(--global-theme-color);
    margin: 1.5em 10px;
    padding: 0.5em 10px;
    font-size: 1.1rem;
  }
  .block-tip {
      border-color: var(--global-tip-block);
      background-color: var(--global-tip-block-bg);
      p {
        color: var(--global-tip-block-text);
      }
      h1,
      h2,
      h3,
      h4,
      h5,
      h6 {
        color: var(--global-tip-block-title);
      }
    }
  .fake-img {
    background: #bbb;
    border: 1px solid rgba(0, 0, 0, 0.1);
    box-shadow: 0 0px 4px rgba(0, 0, 0, 0.1);
    margin-bottom: 12px;
  }
  .fake-img p {
    font-family: monospace;
    color: white;
    text-align: left;
    margin: 12px 0;
    text-align: center;
    font-size: 16px;
  }
---

## Introduction
### Motivation

With the publication of Mamba<d-cite key="gu2024mamba"></d-cite>, the parallel scan algorithm<d-cite key="ladner1980parallelprefix"></d-cite><d-cite key="blelloch1990prefixapplication"></d-cite> received once again the attention of the Deep Learning community<d-footnote> e.g. Tweets by <a href="https://x.com/Algomancer/status/1740171408284782966">Adam Hibble</a>, <a href="https://x.com/francoisfleuret/status/1788489112283996367">Francois Fleuret</a> or <a href="https://github.com/pytorch/pytorch/issues/95408#issuecomment-2327232046">PyTorch Issues</a></d-footnote>. 
This algorithm allows to parallelize the inherently sequential operation of a scan if its update function is associative, for example, to compute cumulative sums and products, or linear recurrent neural network (RNN) layers<d-cite key="martin2018parallelizing"></d-cite>. In particular, it is a fundamental building block of an emerging class of architectures inspired by state space models (SSMs) such as S4<d-cite key="gu2024mamba"></d-cite>, S5<d-cite key="smith2023s5"></d-cite>, LRU<d-cite key="orvieto2023lru"></d-cite>, Hawk <d-cite key="de2024griffin"></d-cite>, xLSTM <d-cite key="beck2024xlstm"></d-cite>, or Mamba<d-cite key="gu2024mamba"></d-cite>. These models show promising results on a wide range of tasks while exhibiting linear runtime complexity $$O(L)$$ in the sequence length $$L$$. Unfortunately, investigating them can be challenging. Because the parallel scan is currently not efficiently expressible in PyTorch, most implementations are hidden in CUDA Kernels, which limits the accessibility of this important research area.

This article aims to convey an intuitive understanding via the abstraction of a linear recursion

$$ 
y_l =  y_{l-1} \cdot c_l + x_l
$$

with inputs $$ x_l $$, coefficients $$ c_l $$, and outputs $$ y_l $$. In contrast to  other great resources<d-cite key="rush2022annotatedS4"></d-cite><d-cite key="chen2024mamba"></d-cite><d-cite key="grootendorst2024mamba"></d-cite><d-cite key="rush2024mamba"></d-cite> explaining the parallel scan and the individual models themselves, we focus on the linear recursion because its computational structure is common across all SSM or diagonal linear RNN-based models. Ignoring the complexities of the general parallel scan allows us to reach a rather simple parallel description of the linear recursion algorithm in the first part of this article.


Coincidentally, this algorithmic description can be mapped very efficiently onto the CUDA architecture<d-cite key="merrill2016cubscan"></d-cite>. So in the second part of the article, we will gradually work towards a simple CUDA implementation in the form of a PyTorch extension. As a side-effect to the educational aspect of the exercise, the code can be used as a template for easy prototyping and research. It allows to express Mamba in terms of 'unfused', primitive operations in PyTorch, much like matrix multiplications can express Flash-Attention<d-footnote> e.g. in Code by <a href="https://github.com/CLAIRE-Labo/flash_attention">Caglar Glucehre</a></d-footnote>. Finally, we show that the runtime of the parallel linear recursion is practically as fast as a binary vector operation (e.g. `torch.add`). 

All the code is available as an anonymous repository [here](https://anonymous.4open.science/r/linrec-2F7F).


### Mamba with a Linear Recursion
Let us first convince ourselves that we can express Mamba, more precisely its sequence mixing layer, with a linear recursion. The code for this section is available in this [Google Colab](https://colab.research.google.com/drive/1Fk4tpzHluKFSLrLdcvWXiZwMEOd_3BtS). To start, we express the linear recursion with a simple loop: 

<details>
<summary>Show code: minimal imports</summary>
{% highlight python %}
import torch
from torch import nn
from einops import rearrange, repeat, einsum
{% endhighlight %}
</details>
{% highlight python %}
# linear recursion y_i = y_{i-1} * c_i + x_i
def linrec(inputs:torch.Tensor, coeffs:torch.Tensor):
    outputs = torch.zeros_like(inputs)
    prev = torch.zeros_like(outputs[..., 0])
    for i in range(0, inputs.shape[-1]):
        outputs[..., i] = prev * coeffs[..., i] + inputs[..., i]
        prev = outputs[..., i].clone()
    return outputs
{% endhighlight %}


 To continue, we dissect [`mamba_simple.py`](https://github.com/state-spaces/mamba/blob/main/mamba_ssm/modules/mamba_simple.py) by the original authors. Since there are a lot of moving parts, we focus on the call to [`selective_scan_fn`](https://github.com/state-spaces/mamba/blob/main/mamba_ssm/ops/selective_scan_interface.py) and how its arguments are prepared. In short, there is an input-independent model parameter `A` and a projection layer `in_proj`, which maps an input `u` to `x` and input-dependent parameters `dt`, `B`, and `C`. Then, the selective scan performs some reparametrizations, expands `x` with `B` into an inner dimension, computes the linear scan of `x` with coefficients `A`, and contracts the result with `C` back to the shape of `x`:


<details>
<summary>Show code: define model parameters</summary>
{% highlight python %}
# model params from
# https://github.com/state-spaces/mamba/blob/main/mamba_ssm/modules/mamba_simple.py
d_model = 1024                          # W: width of u (default: d_model=1024 in mamba1-370m)
expand  = 2                             # expansion from d_state to d_inner
d_inner = expand * d_model              # D: width of x (default: expand=2 => d_inner=2048)
d_state = 16                            # N: width of one SSM-head  (default: d_state=16)
ngroups = 1                             # G: number heads that share B and C projection vectors
assert(d_inner % ngroups == 0)
{% endhighlight %}
</details>


<details>
<summary>Show code: prepare dummy data</summary>
{% highlight python %}
# prepare dummy data according to
# https://github.com/state-spaces/mamba/blob/main/mamba_ssm/modules/mamba_simple.py
device = torch.device('cuda')
dtype = torch.float32
batchsize, seqlen = 1, 2**10

# A is only input independent param
A = torch.rand((d_inner, d_state), device=device, dtype=dtype) * 15 + 1
A = -torch.exp(A.log().float()) # for completeness
in_proj = nn.Linear(d_model, d_inner + d_inner + ngroups*d_state + ngroups*d_state + d_inner, device=device, dtype=dtype)

# prepare input u and input-dependent params
x = torch.randn((batchsize, seqlen, d_model), device=device, dtype=dtype)
_, u, B, C, dt = torch.split(in_proj(x), [d_inner, d_inner, ngroups*d_state, ngroups*d_state, d_inner], dim=-1)
B = rearrange(B, 'B L (G N) -> B G N L', G=ngroups, N=d_state)
C = rearrange(C, 'B L (G N) -> B G N L', G=ngroups, N=d_state)
u = rearrange(u, 'B L D -> B D L')
dt = rearrange(dt, 'B L D -> B D L')
dt = nn.functional.softplus(dt) # map to positive range
{% endhighlight %}
</details>


{% highlight python %}
# selective S6 scan based on linear recursion
def selective_scan_linrec(u:torch.Tensor, dt:torch.Tensor, A:torch.Tensor, B:torch.Tensor, C:torch.Tensor) -> torch.Tensor:
    # prepare A, B, dt (B=batch, D=d_inner, N=d_state, L=seqlen)
    A = repeat(A, 'D N -> B D N L', B=batchsize, L=seqlen)
    B = repeat(B, 'B G N L -> B (G x) N L', x=d_inner//ngroups)
    C = repeat(C, 'B G N L -> B (G x) N L', x=d_inner//ngroups)
    dt = repeat(dt, 'B D L -> B D N L', N=d_state)

    # reparameterize A, B
    A = torch.exp(A * dt)
    B = B * dt

    # expand scalars u with vectors B to vectors h in inner dimension
    h = einsum(B, u, 'B D N L, B D L -> B D N L')

    # compute linear recursion in inner dimension
    h = linrec(inputs=h, coeffs=A)

    # contract vectors h in inner dimension with vectors C to scalars y
    y = einsum(C, h, 'B D N L, B D N L -> B D L')
    return y
{% endhighlight %}


Finally, we test `selective_scan_linrec` by comparing it with two reference implementations:
{% highlight python %}
# selective scan reference implementations
from mamba_ssm.ops.selective_scan_interface import selective_scan_fn, selective_scan_ref
y_linrec = selective_scan_linrec(u, dt, A, B, C)          
y_sol = selective_scan_fn(u=u, delta=dt, A=A, B=B, C=C)   # error: 7.629e-06
y_ref = selective_scan_ref(u=u, delta=dt, A=A, B=B, C=C)  # error: 3.815e-06
{% endhighlight %}

This illustrates how linear recursions are the building block of Mamba, and it can be expanded to many other architectures such as S4<d-cite key="gu2024mamba"></d-cite>, S5<d-cite key="smith2023s5"></d-cite>, LRU<d-cite key="orvieto2023lru"></d-cite>, and even Mamba-2<d-cite key="dao2024mamba2"></d-cite>. In the special case of linear time-invariant (LTI) systems such as S4<d-cite key="gu2024mamba"></d-cite>, the coefficient `coeff` would be shared across sequence length. Note that the reparametrization, as well as the state expansion and contraction, are fused into the linear recursion in practice. This makes the algorithm hardware-aware and drastically increases runtime by reducing memory accesses.


## Part I - Algorithm

In the previous section, we learned how the rather simple PyTorch function `linrec` can express the sequence mixing component of SSMs or linear RNNs such as Mamba. Unfortunately, this formulation is prohibitively slow even for toy problems. In this section, we will establish further intuitions for the linear recursion, how to parallelize it, and how to calculate its gradients. 

Let us begin by defining the linear recursion $$y_l =  y_{l-1} \cdot c_l + x_l$$ of inputs $$x_l$$, coefficients $$c_l$$, and outputs $$y_l$$ starting at $$y_0=x_0$$ and iterating for $$l=0 \ldots L-1$$ steps. By unrolling the recursion, we obtain an equivalent formulation of a weighted sum

$$
y_l 
= \sum_{k=0}^{l} 
\underbrace{(\prod_{i=k+1}^{l} c_i)}_{=\tilde{c}_{k,l}} \cdot x_k
= \sum_{k=0}^{l} \tilde{c}_{k,l} \cdot x_k,
$$

where $$\tilde{c}_{k,l}$$ are cumulative coefficients from $$k$$ to $$l$$ and $$\tilde{c}_{l,l}=1$$. If we consider sequences $$x=[x_{k}]_{k=0}^{L'-1}$$, $$c=[c_{i}]_{i=0}^{L'-1}$$, and $$y=[y_{l}]_{l=0}^{L'-1}$$, we can describe the linear recursion as a linear sequence mixer $$y = f(x,c) = \tilde{C}^T x$$. The mixing matrix $$\tilde{C}^T$$ is lower triangular, where the diagonal contains a sequence of ones, the subdiagonal contains the sequence $$c$$, and each lower diagonal entry at index $$l,k$$ contains the cumulative product $$\tilde{c}_{k,l}$$. In the special case of a single shared coefficient $$c_l=z \in [0,1]$$, the function $$f$$ is an exponential moving average filter. As such, it is an instance of a convolutional operator and therefore $$\tilde{C}$$ a circulant matrix. This allows for parallelization via the fast Fourier transform (FFT), as for example in the original S4 implementation, but it is limited to this special case. Like the FFT, parallel scan is based on a divide-and-conquer approach but it additionally works for time-variant $$c$$ and achieves a sequential runtime complexity of $$O(L)$$ instead of $$O(L \text{log} L)$$.

### Parallel Calculation
We approach the divide-and-conquer algorithm from the bottom up. To compute a linear recursion on two threads, we split the sequences at $$L'$$ into two parts. The first thread simply computes the linear recursion $$[y_{l}]_{l=0}^{L'-1}$$ up to element $$L'$$. To see how the second thread avoids performing the same sequential computation, we decompose $$y_l$$ into two sums 

$$
y_l = 
\underbrace{
\Big(\sum_{k=0}^{L'-1} \tilde{c}_{k,L'-1} \cdot x_k \Big) 
}_{= y_{L'-1}}
\cdot \tilde{c}_{L'-1,l} +
\underbrace{
\sum_{k=L'}^{l} \tilde{c}_{k,l} \cdot x_k
}_{= \bar{y}_{L',l}}
\qquad \text{for}\ l \geq L'.
$$

Note that $$\bar{y}_{L',l}$$ corresponds to a linear recursion starting at $$L'$$ up to $$l$$. This means that the second thread can compute the recursion $$[\bar{y}_{L',l}]_{l=L'}^{L-1}$$ independently and store the cumulative coefficients $$[\tilde{c}_{L'-1,l}]_{l=L'}^{L-1}$$ as a by-product. Finally, the second thread receives $$y_{L'-1}$$ from the first and combines the terms as $$ [y_{l}]_{l=L'}^{L-1} = y_{L'-1} \cdot [\tilde{c}_{L'-1,l}]_{l=L'}^{L-1} + [\bar{y}_{L',l}]_{l=L'}^{L-1}$$ where $$\cdot$$ and $$+$$ act element-wise. In PyTorch pseudo code, this would correspond to:
{% highlight python %}
y[..., :Lp] = linrec(x[..., :Lp], c[..., :Lp]) # thread 1
y[..., Lp:] = linrec(x[..., Lp:], c[..., Lp:]) # thread 2
y[..., Lp:] += y[..., Lp-1] * torch.cumprod(c[..., Lp:], dim=-1) # thread 2
{% endhighlight %}


Now, the attentive reader might have noticed that the second thread still has to perform $$O(L)$$ operations, so what did we gain? In the setting of $$T$$ threads, every thread has to perform $$O(L/T)$$ operations sequentially, then all threads communicate the transition elements with a $$O(\text{log} T)$$ sequential overhead, and finally, the threads combine the result in $$O(L/T)$$. This results in an overall algorithmic complexity of $$O(L/T + \text{log} T)$$ for $$T$$ threads.


### Gradient Calculation

To implement $$f(x,c)$$ in an auto-grad system such as PyTorch, we need to implement a `backward` function which back-propagates the gradient $$\delta^{(y)}:=\frac{\partial \mathcal{L}}{\partial y}^T$$ through the linear recursion and returns $$\delta^{(x)}:= \frac{\partial\mathcal{L}}{\partial x}^T =$$ and $$\delta^{(c)}:= \frac{\partial \mathcal{L}}{\partial c}^T$$. We will now calculate the vector-Jacobian-products $${\delta^{(x)}}^T=\frac{\partial \mathcal{L}}{\partial y} \frac{\partial y}{\partial x}$$ and $${\delta^{(c)}}^T=\frac{\partial \mathcal{L}}{\partial y} \frac{\partial y}{\partial c}$$ as derived from the chain rule:


1. Let us first consider the derivative of an output $y_l$ with respect to an input $$x_k$$
   
   $$\frac{d y_l}{d x_k} = 
     \begin{cases} 
     \tilde{c}_{k,l} &\text{if}\ k \leq l \\
     0 &\text{if}\ l < k
     \end{cases} 
   $$
   
   Inserting the derivative into $$\frac{\partial \mathcal{L}}{\partial y} \frac{\partial y}{\partial x}$$, we again observe the structure of the weighted sum. 
  
   $$
   \delta_k^{(x)}
   = \sum_{l=0}^{L-1} \delta_l^{(y)} \cdot \frac{d y_l}{d x_k}
   = \sum_{l=k}^{L-1} \tilde{c}_{k, l} \cdot \delta_l^{(y)}
   $$

   Rearranging the terms, we unroll $$\delta^{(x)}$$ into a reversed linear recursive form 

   $$
   \delta_k^{(x)} = \delta_{k+1}^{(x)} \cdot c_{k+1} + \delta_k^{(y)}
   $$

2. Let us now consider the derivative of an output $$y_l$$ with respect to a coefficient $$c_i$$. We observe that $$c_i$$ and $$x_k$$ only interact with $$y_l$$ if $$k < i \leq l$$  and therefore 

   $$ 
    \frac{d y_l}{d c_i} = 
    \begin{cases}
    \displaystyle \sum_{k=0}^{i-1} (\prod_{j=k+1}^{i-1} c_j)(\prod_{j=i+1}^{l} c_j) \cdot x_k
    = y_{i-1} \cdot \tilde{c}_{i, l} &\text{if}\ i \leq l \\
    0 &\text{if}\ l < i
    \end{cases}
   $$

   Inserting the derivative into $$\frac{\partial \mathcal{L}}{\partial y} \frac{\partial y}{\partial c}$$, we again observe the structure of the weighted sum, i.e.

   $$
   \delta_i^{(c)}
   = \sum_{l=0}^{L-1} \delta_l^{(y)} \cdot \frac{d y_l}{d c_i}
   = \sum_{l=i}^{L-1}  y_{i-1} \cdot \tilde{c}_{i,l} \cdot \delta_l^{(y)}
   $$

   Rearranging the terms, we express $$\delta^{(c)}$$ as a function of the known $y$ and $$\delta^{(x)}$$

   $$
   \delta_i^{(c)} = y_{i-1} \cdot \delta_{i}^{(x)} 
   $$


This provides a very simple way to write a `backward` function in PyTorch. The only requirements are a shift function and a `forward` function with support for recursion in the reverse direction. 
<details>
<summary>Show code: `shift` and `linrec_ref_fwd` functions</summary>
{% highlight python %}
def linrec_ref_fwd(inputs:torch.Tensor, coeffs:torch.Tensor, reverse=False):
    outputs = torch.zeros_like(inputs)
    prev = torch.zeros_like(outputs[..., 0])
    for i in range(0, inputs.shape[-1])[::-1 if reverse else 1]:
        outputs[..., i] = prev * coeffs[..., i] + inputs[..., i]
        prev = outputs[..., i].clone()
    return outputs

def shift(input, shifts, fillval=0): # torch.roll without the copy of the wrap-around section
    if shifts > 0:
        output = torch.cat([torch.full_like(input[..., :shifts], fillval), input[..., :-shifts]], dim=-1)
    if shifts < 0:
        output = torch.cat([input[..., -shifts:], torch.full_like(input[..., shifts:], fillval)], dim=-1)
    return output
{% endhighlight %}
</details>


{% highlight python %}
def linrec_ref_bwd(d_outputs:torch.Tensor, coeffs:torch.Tensor, outputs:torch.Tensor, reverse=False):
    coeffs = shift(coeffs, -1 if not reverse else 1, fillval=0)
    d_inputs = linrec_ref_fwd(inputs=d_outputs, coeffs=coeffs, reverse=(not reverse))
    d_coeffs =  d_inputs * shift(outputs, shifts=1 if not reverse else -1, fillval=0)
    return d_inputs, d_coeffs
{% endhighlight %}



But wait, in Mamba the `coeffs` are input-dependent parameters! Fortunately, this case is automatically handled by `torch.autograd` via the multi-variable chain rule. In this special case, $$x=z$$ and $$c=g(z)$$ depend on a common input $$z$$, and applying the chain rule yields

$$
\newcommand{\L}{\mathcal{L}} 
{\delta^{(z)}}^T
:= \frac{\partial\L}{\partial z}
= \frac{\partial \L}{\partial y} \frac{\partial y}{\partial z}
= \frac{\partial \L}{\partial y} \Big(
\frac{\partial y}{\partial x} \frac{\partial x}{\partial z} + 
\frac{\partial y}{\partial c} \frac{\partial c}{\partial z}
\Big)
=  {\delta^{(x)}}^T + {\delta^{(c)}}^T \frac{\partial c}{\partial z} .
$$  

The situation is similar for S4 where $$c=z_0$$ depends on a single recurrent coefficient $$z_0$$

$$
{\delta^{(z)}}^T
:= \frac{d \L}{d z_0}
= \frac{\partial \L}{\partial y} \frac{\partial y}{\partial c} \frac{\partial c}{\partial z_0}
= {\delta^{(c)}}^T \frac{\partial c}{\partial z_0}   = \sum \delta_{i}^{(c)}.
$$

PyTorch will derive those cases from the `backward` function of the linear recursion.


## Part II - Implementation

In the previous sections, we learned how to express the backward function `linrec_ref_bwd` in terms of its forward function `linrec_ref_fwd` and we gained some intution into how the latter could be parallelized. But as the reader might be aware, long for-loops in PyTorch still represent a serious barrier to efficient code, particularly if they need to be compiled. Nevertheless, there exist implementations such as [`mamba.py`](https://github.com/alxndrTL/mamba.py)<d-cite key="mambapy"></d-cite> which apply the divide-and-conquer approach to express the scan in PyTorch. This is called a device-wide scan and requires the execution of many independent sub-operations, so-called _kernels_, on a GPU. To avoid this overhead, we would prefer to fuse the entire linear scan into a single kernel. But this is not possible because PyTorch currently does not provide a way to express for-loops which are executed in one kernel. In this section, we learn how to express the linear recursion first as a for-loop and then as a parallel scan in the GPU programming language CUDA.

The goal of this chapter is to familiarize the reader with the basic CUDA programming model in the context of the linear recursion. Many resources such as the [CUDA C++ Programming Guide](https://docs.nvidia.com/cuda/cuda-c-programming-guide), the [CUDA C++ Best Practices Guide](https://docs.nvidia.com/cuda/cuda-c-best-practices-guide), or [the GPU Mode Lectures](https://github.com/gpu-mode) venture very quickly into the intricacies of high-performance optimization for expert users. Furthermore, there exist implementations of a parallel scan<d-cite key="johnryan265pscan"></d-cite><d-cite key="mambapy"></d-cite><d-cite key="acceleratedscan"></d-cite> and a PyTorch higher-order operator (HOP) for the associative scan is [under development](https://github.com/pytorch/pytorch/issues/95408). Here, however, we aim to provide an intuition for the computational structure of the problem and thereby lower the entry bar. The code for Part II is available in this [anonymous repository](https://anonymous.4open.science/r/linrec-2F7F).


### Preparing a CUDA Extension for PyTorch

Although there are a few resources on how to write PyTorch extensions such as the tutorials [Custom C++ and CUDA Extensions](https://pytorch.org/tutorials/advanced/cpp_extension.html) and [Custom C++ and CUDA Operators](https://pytorch.org/tutorials/advanced/cpp_custom_ops.html), the learning curve can be quite steep at times. Therefore, we will aim to provide a rough sketch of what is needed to get started. More in-depth explanations are generally available in the repository. We begin by installing the newest compatible combination of PyTorch, CUDA, and GCC<d-footnote> for more information on compilers see the <a href="https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html#host-compiler-support-policy">CUDA Installation Guide</a></d-footnote>.

{% highlight shell %}
conda create -n CUDA12.4 -c conda-forge gxx==13.2 python==3.12 nvidia::cuda==12.4
conda activate CUDA12.4
pip install numpy pandas matplotlib ninja torch==2.5
{% endhighlight %}

Now, we can write a function using the [C++ frontend of PyTorch](https://pytorch.org/cppdocs/), which might look a bit familiar. Since we want to call our function from within Python, the entry point into the code will not be a classical `main()` function. Instead, we compile the code to a shared library (`.so`) and load it dynamically into the Python runtime. This is conveniently handled by PyTorch and pybind.
{% highlight cpp %}
#include <torch/torch.h>
#include <torch/extension.h>
#include <pybind11/pybind11.h>

using torch::Tensor;
Tensor linrec_ref_fwd(const Tensor &inputs, const Tensor &coeffs, const bool reverse) {
    Tensor outputs = torch::empty_like(inputs);
    // do something
    return outputs;
}

PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {
    m.def("linrec_ref_fwd", wrap_pybind_function(linrec_ref_fwd), 
              "Reference CUDA implementation of linear recursion forward pass.",
              py::arg("inputs"), py::arg("coeffs"), py::arg("reverse")=false);
}
{% endhighlight %}


In `build.py`, we define a build configuration and call `torch.utils.cpp_extension.load`, which in turn invokes `nvcc` and `gcc` and finally loads the pybind module into the Python runtime.

<details>
<summary>Show code: build.py </summary>
{% highlight python %}
import os, shutil
from pathlib import Path
from torch.utils.cpp_extension import load
SRC_DIR = Path(__file__).parent
BUILD_DIR = SRC_DIR / ".build"

### CUDA/C++ Compilation Arguments
EXT_SOURCES = [str(SRC_DIR / "extension.cu")]
INCLUDES = [str(SRC_DIR)] + CUDA_RUNTIME_INCLUDES

CUDA_FLAGS = [
    # Options for specifying behavior of compiler/linker.
    "--generate-line-info",                     # Generate line-number information for device code.
    "-std=c++20",                               # Select a particular C++ dialect
    # Options for passing specific phase options
    # -Xptxas: options for ptxas, the PTX optimizing assembler.
    "-Xptxas", "-warn-spills",                  # Warning if registers are spilled to local memory.
    "-Xptxas", "-warn-lmem-usage",              # Warning if local memory is used.
    # Miscellaneous options for guiding the compiler driver
    "--keep",                                   # Keep all intermediate files that are generated during internal compilation steps.
    # Options for steering GPU code generation.
    "--use_fast_math",                          # Make use of fast math library.
    # Generic tool options.
    "--source-in-ptx",                          # Interleave source in PTX. May only be used in conjunction with --device-debug or --generate-line-info.
    "--resource-usage",                         # Show resource usage such as registers and memory of the GPU code. Implies '--ptxas-options --verbose'.
]
CPP_FLAGS = ["-std=c++20"]

def make_build_dir(clean=False):
    if clean:
        shutil.rmtree(BUILD_DIR, ignore_errors=True)
    os.makedirs(BUILD_DIR, exist_ok=True)
    
def extension(extra_cflags=[], extra_cuda_cflags=[], verbose=False, clean=False):
    make_build_dir(clean=clean)
    ext = load(
        name="pylinrec",
        sources=EXT_SOURCES,
        extra_include_paths=INCLUDES,
        extra_cflags=CPP_FLAGS + extra_cflags,
        extra_cuda_cflags=CUDA_FLAGS + extra_cuda_cflags,
        build_directory=str(BUILD_DIR),
        verbose=verbose)
    return ext
{% endhighlight %}
</details>


{% highlight python %}
>>> import torch
>>> import build
>>> _C = build.extension()  # compiles and loads .build/pylinrec.so as a module
>>> _C.linrec_ref_fwd(torch.Tensor(10), torch.Tensor(10))
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) # empty outputs
{% endhighlight %}


### Reference Implementation

With the build pipeline in place, we translate the reference implementation to CUDA. In the figure below, we see that a GPU consists of many small compute units, so-called streaming multiprocessors or SMs. With a slight oversimplification, SMs execute independent blocks of computation. To partition the work among these blocks, we have to consider the computational structure of our `linrec_ref_fwd` function. Note that the `inputs` and `coeffs` tensors are stored as one flattened array in memory and their their last dimension represents the sequence index. This means that consecutive sequence elements are also consecutive in memory. Therefore, the $$i$$-th block can simply process the $$i$$-th sequence at the memory index `seqLen*blockIdx.x`.

<div class="row mt-3">
    <div class="col-xl mt-3 mt-xl-0">
      {% include figure.html path="assets/img/2025-04-28-linrec/GA100-arch.png" class="img-fluid" %}
    </div>
</div>
<div class="caption">
    The Nvidia A100 GPU Architecture. Source:
    <a href="https://images.nvidia.com/aem-dam/en-zz/Solutions/data-center/nvidia-ampere-architecture-whitepaper.pdf#page=20"> NVIDIA A100 White Paper</a>
</div>

With this insight, we can easily implement `linrec_ref_fwd_kernel` with a parallel for-loop, where each block just computes the linear recursion of its assigned sequence. Recall that the function `linrec_ref_fwd` from the previous section is invoked from within Python and runs on the CPU. To launch the kernel on the GPU, it schedules one block for each sequence. The number of sequences is determined by the number of batches and channels.

<details>
<summary>Show code: kernel launch from `linrec_ref_fwd` </summary>
{% highlight cpp %}
Tensor linrec_ref_fwd(const Tensor &inputs, const Tensor &coeffs) {
    // Input checks: matching dimensions, strides, devices, dtype
    Tensor outputs = torch::empty_like(inputs); // prepare outputs

    // Infer number of sequences and sequence length
    TORCH_CHECK(inputs.stride(-1) == 1);        // inner most dimension is last
    const int seqlen = inputs.size(-1);         // the sequence length
    const int numseq = inputs.numel() / seqlen; // the number of sequences: batches*channels*...

    // Launch kernel function
    const int threads = 1; 
    const int blocks = numseq;
    linrec_ref_fwd_kernel<float><<<blocks, threads>>>(inputs.data_ptr<float>(), 
        coeffs.data_ptr<float>(), outputs.data_ptr<float>(), seqlen
    );
    return outputs;
}

{% endhighlight %}
</details>


{% highlight cpp %}
template <typename kT>
__global__ void linrec_ref_fwd_kernel(const kT* inputs, const kT* coeffs, kT* outputs, const int seqLen) {
    // Layout: dim=(numseq,seqLen), strides=(seqLen,1)
    int seqBaseIdx = seqLen * blockIdx.x; // threads block process sequences independently: inputs[seqBaseIdx + i]
    inputs = &inputs[seqBaseIdx];         // get pointer to sequence
    coeffs = &coeffs[seqBaseIdx];         // get pointer to sequence
    outputs = &outputs[seqBaseIdx];       // get pointer to sequence

    // Linear Recursion
    outputs[0] = inputs[0];                         // set start element
    for(int i = 1; i < seqLen; i++) {               // linear scan
        outputs[i] = outputs[i-1] * coeffs[i] + inputs[i];
    }
}
{% endhighlight %}

Note that the kernel is a _templated_ C++ function taking a datatype `kT` as a compile-time parameter. This allows us to instantiate the kernel for the different required datatypes and is a common theme in CUDA programs. More generally, templating allows to select and dispatch any compile-time configuration at runtime using the `static constexpr` feature from C++20.

<details>
<summary>Show code: the dispatch function, a compile-time matching mechanism </summary>
{% highlight cpp %}
template <std::array CONFIG_LIST, std::size_t I = 0>
inline void dispatch(auto config, auto &&func, std::string funcname) {
    static constexpr auto CONFIG = CONFIG_LIST[I];
    if (CONFIG == config) {
        func.template operator()<CONFIG>();  // call with compile-time config
        return;
    }
    if constexpr (I + 1 < CONFIG_LIST.size()) {
        dispatch<CONFIG_LIST, I + 1>(config, func, funcname);
        return;
    }
    TORCH_CHECK_NOT_IMPLEMENTED(false, "'", funcname, "' is not compiled for this config.")
}
{% endhighlight %}
</details>



{% highlight cpp %}
static constexpr auto CONFIG_LIST = std::array{/*config1*/, /*config2*/, ...};
dispatch<CONFIG_LIST>(config, [&]<auto config>() {
    mytemplatedfunc</*config*/><<<blocks, threads>>>( 
        inputs.data_ptr<T>(),
        outputs.data_ptr<T>(),
    );
}, "mytemplatedfunc"); // name for errors
{% endhighlight %}


### Tile Implementation
In the previous section, each block of work executes a single sequential thread. But zooming in on one streaming multiprocessor, we see that a block can execute operations on up to 1024 threads in synchronization! Furthermore, the SM has a total of 65536 registers which means that one block can hold and process a tile containing two `inputs` and `coeffs` sequences of length up to 32768 at once. How can we efficiently make use of all these resources?

<div class="row justify-content-md-center">
    <div class="col-6 mt-3 mt-sm-0">
      {% include figure.html path="assets/img/2025-04-28-linrec/GA100-sm.png" class="img-fluid" %}
    </div>
</div>
<div class="caption">
    The Nvidia A100 Streaming Multiprocessor. Source:
    <a href="https://images.nvidia.com/aem-dam/en-zz/Solutions/data-center/nvidia-ampere-architecture-whitepaper.pdf#page=21"> NVIDIA A100 White Paper</a>
</div>

We begin by partitioning the work among blocks in the same way as the reference implementation, i.e. all threads in the same block share the same pointer. Then, we need to distribute the sequence of length `seqLen` among the number of threads `numThreads` to obtain `elemsPerThread`. If there is a remainder, a tail of last threads will receive one element less. Note that all threads execute the same code, but self-assign different base indices and sequence lengths depending on their `threadId`. Finally, they load the respective subsequences of `inputs` and `coeffs` into their thread-local arrays. An important detail here is that the compile-time argument `kMaxElemsPerThread` determines the size and indexing into the thread-local array. This guarantees that the array is statically mapped to the registers on the SM.

<details>
<summary>Show code: kernel launch from `linrec_tile_fwd` </summary>
{% highlight cpp %}
Tensor linrec_tile_fwd(const Tensor &inputs, const Tensor &coeffs, const Option& options) {
    // Input checks: matching dimensions, strides, devices, dtype
    Tensor outputs = torch::empty_like(inputs); // Prepare Outputs

    // Infer number of sequences and sequence length
    TORCH_CHECK(inputs.stride(-1) == 1);        // inner most dimension is last
    int seqlen = inputs.size(-1);               // the sequence length
    int numseq = inputs.numel() / seqlen;       // the number of sequences over batches, channels, etc

    // Unpack and determine compile-time arguments
    int kMaxElemsPerThread = get(options, "kMaxElemsPerThread", 16); 
    int kMaxThreadsPerWarp = get(options, "kMaxThreadsPerWarp", 32); 
    int kMaxThreadsPerBlock = get(options, "kMaxThreadsPerBlock", 1024); 

    // Dispatch templated function: instantiate compile-time configuration
    static constexpr auto CONFIG_LIST = std::array{/*config1*/, /*config2*/, ...};
    auto config = std::array{kMaxElemsPerThread, kMaxThreadsPerWarp, kMaxThreadsPerBlock};
    dispatch<CONFIG_LIST>(config, [&]<auto config>() {
        // select kernel based on compile-time arguments 
        static constexpr int kMaxElemsPerThread = config[0];
        static constexpr int kMaxThreadsPerWarp = config[1];
        static constexpr int kMaxThreadsPerBlock = config[2];
        auto kernel = linrec_tile_fwd_kernel<float, kMaxElemsPerThread, kMaxThreadsPerWarp, kMaxThreadsPerBlock>;

        // determine run-time arguments
        int blocks = numseq;
        int threads = std::min(ceildiv(seqlen, kMaxElemsPerThread), kMaxThreadsPerBlock);

        // launch kernel
        kernel<<<blocks, threads, smem, stream>>>(inputs.data_ptr<float>(),
            coeffs.data_ptr<float>(), outputs.data_ptr<float>(), seqlen);
        C10_CUDA_KERNEL_LAUNCH_CHECK();
    }, "linrec_tile_fwd_kernel"); // name for errors
    return outputs;
}

{% endhighlight %}
</details>

<div class="l-body-outset">
{% highlight cpp %}
template <typename kT, ushort kMaxElemsPerThread, ushort kMaxThreadsPerWarp, ushort kMaxThreadsPerBlock>
__global__ void __launch_bounds__(kMaxThreadsPerBlock)
linrec_tile_fwd_kernel(const kT* inputs, const kT* coeffs, kT* outputs, int const seqLen) {
    // Layout: dim=(X,L), strides=(L,1)
    const int seqBaseIdx = seqLen * blockIdx.x; // process sequences independently: inputs[seqBaseIdx+i]
    inputs = &inputs[seqBaseIdx];               // get pointer to sequence
    coeffs = &coeffs[seqBaseIdx];               // get pointer to sequence
    outputs = &outputs[seqBaseIdx];             // get pointer to sequence

    // Determine Tile Layout
    const ushort numThreads = blockDim.x;
    const ushort threadId = threadIdx.x;                                                  // index of current thread
    const ushort elemsPerThread = ceildiv(seqLen, (int) numThreads);                      // distribute seqLen among numThreads
    const ushort numTailThreads = numThreads * elemsPerThread - seqLen;                   // last numTailThreads have one elem less
    const int threadTailId = (int) threadId - (numThreads - numTailThreads);              // tail start indicated by ..., 0, 1, 2, ...
    const ushort threadSeqLen = (threadTailId < 0) ? elemsPerThread : (elemsPerThread-1); // sequence length processed by every thread
    const ushort threadBaseIdx = threadId * elemsPerThread - max(threadTailId, 0);        // base index to process by every thread

    // Load inputs and coeffs of tile into thread-local arrays
    kT threadAccOutput[kMaxElemsPerThread];
    kT threadAccCoeff[kMaxElemsPerThread];
    for(ushort i = 0; i < kMaxElemsPerThread; i++) {
        threadAccOutput[i] = (i < threadSeqLen) ? inputs[threadBaseIdx + i] : 0;  // load or fill with 0
        threadAccCoeff[i] = (i < threadSeqLen) ? coeffs[threadBaseIdx + i] : 1;   // load or fill with 1
    }
    // Compute parallel scan on a tile (=subsequence) that fits into one thread block 
    _linrec_scan_tile_parallel_<kT, kMaxElemsPerThread, kMaxThreadsPerWarp, kMaxThreadsPerBlock, algocode>(
        threadAccOutput, threadAccCoeff, numThreads
    );
    for(ushort i = 0; i < threadSeqLen; i++) { // Store outputs
        outputs[threadBaseIdx + i] = threadAccOutput[i];
    }
}
{% endhighlight %}
</div>


Recall the parallel form of the linear recursion from Part I. The second thread calculated the linear recursion $$[\bar{y}_{L',l}]_{l=L'}^{L-1}$$ and stored the cumulative coefficients $$[\tilde{c}_{L'-1,l}]_{l=L'}^{L-1}$$ on its sub-sequence. This is exactly implemented in algorithmic level 1 of `_linrec_scan_tile_parallel_`. For level 2, we need to introduce the concept of a warp. In CUDA, a warp represents a group of 32 threads that execute the same instruction at the same time. Therefore, communication between threads in the same warp incurrs no overhead. For example, the `__shfl_up_sync` instruction copies a variable to the thread `threadId+delta`. This allows to propagate and accumulate the transition elements across threads so that every thread receives its offset `warpAccOutput` ($$y_{L'-1}$$) within 6 steps. Once that is achieved, the simple re-combination remains. We need to compute the exact warp size because it might not be full.

<div class="l-body-outset">
{% highlight cpp %}

template <typename kT, ushort kMaxElemsPerThread, ushort kMaxThreadsPerWarp, ushort kMaxThreadsPerBlock>
__forceinline__  __device__  void _linrec_scan_tile_parallel_(kT* threadAccOutput, kT* threadAccCoeff, const ushort numThreads) {
    // Level 1: Accumulate elements within this thread
    for(ushort i = 1; i < kMaxElemsPerThread; i++) {
        threadAccOutput[i] = threadAccOutput[i-1] * threadAccCoeff[i] + threadAccOutput[i];
        threadAccCoeff[i]  = threadAccCoeff[i-1] * threadAccCoeff[i];
    }
    
    // Level 2: Accumulate elements across threads within this warp
    // Determine Warp Configuration
    const ushort laneId = threadIdx.x % kMaxThreadsPerWarp;
    const ushort warpId = threadIdx.x / kMaxThreadsPerWarp;
    const ushort numWarps = ceildiv(numThreads, kMaxThreadsPerWarp);
    const ushort lastWarpSize = numThreads - kMaxThreadsPerWarp * (numWarps-1);
    const ushort thisWarpSize = (warpId==numWarps-1) ? lastWarpSize : kMaxThreadsPerWarp;
    
    kT warpAccOutput = __shfl_up_sync(0xffffffff, threadAccOutput[kMaxElemsPerThread-1], 1); // get transition elements between threads
    kT warpAccCoeff  = __shfl_up_sync(0xffffffff, threadAccCoeff[kMaxElemsPerThread-1], 1);  // get transition elements between threads
    warpAccOutput = (laneId == 0) ? 0 : warpAccOutput;  // set default 1 for first lane (=thread in warp)
    warpAccCoeff  = (laneId == 0) ? 1 : warpAccCoeff;   // set default 0 for first lane (=thread in warp)
    for (ushort delta = 1; delta < thisWarpSize; delta *= 2) { 
        kT prevAccOutput = __shfl_up_sync(0xffffffff, warpAccOutput, delta);
        kT prevAccCoeff  = __shfl_up_sync(0xffffffff, warpAccCoeff, delta);

        // don't update warpAccOutput and warpAccCoeff in delta lower lanes
        warpAccOutput = (laneId < delta) ? warpAccOutput : prevAccOutput * warpAccCoeff + warpAccOutput;
        warpAccCoeff  = (laneId < delta) ? warpAccCoeff  : prevAccCoeff * warpAccCoeff;
    }

    for (ushort i = 0; i < kMaxElemsPerThread; i++) { // distribute accumulates into thread elements
        threadAccOutput[i] = warpAccOutput * threadAccCoeff[i] + threadAccOutput[i];
    }
}
{% endhighlight %}
</div>

When more than one warp per block is used, we need to propagate the transition elements across warps. This can be achieved by passing the warp-level transition elements to the first warp (via block-shared memory) where the propagation is performed. Then each thread recombines its entries with the propagated offsets `blockAccOutput`. In this way, a linear recursion of length 32768 could be computed with 5*32+20 floating point operations on 1024 threads if all registers could be used for the thread-local arrays. Pretty cool!


### Pipe Implementation

In order to support linear recursions exceeding the maximum tile size, we introduce variables `tileBaseIdx` and `tileSeqLen` which allow us to sequentially accumulate across tiles in an outer loop. This requires some minor adjustments to the `threadBaseIdx` and `threadSeqLen` calculation. The kernel now processes tiles in a pipelined manner and it could even be feasible to overlap asynchronous memory loading with actual computation. At this point, we would like to draw the reader's attention to the chosen data type for most indices. Since the `tileSeqLen` is limited by the number of registers of an SM, we know that all variables in the range of the tile are guaranteed to be smaller than 65536. We can therefore safely use 8-bit `ushort` indexing and make more registers available for the thread-local arrays containing the actual data.

<details>
<summary>Show code: `linrec_pipe_fwd` kernel </summary>.
{% highlight cpp %}
template <typename kT, ushort kMaxElemsPerThread, ushort kMaxThreadsPerWarp, ushort kMaxThreadsPerBlock>
__global__ void __launch_bounds__(kMaxThreadsPerBlock)
linrec_pipe_fwd_kernel(const kT* inputs, const kT* coeffs, kT* outputs, int const seqLen) {
    // Layout: dim=(X,L), strides=(L,1)
    const int seqBaseIdx = seqLen * blockIdx.x; // process sequences independently in reverse: inputs[seqBaseIdx-i]
    inputs = &inputs[seqBaseIdx];               // get pointer to sequence
    coeffs = &coeffs[seqBaseIdx];               // get pointer to sequence
    outputs = &outputs[seqBaseIdx];             // get pointer to sequence

    __shared__ kT seqAccOutput, seqAccCoeff; // for sequential accumulation between tiles
    if (threadIdx.x == 0) {
        seqAccOutput = 0;
        seqAccCoeff = 1;
    } __syncwarp(); // avoid divergence

    // Determine Tile Layout
    const ushort numThreads = blockDim.x;
    const ushort threadId = threadIdx.x;                     // index of current thread
    const ushort elemsPerTile = kMaxElemsPerThread * numThreads;                                        // the default number of elements per tile
    for (int tileBaseIdx = !rev ? 0; tileBaseIdx < seqLen; tileBaseIdx += elemsPerTile) { // linear scan over tiles
        const ushort tileSeqLen = min(seqLen - tileBaseIdx, elemsPerTile);                              // length of the tile to scan with thread block
        const ushort elemsPerThread = ceildiv(tileSeqLen, numThreads);                                  // distribute tileSeqLen among numThreads
        const ushort numTailThreads = numThreads * elemsPerThread - tileSeqLen;                         // last numTailThreads have one elem less
        const int threadTailId = (int) threadId - (numThreads - numTailThreads);                        // tail start indicated by ..., 0, 1, 2, ...
        const ushort threadSeqLen = (threadTailId < 0) ? elemsPerThread : (elemsPerThread-1);           // sequence length processed by every thread
        const ushort threadBaseIdx = threadId * elemsPerThread - max(threadTailId, 0);                  // base index to process by every thread

        //
        // Load inputs and coeffs of tile into thread-local arrays
        kT threadAccOutput[kMaxElemsPerThread];
        kT threadAccCoeff[kMaxElemsPerThread];
        for(ushort i = 0; i < kMaxElemsPerThread; i++) {
            threadAccOutput[i] = (i < threadSeqLen) ? inputs[tileBaseIdx + threadBaseIdx + i] : 0;  // load or fill with 0
            threadAccCoeff[i] = (i < threadSeqLen) ? coeffs[tileBaseIdx + threadBaseIdx + i] : 1;   // load or fill with 1
        }

        // Combine seqAccOutput and and -Gate with first threadAccOutput and -Gate 
        if (threadIdx.x == 0){
            threadAccOutput[0] = seqAccOutput * threadAccCoeff[0] + threadAccOutput[0];
            threadAccCoeff[0] =  seqAccCoeff * threadAccCoeff[0];
        } __syncthreads(); // avoid race condition

        _linrec_scan_tile_parallel_<kT, kMaxElemsPerThread kMaxThreadsPerWarp, kMaxThreadsPerBlock>(
            threadAccOutput, threadAccCoeff, numThreads
        );
    
        // Store last threadAccOutput and -Gate into seqAccOutput and and -Gate
        if (threadIdx.x == numThreads-1) {
            seqAccOutput = threadAccOutput[kMaxElemsPerThread-1];
            seqAccCoeff = threadAccCoeff[kMaxElemsPerThread-1];
        } __syncthreads(); // avoid race condition

        for(ushort i = 0; i < threadSeqLen; i++) { // Store outputs
            outputs[tileBaseIdx + threadBaseIdx + i] = threadAccOutput[i];
        }
    }
}
{% endhighlight %}
</details>

### Reverse and Backward Implementation

From Part I, we know that computing the backward pass through the linear recursion mainly consists of a reversed linear recursion and an index shift. We support the reverse recursion by loading and storing the tiles in reverse order into the thread-local arrays. For the tile layout, we reverse the `threadId` and thereby the `threadBaseIdx` if the runtime-argument `rev` is true:

{% highlight cpp %}
const ushort threadIdrev = !rev ? threadIdx.x : (numThreads - threadIdx.x - 1);
{% endhighlight %}

With the base indices reversed, we still need to copy the data in reverse order. To keep our kernels nice and tidy, we move the memory I/O functionality into a separate `memio.h` file:
{% highlight cpp %}
template <typename kT, typename count_t>
__forceinline__  __device__  void copy_naive(kT* __restrict__ dst, 
                                              const kT* __restrict__ src,  
                                              const count_t dstElemsPerThread, 
                                              const count_t srcElemsPerThread, 
                                              const bool rev, const kT fillval) {
    for (count_t i = 0; i < dstElemsPerThread; i++) {
        count_t j = !rev ? i : (srcElemsPerThread-1)-i;
        dst[i] = (i < srcElemsPerThread) ? src[j] : fillval;
    }
}
{% endhighlight %}

The logic to shift indices correctly between tiles and threads is implemented is this file as well. Finally, we experiment with different approaches to copying, such as vectorization and coalescing<d-footnote> for more information on memory accesses see  <a href="https://developer.nvidia.com/blog/cuda-pro-tip-write-flexible-kernels-grid-stride-loops/">CUDA Pro Tip: Grid-Stride Loops</a>,  <a href="https://developer.nvidia.com/blog/cuda-pro-tip-increase-performance-with-vectorized-memory-access/">CUDA Pro Tip: Vectorized Memory Access</a>, and <a href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/#device-memory-spaces">CUDA Best Practices: Memory Optimizations</a>.</d-footnote>. The memory loading method is determined by the compile-time parameter `memcode` where `memcode=0` denotes the naïve baseline described in the code above.

### Tuning and Benchmarking

To gain a better understanding of our implementations, we compile these configurations:
<div class="l-body-outset">
{% highlight cpp %}
static constexpr auto CONFIG_NAMES = std::array{"kMaxElemsPerThread", "kMaxThreadsPerWarp", 
                                                "kMaxThreadsPerBlock", "memcode", "algocode"};
static constexpr auto CONFIG_LIST = product(std::array{4, 8, 16}, std::array{32}, 
                                                std::array{32, 64, 128, 256, 512, 1024}, 
                                                std::array{0, 1, 2}, std::array{0, 3});
{% endhighlight %}
</div>

One of the biggest pitfalls in writing CUDA kernels occurs when intermediate variables are not stored in the register file but as local memory on the DRAM<d-footnote>for more information see <a href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/#local-memory">CUDA Best Practices: Local Memory</a> and <a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses:~:text=to%20these%20constraints.-,Local%20Memory,-Local%20memory%20accesses">Cuda Programming Guide: Local Memory</a></d-footnote>. This happens, when local arrays cannot be statically allocated or when the number of live variables exceeds the number of registers. To find out more, we wrap the function `cudaFuncGetAttributes` which returns meta data for a given kernel. Invoking `python eval/func_attrs.py --algocode 3` shows that `linrec_tile_fwd`, `linrec_tile_bwd`, and `linrec_pipe_fwd` make full use of the registers and only in a few configurations exhibit register spilling. On the other hand, `linrec_pipe_fwd` has high register pressure which results in slight spilling to local memory in some configurations.

Now, we compare configurations with `python eval/tune.py linrec_pipe_{f|b}wd --algocode 3`. The script evaluates the kernels on random test data with `#SMs*100` sequences of increasing length. Before benchmarking, we quickly confirm that all errors are in the numerical regime. The tables below depict the performance of the best configurations on an `A100-SXM4-80GB` (with `#SM=108`). We first note that the runtime seems to be dominated by the kernel launch for `seqLen<256` and then it increases linearly. To put these numbers in relation, we transform them into throughput with `(bytes * 1e-9) / (ms * 1e-3)` and compare them to the theoretical bandwidth of `2039 GB/s` in the case of the used GPU<d-footnote> for more information on bandwidth  and throughput see <a href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/#bandwidth">CUDA Best Practices: Performance Metrics</a>.</d-footnote>. We observe that the best configurations effectively use only 75% of the available memory bandwidth . This could be explained either by inefficient memory accesses or by many sequential operations per accessed byte. If we now consider the typical tile size of 512 or 1024 for the best-performing configurations, we notice that it is surprisingly small compared to the maximum size of 16384. It is thus more efficient to sequentially process smaller tiles than to reduce sequential operations by processing large tiles in parallel. From this, we conclude that linear recursions are memory-bound and that memory access patterns are most important for performance.
<div class="l-page" style="display: flex; justify-content: center;">
<div style="display: flex; overflow-x: auto;">
<table style="margin-top: 10px; margin-bottom: 0px;">
  <thead>
    <tr>
      <th style="text-align: left;">Sequence Length</th>
      <th style="text-align: right;">16</th>
      <th style="text-align: right;">32</th>
      <th style="text-align: right;">64</th>
      <th style="text-align: right;">128</th>
      <th style="text-align: right;">256</th>
      <th style="text-align: right;">512</th>
      <th style="text-align: right;">1024</th>
      <th style="text-align: right;">2048</th>
      <th style="text-align: right;">4096</th>
      <th style="text-align: right;">8192</th>
      <th style="text-align: right;">16384</th>
      <th style="text-align: right;">32768</th>
      <th style="text-align: right;">65536</th>
    </tr>
  </thead>
  <tbody>
    <tr style="text-align: right;">
      <td style="text-align: left;">Runtime (ms)</td>
      <td>0.02</td>
      <td>0.02</td>
      <td>0.02</td>
      <td>0.02</td>
      <td>0.03</td>
      <td>0.05</td>
      <td>0.09</td>
      <td>0.17</td>
      <td>0.35</td>
      <td>0.69</td>
      <td>1.35</td>
      <td>2.70</td>
      <td>5.38</td>
    </tr>
    <tr style="text-align: right;">
      <td style="text-align: left;">Memory I/O (GB)</td>
      <td><0.01</td>
      <td><0.01</td>
      <td>0.01</td>
      <td>0.02</td>
      <td>0.03</td>
      <td>0.07</td>
      <td>0.13</td>
      <td>0.27</td>
      <td>0.53</td>
      <td>1.06</td>
      <td>2.12</td>
      <td>4.25</td>
      <td>8.49</td>
    </tr>
    <tr style="text-align: right;">
      <td style="text-align: left;">Throughput (GB/s)</td>
      <td>126.6</td>
      <td>238.2</td>
      <td>476.5</td>
      <td>852.6</td>
      <td>1157.1</td>
      <td>1322.4</td>
      <td>1472.7</td>
      <td>1533.7</td>
      <td>1524.7</td>
      <td>1549.8</td>
      <td>1574.5</td>
      <td>1573.3</td>
      <td>1578.4</td>
    </tr>
    <tr style="text-align: right;">
      <td style="text-align: left;">kMaxElemsPerThread</td>
      <td>4</td>
      <td>4</td>
      <td>4</td>
      <td>4</td>
      <td>8</td>
      <td>8</td>
      <td>8</td>
      <td>8</td>
      <td>8</td>
      <td>8</td>
      <td>8</td>
      <td>8</td>
      <td>8</td>
    </tr>
    <tr style="text-align: right;">
      <td style="text-align: left;">kMaxThreadsPerBlock</td>      <td>32</td>
      <td>32</td>
      <td>32</td>
      <td>32</td>
      <td>32</td>
      <td>64</td>
      <td>64</td>
      <td>64</td>
      <td>64</td>
      <td>64</td>
      <td>64</td>
      <td>64</td>
      <td>64</td>
    </tr>
    <tr style="text-align: right;">
      <td style="text-align: left;">memcode</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
</div>
</div>

<div class="caption">
    Best performing configurations for `linrec_pipe_fwd` on `A100-SXM4-80GB`.
</div>

<div class="l-page" style="display: flex; justify-content: center;">
<div style="display: flex; overflow-x: auto;">
<table style="margin-top: 10px; margin-bottom: 0px">
  <thead>
    <tr style="text-align: right;">
      <th style="text-align: left;">Sequence Length</th>
      <th style="text-align: right;">16</th>
      <th style="text-align: right;">32</th>
      <th style="text-align: right;">64</th>
      <th style="text-align: right;">128</th>
      <th style="text-align: right;">256</th>
      <th style="text-align: right;">512</th>
      <th style="text-align: right;">1024</th>
      <th style="text-align: right;">2048</th>
      <th style="text-align: right;">4096</th>
      <th style="text-align: right;">8192</th>
      <th style="text-align: right;">16384</th>
      <th style="text-align: right;">32768</th>
      <th style="text-align: right;">65536</th>
    </tr>
  </thead>
  <tbody>
    <tr style="text-align: right;">
      <td style="text-align: left;">Runtime (ms)</td>
      <td>0.03</td>
      <td>0.03</td>
      <td>0.03</td>
      <td>0.03</td>
      <td>0.05</td>
      <td>0.08</td>
      <td>0.15</td>
      <td>0.29</td>
      <td>0.58</td>
      <td>1.17</td>
      <td>2.34</td>
      <td>4.70</td>
      <td>9.39</td>
    </tr>
    <tr style="text-align: right;">
      <td style="text-align: left;">Memory I/O (GB)</td>
      <td><0.01</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>0.03</td>
      <td>0.06</td>
      <td>0.11</td>
      <td>0.22</td>
      <td>0.44</td>
      <td>0.88</td>
      <td>1.77</td>
      <td>3.54</td>
      <td>7.08</td>
      <td>14.16</td>
    </tr>
    <tr style="text-align: right;">
      <td style="text-align: left;">Throughput (GB/s)</td>
      <td>135.0</td>
      <td>270.0</td>
      <td>519.2</td>
      <td>1000.0</td>
      <td>1227.3</td>
      <td>1384.6</td>
      <td>1479.5</td>
      <td>1505.2</td>
      <td>1513.1</td>
      <td>1518.5</td>
      <td>1509.8</td>
      <td>1506.9</td>
      <td>1506.9</td>
    </tr>
    <tr style="text-align: right;">
      <td style="text-align: left;">kMaxElemsPerThread</td>
      <td>4</td>
      <td>4</td>
      <td>4</td>
      <td>4</td>
      <td>8</td>
      <td>8</td>
      <td>8</td>
      <td>8</td>
      <td>8</td>
      <td>8</td>
      <td>8</td>
      <td>8</td>
      <td>8</td>
    </tr>
    <tr style="text-align: right;">
      <td style="text-align: left;">kMaxThreadsPerBlock</td>
      <td>256</td>
      <td>256</td>
      <td>256</td>
      <td>256</td>
      <td>512</td>
      <td>64</td>
      <td>64</td>
      <td>64</td>
      <td>128</td>
      <td>128</td>
      <td>128</td>
      <td>64</td>
      <td>64</td>
    </tr>
    <tr style="text-align: right;">
      <td style="text-align: left;">memcode</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
</div>
</div>

<div class="caption">
    Best performing configurations for `linrec_pipe_bwd` on `A100-SXM4-80GB`.
</div>

We wrap the `_C.linrec_*` bindings into PyTorch operators to integrate them with autograd and compile systems. While we could automatically tune our kernels for a good configuration given an input shape and a GPU, we manually set `kMaxElemsPerThread=8`, `kMaxThreadsPerBlock=64`, and `memcode=0`. Finally, we compare the CUDA implementations with PyTorch implementations based on the higher-order associative scan [`torch._higher_order_ops.associative_scan`](https://github.com/pytorch/pytorch/blob/main/torch/_higher_order_ops/associative_scan.py) beta version, and the for-loop reference from the first chapter:

{% highlight python %}
linrec.impl.cuda.ops.linrec_pipe        # CUDA: Pipe Implementation
linrec.impl.cuda.ops.linrec_tile        # CUDA: Tile Implementation
linrec.impl.cuda.ops.linrec_ref         # CUDA: Reference Implementation
linrec.impl.python.ops.linrec_hop       # PyTorch: Higher-Order Implementation
linrec.impl.python.ops.linrec_ref       # PyTorch: Reference Implementation
{% endhighlight %}

Comparing these implementations in the figure below, we observe that simply translating the for-loop-based PyTorch implementation into CUDA already yields a significant speedup. The PyTorch higher-order operation allows for further speed-ups, but cannot reach beyond 1000 GB/s and does not efficiently run in backward mode. The tile and pipe implementations are significantly faster than all other implementations and remain consistent even in the backward mode. In absolute terms, they are a bit slower but still comparable to the `torch.add` operation. In other words, the scan behaves almost like a binary element-wise operation despite its sequential computational structure. This is possible, because it is memory-bound and requires the same amount of memory I/O as `torch.add`. Finally, our implementation makes rather efficient use of the available DRAM bandwidth  depicted in brown.
 
<div class="row justify-content-md-center" style="margin-bottom: 0px">
    <div class="col-xl mt-3 mt-sm-0">
      {% include figure.html path="assets/img/2025-04-28-linrec/bench.png" class="img-fluid" %}
    </div>
</div>
<div class="caption">
    Linear recursion implementations in comparison.
</div>


## Discussion

In the previous sections we learned that linear recursions can be computed in parallel, how they can be mapped onto the CUDA architecture, and why they behave similarly to a simple `torch.add`. The goal was to understand linear recursions as a fundamental building block of an 'unfused' Mamba implementation where inputs and coefficients are assumed to be pre-computed. In this section, we briefly discuss this fundamental assumption. Then, we conclude with an outlook on promisingdirections for future research on linear recursions.

The problem of memory-bound operations such as linear recursions is that they waste computational resources. Framed more positively, they present an opportunity to perform computations 'for free' once the memory is transferred to shared or thread-local memory. Mamba for example uses on-device state expansion: the function [`selective_scan_fn`](https://github.com/state-spaces/mamba/blob/main/mamba_ssm/ops/selective_scan_interface.py#L21) expands a loaded sequence to multiple sequences, performs multiple linear recursions, contracts it to the original dimension, and writes the result back. This means that the expanded state is never materialized on the DRAM which reduces memory transfers and enables the use of longer sequences and larger states. In Mamba-2, the transition coefficients are shared across all channels in one head, which enables even more computation per transferred byte using the same algorithm as flash linear attention<d-cite key="yang2024fla"></d-cite>, GLA<d-cite key="yang2024gla"></d-cite>, DeltaNet<d-cite key="yang2024deltanet"></d-cite> and Gated DeltaNet<d-cite key="yang2024gateddeltanet"></d-cite>. These developments show that memory I/O plays a crucial role in architecture design considerations. The paper by Golami et al <d-cite key="gholami2020memorywall"></d-cite> with its main figure below suggest that maximizing model expressivity per transferred byte will become even more important in the future.  

<div class="row justify-content-md-center">
    <div class="col-xl mt-3 mt-sm-0">
      {% include figure.html path="assets/img/2025-04-28-linrec/hw_scaling.png" class="img-fluid" %}
    </div>
</div>
<div class="caption">
    Memory I/O scales slower than computation. Source: AI and Memory Wall, Fig. 1 <d-cite key="gholami2020memorywall"></d-cite>
</div>


That said, state expansion and parameter sharing might not be the only way to increase model expressivity per transferred byte. For some applications, it might even be counterproductive to introduce such assumptions. There are still many open questions in the space of (linear) RNNs for which the cheaply available computation per transferred byte might be useful. For example, diagonal linear RNNs have less expressive power than dense, or non-linear RNNs<d-cite key="cirone2024theoreticalfoundations"></d-cite>, in particular for state-tracking tasks<d-cite key="merrill2024illusionstate"></d-cite>. Complex or at least negative transition coefficients can help to mitigate this problem as discussed by Grazzi, Siems, et al<d-cite key="grazzi2024unlockingstate"></d-cite>. Alternatively, non-linear RNNs can be parallelized by approximation using iterated linearized dynamics <d-cite key="lim2024elk"></d-cite><d-cite key="gonzalez2024deer"></d-cite>, or accelerated sequentially with hardware-aware implementations such as FlashRNN<d-cite key="pöppel2024flashrnn"></d-cite>. For all of these computational structures, learning long-range interaction requires intricate parametrizations. Here, the dominant approaches are motivated by dynamical systems <d-cite key="gu2022s4"></d-cite><d-cite key="orvieto2023lru"></d-cite><d-cite key="rusch2024ossm"></d-cite> and associative memory <d-cite key="schlag2021fwp"></d-cite><d-cite key="yang2024deltanet"></d-cite><d-cite key="beck2024xlstm"></d-cite> to tackle the vanishing gradient problem <d-cite key="pascanu2012vanishing"></d-cite><d-cite key="zucchet2024rnnoptimization"></d-cite>. 

In conclusion, linear recursions present a simple mathematical object with surprising modeling expressivity. These properties make it an interesting object for various problems in deep learning, potentially even search or test-time computation. We hope that this blog post lowers the entry bar and sparks the interest to pursue research on linear-time sequence mixing.