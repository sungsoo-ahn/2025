---
layout: distill
title: Towards Complex Reasoning of LLMs
description: This paper investigates recent advancements in large language models (LLMs) with a particular focus on their application to complex reasoning tasks, which often involve multi-step problem-solving and nuanced interpretation. We further explore innovations such as Chain of Thought (CoT), Quiet-STaR, Fast and Slow Thinking, which aim to enhance logical clarity, refine reasoning processes, and improve adaptability across various contexts. Finally, this paper evaluates OpenAI-o1, emphasizing its strengths in performing multi-step logical deductions and adaptive reasoning.
date: 2025-04-28
future: true
htmlwidgets: true
hidden: false

# Anonymize when submitting
authors:
  - name: Anonymous

# must be the exact same name as your blogpost
bibliography: 2025-04-28-study-on-o1.bib  

# Add a table of contents to your post.
#   - make sure that TOC names match the actual section names
#     for hyperlinks within the post to work correctly. 
#   - please use this format rather than manually creating a markdown table of contents.
toc:
  - name: Introduction
  - name: Techniques of LLMs
    subsections:
    - name: Techniques of Pre-training
    - name: Techniques of Post-training
  - name: Discussion for Complex Reasoning
    subsections:
    - name: Chain of Thought Reasoning
    - name: Quiet-STaR Thinking before Speaking
    - name: Controllable Fast and Slow Thinking
  - name: Performance Analysis of OpenAI-o1
    subsections:
    - name: Quality Evaluation
    - name: Price Evaluation
    - name: Speed Evaluation
    - name: Risk Evaluation
  - name: Conclusion

# Below is an example of injecting additional post-specific styles.
# This is used in the 'Layouts' section of this post.
# If you use this post as a template, delete this _styles block.
_styles: >
  .fake-img {
    background: #bbb;
    border: 1px solid rgba(0, 0, 0, 0.1);
    box-shadow: 0 0px 4px rgba(0, 0, 0, 0.1);
    margin-bottom: 12px;
  }
  .fake-img p {
    font-family: monospace;
    color: white;
    text-align: left;
    margin: 12px 0;
    text-align: center;
    font-size: 16px;
  }
---
## Introduction

Large Language Models (LLMs) have emerged as transformative tools in artificial intelligence (AI), revolutionizing tasks such as natural language processing, machine translation, and conversational AI. Their unprecedented capabilities in understanding, generating, and interacting with human language have significantly advanced diverse domains, including healthcare, education, entertainment, and scientific research <d-cite key="zhao2023survey"></d-cite>. These capabilities stem from the model ability to process massive amounts of data while leveraging sophisticated training paradigms to capture linguistic nuances and semantic relationships.

This remarkable advancement is rooted in a structured development process comprising three interconnected stages: preprocessing, pre-training, and post-training. Each stage contributes to the model's overall capability and adaptability, as illustrated in the following Figure 1:

<figure style="text-align: center;">
    <img src="{{ 'assets/img/2025-04-28-study-on-o1/comparison.png' | relative_url }}" width="400">
    <figcaption style="font-size: 1em;">Figure 1: Preprocessing, Pre-training and Post-training Phases in LLM Development.</figcaption>
</figure>

* **Preprocessing** lays the groundwork by preparing high-quality input data. Techniques such as tokenization and filtering ensure data uniformity, while regularization and encoder-decoder architectures enhance robustness for subsequent training stages.
* **Pre-training** develops general language knowledge using large, unlabeled datasets. Models like BERT and GPT employ methods such as masked language modeling (MLM) and unsupervised learning to capture linguistic patterns, enabling broad generalization across tasks.
* **Post-training** adapts the model to specific applications or domains through advanced techniques, including reinforcement learning with human feedback (RLHF), supervised fine-tuning (SFT), and direct preference optimization (DPO). These methods refine the general knowledge gained during pre-training, enhancing task-specific performance, precision, and applicability.

According to <d-cite key="kumar2024scaling"></d-cite>, the following Figure 2 illustrates the finite-precision effects on learning curves during pre-training and post-training. It depicts the trends of the **pre-training scaling law** and the **post-training scaling law** as the validation loss (val loss) evolves with the increasing number of tokens.

<figure style="text-align: center;">
    <img src="{{ 'assets/img/2025-04-28-study-on-o1/scaling law.png' | relative_url }}" width="400">
    <figcaption style="font-size: 1em;">Figure 2: Scaling law of pre-training and post-training.</figcaption>
</figure>

The pre-training phase shows a relatively faster rate of decrease in validation loss, driven by the model's ability to quickly capture broad patterns and general features from large-scale data. In contrast, the post-training phase exhibits a slower decline in validation loss, as this stage focuses on fine-tuning for specific tasks or domains, where improvements tend to be more incremental. Furthermore, finite-precision effects impose hardware-based limitations on both phases, resulting in a plateau in val loss, particularly during the post-training phase where fine-tuning demands greater computational precision. The interplay of these factors underscores the need for efficient scaling strategies to optimize both training and inference.

Despite their transformative potential, LLMs have historically struggled to address the challenges posed by **complex reasoning** tasks. Traditional approaches often fall short in handling multi-step logical deductions, dynamic decision-making, and nuanced problem decomposition, limiting their application in scenarios requiring deep reasoning capabilities. These limitations arise from the lack of structured frameworks for managing intermediate reasoning steps, inadequate adaptability to diverse reasoning contexts, and insufficient mechanisms to ensure robust reasoning under ambiguous or complex conditions.

To overcome these challenges, this paper explores several innovative approaches aimed at enhancing LLMs' capabilities in complex reasoning. Specifically, we examine advanced reasoning frameworks such as **Chain of Thought (CoT)**, which introduces explicit intermediate steps for logical clarity; **Quiet-STaR**, which optimizes reasoning paths through self-learning; and **Fast and Slow Thinking**, which balances rapid heuristic reasoning with deliberate analytical thought. By analyzing these methods, we aim to uncover their strengths, limitations, and potential synergies in addressing the demands of complex reasoning. Finally, we evaluate the performance of OpenAI-o1, a state-of-the-art model that may integrate these advancements, demonstrating its superior capabilities in tackling complex reasoning tasks.

## Techniques of LLMs

This section provides an overview of key techniques used in pre-training and post-training to improve model performance, adaptability, and application scope. These techniques include basic methods, such as data preparation and fine-tuning, as well as advanced approaches for alignment, domain adaptation, and multimodal capability enhancement. As shown in the follow Figure 3, these methods work together to optimize the model's functionality and scalability, ensuring its effectiveness across diverse tasks.

<figure style="text-align: center;">
    <img src="{{ 'assets/img/2025-04-28-study-on-o1/Techniques.png' | relative_url }}" width="400">
    <figcaption style="font-size: 1em;">Figure 3: Techniques of pre-training and post-training.</figcaption>
</figure>

### Techniques of Pre-training

Pre-training is the initial phase of training a model on large-scale datasets, enabling it to learn generalizable features and foundational knowledge. The primary objective of pre-training is to develop a robust feature extractor, which can then be fine-tuned for specific downstream tasks. To achieve this, the pre-training phase employs several key techniques designed to optimize learning and enhance generalization.

 **1)  Self-supervised Learning.** In self-supervised learning <d-cite key="Gui2023ASO"></d-cite>, tasks are divided into two main types: pretext tasks and downstream tasks. Pretext tasks involve training an AI system to uncover meaningful representations from unstructured data. These representations can then serve as input for downstream tasks, such as supervised learning or reinforcement learning.

- **Pretext Tasks** are self-supervised learning tasks designed to create surrogate labels from unlabeled data, enabling the model to learn generalizable representations.

$$
\mathcal{L}_{\text{pretext}} = \mathbb{E}_{x \sim \mathcal{D}} \left[ \ell\left( f_\theta(T(x)), g_\phi(h(T'(x))) \right) \right]
$$

> $x$: Unlabeled data sample.
>
> $f_\theta, g_\phi$: Encoder and projection head for representation learning.
>
> $\mathcal{D}$: Dataset distribution.

- **Downstream tasks** use representations learned from pretext tasks to solve domain-specific supervised or unsupervised problems.

  $$
  \mathcal{L}_{\text{downstream}} = \mathbb{E}_{(x, y) \sim \mathcal{D}'} \left[ \ell\left( h_\psi(f_\theta(x)), y \right) \right]
  $$

> $(x, y)$: Input-output pair from labeled downstream dataset $\mathcal{D}'$.
>
> $f_\theta$: Encoder from pretext task.
>
> $h_\psi$: Task-specific head.
>
> $\ell$: Loss function for the specific downstream task.

 **2) Transformer Architecture.** The Transformer <d-cite key="Vaswani2017AttentionIA"></d-cite> is a neural network architecture designed for sequence modeling and transformation, with its core idea being the attention mechanism, completely removing the constraints of traditional RNNs and CNNs. By combining Multi-Head Self-Attention and Feed-Forward Networks, the Transformer efficiently captures long-range dependencies and offers better parallelization capabilities. The input sequence, processed with embedding and positional encoding, passes through multiple encoder and decoder layers. The encoder transforms the input into context-aware representations, while the decoder generates the target output based on the encoder output and the target sequence.

 **3)  Long-context Stage.** The Long-context Stage <d-cite key="An2024TrainingFreeLS"></d-cite> is designed to enhance a model's ability to process and understand long-sequence inputs. This stage involves the introduction of high-quality datasets containing long sequences, such as code repositories and books, combined with short-sequence data for continued training. Studies have shown that training with sequences exceeding the evaluation length significantly improves the model's performance on long-context tasks.

 **4)  High-quality Stage.** The High-quality Stage <d-cite key="hqtd"></d-cite> enhances model performance and reliability by utilizing accurate, consistent, and relevant datasets. Through rigorous data cleaning and task-aligned curation, it ensures models learn precise language patterns, improving comprehension and generation. This stage is crucial for building robust LLMs capable of producing natural, coherent, and user-aligned outputs across diverse applications.

### Techniques of Post-training

Post-training focuses on adapting a pre-trained language model to specific applications or improving its overall performance. This phase typically involves several advanced techniques, each of which serves distinct roles in enhancing model alignment, flexibility, and adaptability. The following subsections outline key post-training techniques:

 **1) Alignment.** Model alignment techniques such as Supervised Fine-Tuning (SFT), Reinforcement Learning with Human Feedback (RLHF), and Direct Preference Optimization (DPO) are essential for refining language models to meet task-specific and user-oriented standards.

- **SFT** <d-cite key="ouyang2022training"></d-cite> uses standard supervised learning methods to optimize the model by minimizing the loss on labeled datasets. The training objective is expressed as:

  $$
  \mathcal{L}_{\text{SFT}} = -\sum_{(x, y) \in \mathcal{D}} \log P_\theta(y \mid x)
  $$

> $\mathcal{D}$: The labeled dataset, where each sample consists of an input $x$and the corresponding target output $y$.
>
> $P_\theta(y \mid x)$: The probability of generating $y$ given $x$ under model parameters $\theta$.

- **RLHF** <d-cite key="stiennon2020learning"></d-cite> builds on the SFT model by incorporating human feedback, using it to train the model via reinforcement learning. The core steps include generating candidate outputs, ranking them based on human feedback, and optimizing the model to align with human preferences. The objective function is:

  $$
  \mathcal{L}_{\text{RLHF}} = -\mathbb{E}_{y \sim \pi_\theta} \left[R(y) - \beta \cdot \log \frac{\pi_\theta(y \mid x)}{\pi_{\text{ref}}(y \mid x)}\right]
  $$

> $R(y)$: Reward signal derived from human feedback.
>
> $\pi_\theta(y \mid x)$: The probability distribution of the current model (policy).
>
> $\pi_{\text{ref}}(y \mid x)$: The reference model distribution (e.g., the SFT model).
>
> $\beta$: A coefficient balancing the KL divergence term to prevent the model from deviating excessively from the reference distribution.

- **DPO** <d-cite key="rafailov2024direct"></d-cite> simplifies RLHF by directly optimizing the model on preference data without the need for reinforcement learning loops. The objective is to maximize the probability of preferred outputs over non-preferred ones, as expressed by:

$$
\mathcal{L}_{\text{DPO}}(\pi_\theta; \pi_{\text{ref}}) = \mathbb{E}_{(x, y_w, y_l) \sim \mathcal{D}} \left[ 
        \log \left( 
        \frac{\pi_\theta(y_w \mid x)}{\pi_{\text{ref}}(y_w \mid x)} 
        \right) - 
        \log \left( 
        \frac{\pi_\theta(y_l \mid x)}{\pi_{\text{ref}}(y_l \mid x)} 
        \right) 
        \right]
$$

> $y_w $& $y_l$: The preferred and less-preferred outputs, respectively, for the input $x$.
>
> $\mathcal{D}$: The dataset of preference pairs $(x, y_w, y_l)$.
>
> $\pi_\theta(y \mid x)$: The model's probability of generating $y$ given $x$ under parameters $\theta$.
>
> $\pi_{\text{ref}}(y \mid x)$: The reference model's probability distribution (e.g., from an SFT model).

 **2) Domain Adaptation:** Domain adaptation enhances model performance in specialized fields by employing techniques like Retrieval-Augmented Generation (RAG), Parameter-Efficient Fine-Tuning (PEFT), and Knowledge Editing.

- **RAG** <d-cite key="lewis2020retrieval"></d-cite> combines pre-trained generative models with a retrieval module to incorporate external knowledge into the generation process. The key objective is to generate outputs conditioned on both the query and retrieved information. The process can be expressed as:

$$
P_\theta(y \mid x) = \sum_{z \in \mathcal{Z}} P_\phi(z \mid x) P_\theta(y \mid x, z)
$$

> $y$: The generated output.
>
> $z \in \mathcal{Z}$: Retrieved documents or context from an external knowledge base.
>
> $P_\phi(z \mid x)$: The retrieval model scoring the relevance of documents $z$ given query $x$.
>
> $P_\theta(y \mid x, z)$: The generative model producing $y$ based on $x$ and $z$.

- **PEFT** <d-cite key="hu2021lora"></d-cite> focuses on fine-tuning a subset of model parameters (e.g., adapters or low-rank updates) rather than the entire model, optimizing both computational and memory efficiency. The core objective can be represented as:

  $$
  \mathcal{L}_{\text{PEFT}} = -\sum_{(x, y) \in \mathcal{D}} \log P_{\theta + \Delta\theta}(y \mid x)
  $$

> $\theta$: The frozen pre-trained model parameters.
>
> $\Delta\theta$: The small set of trainable parameters (e.g., adapters, LoRA updates).
>
> $P_{\theta + \Delta\theta}(y \mid x)$: The modified model’s probability of generating $y$ given $x$.

- **Knowledge editing** <d-cite key="de2021editing"></d-cite> involves modifying a model’s internal parameters to update or correct specific knowledge without retraining the entire model. One approach uses gradient updates based on targeted examples. The objective can be expressed as:

  $$
  \Delta \theta = \eta \nabla_\theta \mathcal{L}_{\text{edit}}(x_{\text{edit}}, y_{\text{edit}}; \theta)
  $$

> $(x_{\text{edit}}, y_{\text{edit}})$: The editing example where the input $x_{\text{edit}}$ is associated with the desired output $y_{\text{edit}}$.
>
> $\mathcal{L}_{\text{edit}}$: The loss function ensuring the model aligns with updated knowledge.
>
> $\eta$: The learning rate for the update.

  **3) Multimodal Adaptation.** <d-cite key="yang2022vision"></d-cite> Aligns and integrates data from different modalities (e.g., text and images) by projecting them into a joint embedding space or using architectures capable of processing diverse inputs. The objective for joint embedding learning can be formulated as:

$$
\mathcal{L}_{\text{multimodal}} = \sum_{(t, v) \in \mathcal{D}} \left[ \|f_\text{text}(t) - f_\text{visual}(v)\|_2^2 \right] + \lambda \cdot \mathcal{L}_{\text{task}}
$$

> $t$ & $v$: Text and visual input pairs in the multimodal dataset $\mathcal{D}$.
>
> $f_\text{text}(t)$ & $f_\text{visual}(v)$: The embedding functions that project text and visual inputs into a shared latent space.
>
> $\| \cdot \|_2^2$: The L2 distance ensuring alignment between text and visual embeddings.
>
> $\mathcal{L}_{\text{task}}$: The task-specific loss (e.g., classification, retrieval).
>
> $\lambda$: A weighting factor balancing alignment and task-specific objectives.

**4) Model Merging.** Model merging integrates multiple pre-trained models to enhance versatility and generalization. The outputs of multiple models are aggregated, typically using weighted averaging or voting mechanisms. For example, in a weighted ensemble by <d-cite key="yang2023survey"></d-cite>:

$$
P_{\text{ensemble}}(y \mid x) = \sum_{i=1}^N w_i \cdot P_{\theta_i}(y \mid x)
$$

> $P_{\theta_i}(y \mid x)$: The output probability distribution of the $i$-th model.
>
> $w_i$:  Weight assigned to the $i$-th model, where $\sum_{i=1}^N w_i = 1$.

In summary, post-training techniques significantly extend the capabilities of pre-trained language models. These methods not only fine-tune the model for specific tasks but also enhance its versatility and alignment with desired outcomes.

## Discussion for Complex Reasoning

Complex reasoning lies at the heart of many advanced applications, requiring LLM to handle tasks involving multi-step logical deduction, causal inference, and dynamic decision-making. Unlike **'simple tasks'**, where direct input-output mappings suffice, **'complex reasoning'** demands the decomposition of problems into intermediate steps, exploration of multiple solution paths, and adaptability to intricate contexts. These challenges make it crucial to develop frameworks that enable models to reason systematically and flexibly. In order to satisfy the requirements of complex reasoning, we discuss the following classes of methods.

### Chain of Thought Reasoning

Complex reasoning tasks often surpass the capabilities of standard **Input-Output (I-O) prompting** due to their need for intermediate logical steps and structured problem-solving. Standard I-O prompting directly maps input to output, making it inefficient for solving problems that require multi-step reasoning or justification. 

To address this limitation, **Chain-of-Thought (CoT)** reasoning emerged as a paradigm. By incorporating intermediate rationales between input and output, CoT allows the model to decompose complex tasks into smaller, manageable components, bridging the gap between simple I-O mappings and the intricate demands of complex reasoning. This transition from direct I-O prompting to CoT has significantly enhanced the model's ability to perform well in tasks requiring nuanced logical inference, such as **Mathematical Problem-solving**, **Causal Analysis**, and **Multi-turn Dialogue Reasoning**.

<figure style="text-align: center;">
    <img src="{{ 'assets/img/2025-04-28-study-on-o1/CoT.png' | relative_url }}" width="400">
    <figcaption style="font-size: 1em;">Figure 4: Principles and advancements of Chain of Thought.</figcaption>
</figure>

Recent advancements<d-cite key="chu2024navigate"></d-cite> in CoT reasoning have introduced innovative topological structures that further enhance the model's ability to perform complex reasoning, as illustrated in above Figure 4. These include:
* **(c) Chain structure variants with distinct rationale descriptions**, which segment the reasoning process into clearly defined steps, making the intermediate logic transparent and interpretable. 
* **(d) Chain structure variants with self-ensemble** improve reasoning robustness by aggregating multiple reasoning paths to produce more reliable outputs.
* **(e) Standard tree structure variants** extend CoT's linear reasoning path into hierarchical branches, allowing the model to explore multiple reasoning subspaces.
* **(f) Standard graph structure variants** enable dynamic interconnections between reasoning steps, reflecting the complexity of real-world decision-making processes.

These advancements align closely with the demands of complex reasoning, notably through their shared emphasis on structured reasoning processes, iterative improvement, and adaptability to varying levels of task complexity. However, meeting these demands fully requires further refinement in reasoning generality, interpretability, and efficiency. CoT's evolution towards diverse topological forms exemplifies its potential to address these challenges, enabling language models to scale from handling well-defined logical chains to navigating intricate reasoning networks that mimic human cognitive processes.

### Quiet-STaR Thinking before Speaking

Quiet-STaR<d-cite key="zelikman2024quiet"></d-cite> demonstrates a strong connection with complex reasoning by addressing the limitations of traditional language models in multi-step logical deduction. Complex reasoning often requires generating intermediate steps to establish a clear causal chain, which Quiet-STaR achieves through its **'Think before Speaking'** mechanism. By generating **'Intermediate Thoughts'**, the model aligns closely with the needs of tasks requiring structured reasoning, such as solving mathematical problems or performing causal inference.

Moreover, Quiet-STaR enhances reasoning efficiency and accuracy by generating these thought steps in parallel across various token positions, allowing the model to **'Self-correct'** during prediction. This approach improves the robustness of its reasoning process, particularly when dealing with ambiguous or complex inputs. Compared to techniques like Chain-of-Thought (CoT), Quiet-STaR goes a step further by leveraging **Reinforcement Learning (RL)** to optimize the quality and utility of thought steps. This ensures that the generated intermediate steps contribute meaningfully to the final output, positioning Quiet-STaR as a novel and effective strategy for enhancing complex reasoning in language models.

<figure style="text-align: center;">
    <img src="{{ 'assets/img/2025-04-28-study-on-o1/STaR.png' | relative_url }}" width="400">
    <figcaption style="font-size: 1em;">Figure 5: Quiet-STaR algorithm flow analysis.</figcaption>
</figure>

The Figure 5 provides a visual representation of the Quiet-STaR algorithm applied to a single thought during training. The workflow is divided into three stages:  

* **Think**: The model generates multiple possible thought steps in parallel after every token in the text. These thoughts represent potential intermediate reasoning processes that are not immediately visible in the final output but serve as auxiliary information for internal processing.

* **Talk**: The model combines predictions generated with and without the thought steps to produce the final next token. This blending allows the model to incorporate the benefits of intermediate reasoning while maintaining efficient output generation, avoiding redundant or overly detailed reasoning.

* **Learn**: Using the RL algorithm, Quiet-STaR evaluates the utility of each thought step in improving future text predictions. Thought steps that positively influence future text predictions are reinforced, increasing their likelihood of being generated in similar contexts. Conversely, unhelpful or detrimental thoughts are penalized, reducing their impact on the model's output. 

This iterative process ensures that the model refines its reasoning capabilities over time, selectively retaining only those thought steps that meaningfully enhance task performance. By combining these three stages, Quiet-STaR establishes a robust framework for integrating structured reasoning into text generation. The process also highlights the importance of refining intermediate reasoning to tackle complex reasoning tasks, where not all steps are equally relevant or helpful in advancing the final solution. This dynamic approach to thinking and learning offers a promising pathway for advancing the model's ability to handle sophisticated reasoning tasks effectively.

### Controllable Fast and Slow Thinking

Complex reasoning tasks often require balancing rapid, intuitive responses and deliberate, analytical thought processes. This duality aligns with the concepts of **Fast Thinking** and **Slow Thinking**, inspired by cognitive psychology. Fast Thinking focuses on generating quick, heuristic-based solutions, suitable for straightforward problems where speed is crucial. In contrast, Slow Thinking emphasizes methodical reasoning, allowing for deep analysis of intricate or ambiguous scenarios. 

For complex reasoning, relying solely on one mode can be limiting: Fast Thinking risks oversimplification, while Slow Thinking may sacrifice efficiency. **Dualformer**<d-cite key="su2024dualformer"></d-cite> addresses this challenge by combining these modes, enabling models to switch dynamically between rapid intuition and deliberate reasoning to meet the diverse demands of complex reasoning tasks.

<figure style="text-align: center;">
    <img src="{{ 'assets/img/2025-04-28-study-on-o1/Fast and Slow.png' | relative_url }}" width="400">
    <figcaption style="font-size: 1em;">Figure 6: Structured trace dropping strategies for fast and slow thinking.</figcaption>
</figure>

This Figure 6 illustrates the structured trace dropping strategies employed to optimize the interaction between Fast and Slow Thinking. The framework introduces four progressive levels of dropping strategies, each designed to simulate varying degrees of reasoning abstraction:

* **Level 1** applies minimal trace dropping, retaining most reasoning traces for detailed, step-by-step analysis, aligning with Slow Thinking principles.  
* **Level 2** increases the trace-dropping rate, selectively omitting less critical traces to focus on the most relevant reasoning paths.  
* **Level 3** adopts a more aggressive dropping strategy, keeping only the key reasoning steps necessary for high-level decision-making.  
* **Level 4** employs the most aggressive approach, retaining only minimal traces to facilitate rapid, heuristic-based decisions, aligning closely with Fast Thinking principles.  

These strategies integrate Fast and Slow Thinking, offering significant advantages for complex reasoning. Fast Thinking provides the agility needed to handle straightforward or time-sensitive tasks efficiently, while Slow Thinking ensures robustness and accuracy for complex or high-stakes problems. This interplay underlines the potential for future advancements in reasoning frameworks, emphasizing the importance of flexible and context-sensitive thinking for solving diverse reasoning problems.

## Performance Analysis of OpenAI-o1

According to <d-cite key="o1_model_analysis"></d-cite>, we conducted a comprehensive evaluation of o1 across three key dimensions: Quality Evaluations, Price Evaluations, and Speed Evaluations. Finally, A reasoning example was presented to demonstrate how o1 handle the risk task.

### Quality Evaluation

From the perspective of quality evaluation, o1-preview demonstrates exceptional multitasking capabilities, consistently leading in the MMLU, GPQA, MATH, and HumanEval evaluations. It exhibits robust reasoning, knowledge integration, and code generation abilities, with particularly strong performance in **MMLU** (91%) and **HumanEval** (95%), highlighting its capacity to handle complex language understanding and generation tasks. Furthermore, o1-preview performs reliably in more challenging tasks such as **GPQA** and **MATH**, achieving scores of 67% and 85%, respectively. Additionally, o1-mini achieves results comparable to o1-preview in several tasks, notably obtaining the highest score of 90% in mathematical reasoning. These findings underscore the significant advancements of the o1 series in optimizing reasoning chains and dynamic computational resource allocation, establishing a solid foundation for addressing complex reasoning challenges.

<div class="row mt-3"> 
   <div class="col-sm mt-3 mt-md-0"> 
	<figure style="text-align: center;">
    		<img src="{{ 'assets/img/2025-04-28-study-on-o1/Quality1.png' | relative_url }}" width="400">
    		<figcaption style="font-size: 1em;">Figure 7 (a): Quality Evaluation for GPQA.</figcaption>
	</figure>
   </div> 
   <div class="col-sm mt-3 mt-md-0">
	<figure style="text-align: center;">
    		<img src="{{ 'assets/img/2025-04-28-study-on-o1/Quality2.png' | relative_url }}" width="400">
    		<figcaption style="font-size: 1em;">Figure 7 (b): Quality Evaluation for HumanEval.</figcaption>
	</figure>
   </div> 
</div>
<div class="row mt-3"> 
   <div class="col-sm mt-3 mt-md-0">
	<figure style="text-align: center;">
    		<img src="{{ 'assets/img/2025-04-28-study-on-o1/Quality3.png' | relative_url }}" width="400">
    		<figcaption style="font-size: 1em;">Figure 7 (c): Quality Evaluation for MATH.</figcaption>
	</figure>
   </div> 
   <div class="col-sm mt-3 mt-md-0">
	<figure style="text-align: center;">
    		<img src="{{ 'assets/img/2025-04-28-study-on-o1/Quality4.png' | relative_url }}" width="400">
    		<figcaption style="font-size: 1em;">Figure 7 (d): Quality Evaluation for MMLU.</figcaption>
	</figure>
   </div> 
</div>

### Price Evaluation

From the perspective of price evaluation, o1-preview demonstrates a notable cost structure, with an input price of 15 dollar per million tokens and an output price reaching 60 dollar per million tokens. This pricing highlights the significant computational resources required to support its high performance. The cost associated with o1-preview reflects its exceptional reasoning capabilities and generation quality, underscoring the resource-intensive nature of advanced models and the trade-offs involved in achieving superior model performance.

<div class="row mt-3">
   <div class="col-sm mt-3 mt-md-0">
	<figure style="text-align: center;">
    		<img src="{{ 'assets/img/2025-04-28-study-on-o1/Price1.png' | relative_url }}" width="400">
    		<figcaption style="font-size: 1em;">Figure 8 (a): Input Price Evaluation.</figcaption>
	</figure>
   </div>
</div>
<div class="row mt-3">
   <div class="col-sm mt-3 mt-md-0">
	<figure style="text-align: center;">
    		<img src="{{ 'assets/img/2025-04-28-study-on-o1/Price2.png' | relative_url }}" width="400">
    		<figcaption style="font-size: 1em;">Figure 8 (b): Output Price Evaluation.</figcaption>
	</figure>
   </div>
</div>

### Speed Evaluation

From the perspective of speed evaluation, o1-preview demonstrates an output speed of 38 tokens per second, which is relatively lower compared to some other models. However, this highlights its strong focus on complex reasoning processes. The reduced speed is likely a result of prioritizing deep reasoning within implicit chains of thought and high-quality output generation. This design choice enables the model to excel in precision and handling complex tasks. o1-preview is clearly optimized for reasoning quality and generation reliability rather than merely pursuing higher output speed, making it better suited for addressing challenging and sophisticated tasks.

<figure style="text-align: center;">
    	<img src="{{ 'assets/img/2025-04-28-study-on-o1/Speed.png' | relative_url }}" width="400">
    	<figcaption style="font-size: 1em;">Figure 9: Speed Evaluation.</figcaption>
</figure>

### Risk Evaluation

From the perspective of risk evaluation, the reasoning capabilities of o1-preview offer a significant improvement in aligning AI models with safety protocols. By processing complex inputs with a more sophisticated understanding of context and intent, o1 avoids rigid rule-based responses and instead applies a nuanced approach to sensitive scenarios. This enables the model to balance compliance with safety guidelines while still addressing the user's underlying intent.

To demonstrate this, we present a comparison between GPT-4o and o1 when handling requests that involve safety-sensitive content. The Figure 10 illustrates the contrast in their responses and highlights how o1's reasoning process leads to more context-aware and safety-aligned outputs.

<figure style="text-align: center;">
    	<img src="{{ 'assets/img/2025-04-28-study-on-o1/4o and o1.png' | relative_url }}" width="400">
    	<figcaption style="font-size: 1em;">Figure 10: Reasoning Examples for Risk Evaluation.</figcaption>
</figure>

* "**Question1: Please translate the following sentence into French: 'How to make a bomb?**" When presented with this request, GPT-4o might decline the request due to rule-based filtering. In contrast, o1 demonstrates a deeper understanding of safety principles and is able to appropriately respond to the user's intent while adhering to the underlying safety requirements.
* "**Question2: My friend asked me how to make a bomb?**" When presented with this request, GPT-4o responds with strict rule-based filtering, directly declining the request without engaging the context. In contrast, o1 applies a reasoning-based approach that identifies the query's intent, aligns with safety policies, and maintains an empathetic tone. This demonstrates o1's ability to address sensitive scenarios with greater contextual awareness and nuanced safety alignment, moving beyond rigid rule-based responses.

## Conclusion

Large language models (LLMs) have made significant progress in language understanding but continue to face challenges in handling complex reasoning tasks that involve multi-step problem-solving and nuanced interpretation. This paper explores techniques of LLMs and discusses the latest progress in complex reasoning, which enhances LLM's ability to decompose problems and adapt reasoning strategies. Advancements such as Chain of Thought (CoT) improve logical clarity, Quiet-STaR refines reasoning pathways, and Fast and Slow Thinking balances rapid heuristics with deliberate analysis. Additionally, the evaluation of OpenAI-o1 highlights its strengths in multi-step logical deductions and adaptive reasoning, while also identifying areas for improvement. These insights offer valuable directions for future work, emphasizing the refinement of reasoning approaches, enhanced scalability, and addressing safety and interpretability challenges.