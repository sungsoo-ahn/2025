---
layout: distill
title: 'In Search of the Engram in LLMs: A Neuroscience Perspective on the Memory Functions in AI Models'
description: Large Language Models (LLMs) offer significant benefits but also pose risks like misinformation and privacy violations, highlighting the need to understand how they process and store information. To address this, we examine two recent LLM studies through a neuroscience-inspired perspective, particularly borrowing the concept of engrams, which are physical traces or structures in the brain that form specific memories. Through these efforts, we aim to explore the potential for synergy between AI research and neuroscience, as both fields are studying complex intelligent systems.
date: 2025-04-28
future: true
htmlwidgets: true
hidden: false

# Anonymize when submitting
authors:
  - name: Anonymous

# must be the exact same name as your blogpost
bibliography: 2025-04-28-engram.bib  

# Add a table of contents to your post.
#   - make sure that TOC names match the actual section names
#     for hyperlinks within the post to work correctly. 
#   - please use this format rather than manually creating a markdown table of contents.
toc:
  - name: Introduction
  - name: 'What is "Engram"?'
  - name: Explaining AI Research Through the Concept of Engrams
    subsections:
    - name: '1. Grafting: Task-specific skill localization in LLMs'
    - name: '2. ROME: Targeted Memory Editing in LLMs'
  - name: "Concluding Remarks: Let's connect AI research with neuroscience!"

# Below is an example of injecting additional post-specific styles.
# This is used in the 'Layouts' section of this post.
# If you use this post as a template, delete this _styles block.
_styles: >
  .fake-img {
    background: #bbb;
    border: 1px solid rgba(0, 0, 0, 0.1);
    box-shadow: 0 0px 4px rgba(0, 0, 0, 0.1);
    margin-bottom: 12px;
  }
  .fake-img p {
    font-family: monospace;
    color: white;
    text-align: left;
    margin: 12px 0;
    text-align: center;
    font-size: 16px;
  }
---


## Introduction

Large Language Models (LLMs) are increasingly prevalent, assisting with information retrieval, answering questions, and generating content. However, they’re a “double-edged sword”: while capable of remarkable feats, they also risk spreading misinformation, revealing sensitive data, or generating harmful content. Unlike traditional computer systems, where data storage is transparent and traceable, LLMs function more like “black boxes,” making it nearly impossible to pinpoint where specific information is stored and therefore challenging to edit. To effectively and efficiently edit models in this context, we need a clearer understanding of how memory is stored within these models and develop methodologies that allow for targeted editing.

In the field of neuroscience, which also studies highly complex systems, the biological brain, researchers have made great strides in understanding how memory is formed and stored. In neuroscience, memory storage occurs through physical traces in the brain called **engrams**, networks of neurons and synapses that “record” information after learning or an event. This process strengthens the connections between neurons that fire together based on synaptic plasticity, creating a lasting representation of that memory or skill<d-cite key="schacter1978richard, josselyn2015finding, liu2012optogenetic"></d-cite>. But what if something analogous to engrams existed in LLMs?

If we start thinking about how to employ engrams in large language models, we open up the potential to draw deep insights from neuroscience’s experimental findings and well-established theories. In this blog post, we aim to discuss three key points:
 
1. How engrams are formulated in neuroscience.
2. Two interesting works that seemingly track the engrams (memory locations) in LLMs.
3. The potential for LLM research and neuroscience to inspire one another.

## What is "Engram"?

<div id="fig_engram_concept">
    <div class="row mt-3">
        <div class="col-sm mt-3 mt-md-0">
            {% include figure.html path="assets/img/2025-04-28-engram/engram_concept_v3.png" class="img-fluid"%}
        </div>
    </div>
    <div class="caption">
    Figure 1. A schematic diagram illustrating the four stages of the memory system and the concept of engrams in neuroscience. During memory formation, external stimuli (e.g., the "SUN") activate specific neuronal circuits, leading to the formation of memory engrams (red connections) through strengthened synaptic connections. In the retrieval stage, these engrams are reactivated to recall the stored information (e.g., "MOON") via corresponding neuronal circuits (yellow connections). Memory modification occurs when previously encoded engrams are updated or altered, incorporating new synaptic changes (blue dashed connections). Finally, in memory forgetting, synaptic weakening or suppression leads to the degradation of engrams, resulting in the inability to recall specific information (e.g., "?"). The central inset highlights the engram as a physical substrate of memory, represented by patterns of neuronal and synaptic ensembles.
    </div>
</div>

The concept of **engram**, coined by Richard Semon over a century ago<d-cite key="schacter1978richard"></d-cite>, suggests that each memory leaves a lasting mark within our neural networks, like a footprint. Semon introduced the term "engram" in his early 20th-century writings to describe the physical substrate of memory. He proposed that experiences leave persistent changes in the brain's structure, enabling them to be recalled later—a revolutionary idea that laid the foundation for modern memory research. Neuroscientists today define an engram as the changes occurring within neurons, synapses, and neural circuits that store a specific memory, allowing it to be reactivated and recalled later<d-cite key="josselyn2020memory"></d-cite>.

Before we talk about the engram, what is memory? Memories follow a dynamic life cycle that involves four key stages: formation, retrieval, modification, and forgetting ([Fig. 1](#fig_engram_concept))<d-cite key="josselyn2015finding,guskjolen2023engram"></d-cite>. During formation, external stimuli trigger activity in neural circuits, strengthening synaptic connections through mechanisms like long-term potentiation (LTP) to create an engram<d-cite key="bliss1993synaptic, choi2018interregional"></d-cite>. Retrieval involves the reactivation of these neural circuits, enabling the recall of stored information<d-cite key="kim2014memory, khalaf2018reactivation"></d-cite>. In the stage of modification, previously encoded memories can be updated or altered through processes such as reconsolidation, which makes memory both stable and adaptable<d-cite key="schiller2010preventing, monfils2009extinction, ramirez2013creating"></d-cite>. Finally, forgetting serves as a crucial mechanism to prune unnecessary or outdated memories, often driven by synaptic weakening or neural activity suppression<d-cite key="ryan2022forgetting, Wimber2015RetrievalIA"></d-cite>. These stages highlight the intricate, dynamic processes underpinning memory, with engrams serving as ensembles or patterns of neurons and synapses that represent the physical foundation of specific memories.

The search for engrams has long captivated neuroscience, beginning with the groundbreaking work of Karl Lashley. In his influential 1950 lecture *In Search of the Engram*<d-cite key="lashley1950search"></d-cite>, Lashley sought to uncover the physical basis of memory by conducting experiments on animals trained to navigate mazes. By systematically removing or damaging parts of their brains, he aimed to identify specific regions responsible for memory storage. Surprisingly, his findings led to the principles of mass action and equipotentiality, which revealed that memory is not localized to a single region but distributed across the brain. While Lashley ultimately failed to locate the elusive engram, his pioneering work laid the foundation for modern neuroscience, shaping our understanding of memory as a dynamic and distributed process.

Recent advances in neuroscience have made it possible to identify and even manipulate these engrams. Using optogenetics and advanced imaging techniques, researchers have successfully mapped the neural circuits associated with specific memories in animal models<d-cite key="han2009selective, liu2012optogenetic"></d-cite>. They’ve found that certain groups of neurons are activated and modified during learning, creating a network that represents the memory<d-cite key="abdou2018synapse, choi2018interregional"></d-cite>. This network of neurons can be reactivated to “recall” the memory, and, interestingly, even artificially activating these neurons can bring about a conscious recollection of the memory in the brain<d-cite key="roy2016memory"></d-cite>.

The engram concept has provided valuable insights into how memories are formed, retrieved, modified, and forgotten ([Fig. 1](#fig_engram_concept)), and it highlights the physical reality of memory as a process deeply rooted in the brain’s structure<d-cite key="josselyn2015finding, guskjolen2023engram"></d-cite>. Yet, it’s important to distinguish this from the broader idea of memory, which refers to the actual information or experience we recall—not necessarily the specific neurons that store it.

This engram-based understanding could offer a model for artificial systems as well. If we could pinpoint specific “engrams” within a neural network or LLM, we might be able to selectively edit or delete certain memories in AI, similar to how neuroscientists can activate or deactivate specific memories in the brain. This cross-disciplinary exploration could open up new possibilities for making AI models more transparent, safe, and adaptable.

## Explaining AI Research Through the Concept of Engrams

We have explored what engrams are, and now we can examine LLM research through this lens. Here, we introduce two research efforts to uncover what appear to be engrams in LLMs. The first is **Grafting**, a method that identifies and grafts specific parameter regions crucial for a newly learned task. The second study is **ROME** (Rank-One Model Editing), which aims to introduce targeted updates in LLMs without retraining the entire model. We note that, although the original studies do not explicitly reference engrams, we are the first to adopt this concept to reinterpret and analyze their findings.

### 1. Grafting: Task-specific skill localization in LLMs

<div id="graft_concept">
    <div class="row mt-3">
        <div class="col-sm mt-3 mt-md-0">
            {% include figure.html path="assets/img/2025-04-28-engram/graft_concept_v2.png" class="img-fluid" %}
        </div>
    </div>
  <div class="caption">
  Figure 2. A conceptual figure of model grafting on a neuroscience perspective
  </div>
</div>


Panigrahi et al.(2023)<d-cite key="panigrahi2023task"></d-cite> explored how the weights in a pretrained language model are modified after it’s finetuned for a specific task. Remarkably, they found that the performance improvement in the fine-tuned model can be achieved by grafting only a tiny fraction of the finetuned model’s parameters to the pretrained model. This suggests that this tiny subset of parameters essentially encodes the core of the newly acquired skill, capturing the essential information needed to perform the specific task. We might even consider this subset of weight parameters as a synaptic engram for the skill of the finetuning task ([Fig. 2](#graft_concept)).

In neuroscience, an intriguing parallel can be drawn to the work of Hayashi-Takagi et al. (2015)<d-cite key="hayashitakagi2015labeling"></d-cite>, which investigated how synaptic clusters in the motor cortex encode learning-specific changes. Using an optogenetic tool, the researchers labeled synaptic spines that had been recently potentiated during the acquisition of a motor skill. When they selectively reduced the size of these tagged spines, the motor skill was disrupted, while manipulating non-tagged spines had no effect. This demonstrated that the learned task's memory was stored in a dense, task-specific cluster of modified synapses, referred to as a synaptic ensemble or engram. This discovery parallels the concept of model grafting in AI, where a small, localized region of modified parameters is responsible for encoding a new, task-specific ability.

However, the biggest difference between research from the two domains lies in the methods to identify and label the “synaptic engram.” In the model grafting process within the AI domain, the researchers directly computed the engram through optimization.

**Finding synaptic engrams in LLMs**

To localize the weight parameters or 'engram' that have an important contribution to the performance of the fine-tuned task (e.g., sentiment analysis), Panigrahi et al.(2023)<d-cite key="panigrahi2023task"></d-cite> grafted the small region of the fine-tuned model $$\theta_{\textrm{ft}}$$ to a pretrained model $$\theta_{\textrm{pre}}$$ using a binary mask $$\gamma \in \{0,1\}^{\lvert\theta_{\textrm{ft}}\rvert}$$, which identifies a subset of parameters. They then evaluated the performance of the resulting grafted model.



<div id="graft_org">
    <div class="row mt-3">
        <div class="col-sm mt-3 mt-md-0">
            {% include figure.html path="assets/img/2025-04-28-engram/graft_org.png" class="img-fluid" %}
        </div>
    </div>
  <div class="caption">
Figure 3. An illustration of model grafting process. Source: <i>A. Panigrahi et al., Task-specific skill localization in fine-tuned language models, 2023.</i>
</div>
</div>


The grafted model is defined as:

$$
\overline{\theta_{\textrm{ft}}}(\gamma) = \gamma \odot \theta_{\textrm{ft}} + (1-\gamma) \odot \theta_{\textrm{pre}}
$$

where $$\odot$$ denotes element-wise multiplication. [Figure 3](#graft_org) illustrates this equation. 

So if the grafted model $$\overline{\theta_{\textrm{ft}}}(\gamma)$$ has a similar performance to the fine-tuned model $$\theta_{\textrm{ft}}$$, we can infer that the region in $$\theta_{\textrm{ft}}$$, corresponding to $$\gamma$$ is the engram responsible for the skill for the fine-tuned task.

To find this engram, they directly optimized the mask $$\gamma$$ to minimize the task loss for the grafted model. Formally, this can be expressed as:

$$
\underset{\gamma \in \{0, 1\}^{|\theta_{\textrm{ft}}|} : \|\gamma\|_{0} \le s}{\operatorname{argmax}} \mathcal{L}_{\mathcal{T}}(\gamma \odot \theta_{\textrm{ft}} + (1 - \gamma) \odot \theta_{\textrm{pre}})
$$

where $$\mathcal{L}_{\mathcal{T}}$$ is a metric for performance on the Task $$\mathcal{T}$$ (e.g., classification error), and $$s$$ is a sparsity constraint on the mask $$\gamma$$.

To enable optimization through gradient descent, they reparameterized the mask $$\gamma$$ as a differentiable variable using a sigmoid function applied to a real-valued vector $$S$$, i.e., $$\gamma = \sigma(S)$$. 

Using this optimization objective directly, they successfully identified task-specific skill engrams. By grafting just 0.01% of $$\theta_{\textrm{ft}}$$ (the identified engram) onto $$\theta_{\textrm{pre}}$$, $$\overline{\theta_{\textrm{ft}}}(\gamma)$$ achieved over 95% of the original fine-tuned model's performance. This confirms that the identified engram is indeed responsible for the task-specific skill.

**Leveraging Engrams in LLMs**

In the AI domain, the researchers have demonstrated how engrams can be identified through direct optimization techniques. Once these task-specific engrams are located, they can be applied to practical use cases, such as continual learning. For example, Panigrahi et al.(2023)<d-cite key="panigrahi2023task"></d-cite> showed that by freezing the engram region responsible for a specific skill and ensuring subsequent tasks are learned in non-overlapping regions, it is possible to achieve "forget-free" continual learning. This approach enables models to retain previous skills while sequentially learning new ones.

Identifying engrams in LLMs opens up a wide range of possibilities, including targeted memory editing. In the following section, we will delve into research that has explored memory editing techniques in LLMs.

### 2. ROME: Targeted Memory Editing in LLMs

Meng et al. (2022)<d-cite key="meng2022locating"></d-cite> introduced a sophisticated, mathematically grounded approach to update specific memory within LLMs. In this work, authors treated memory as a factual association and developed a method for inserting new memory while preserving knowledge that is unrelated to the new memory. A factual association is a link between a subject (entity) and specific information about it, which a model has learned. For example, in the fact “The Eiffel Tower is located in Paris,” the subject is “Eiffel Tower,” and the associated information is “Paris.” The model recognizes this connection as a factual association, so when prompted with “The Eiffel Tower is located in…”, it outputs “Paris” based on this stored knowledge. 

**Finding engram cells**

To edit a specific fact in the model, the authors first identified where that fact, or 'engram,' is stored. Much like neuroscientists reactivating tagged engram cells to probe stored memories<d-cite key="liu2012optogenetic"></d-cite>, they exposed the model to an unrelated context or input prompt to identify where specific factual associations reside. For instance, to trace the memory for 'The Eiffel Tower is located in Paris,' they first recorded the model’s firing patterns of every layer when prompted with this fact. Then, they fed the model different contexts or prompts, such as 'The Colosseum is located in,' and reactivated a specific layer only with the previously stored firing patterns. By doing this on every layer and token position, they identified the layer where the model outputs 'Paris' instead of 'Rome,' in the different context, indicating the presence of the Eiffel Tower’s engram. Their findings reveal that factual association engrams appear to be stored in the multilayer perceptron (MLP) modules of the middle layers, specifically at the position of the final subject token.

**Injecting a False Memory**

Suppose memory A encodes the factual association "The Eiffel Tower is located in Paris," and memory B encodes the fact association "The Colosseum is located in Rome." (See [Fig. 4](#rome) Top) Now, we want to inject a false memory into the model, such as "The Eiffel Tower is located in *Rome*." To do this, we need to identify the specific engram and its corresponding firing pattern. The authors hypothesize that the MLP module can be modeled as linear associative memories. In the output linear layer of an MLP, a vector $$ k $$ serves as the input, and the value vector $$ v = Wk $$, where $$ W $$ represents the linear associative memory.

To determine the firing pattern $$ k_A $$ for engram A, researchers use the prompt "The Eiffel Tower is located in" combined with various prefixed contexts. They calculate the $$ k_A $$ vector at the middle MLP layer where the factual association engram is located by averaging activity patterns generated from inputs with different prefixes, such as “The first thing. \{prompt\}” and “A few days ago. \{prompt\}.” By averaging the responses to these prefixed inputs, they obtain a reliable estimate of the $$ k_A $$ vector that represents the engram's pattern in that layer. The corresponding value vector for this pattern is $$ v_A = Wk_A $$, which leads the model to output "Paris."

Next, to implant the false memory, the model modifies the output neurons $$ v $$ using optimization techniques to create a new vector $$ v_B $$ that leads the model to a false output "Rome." By using gradient descent, $$ v $$ is iteratively adjusted until $$ v_B $$ reliably corresponds to "Rome."

After finding the firing patterns $$ k_A $$ and $$ v_B $$, the authors proposed a rank-one model editing (ROME) method (See [Fig. 4](#rome) bottom). This method produces a weight change matrix, $$ \Delta W $$, which is then added to the original weight matrix, resulting in an updated matrix: $$ \hat{W} = W + \Delta W $$, where

$$
\Delta W = \eta (v_B k_A^{T} - W k_A k_A^{T})C^{-1}
$$

Here, $$ C $$ approximates the covariance of existing key vectors and $$ C^{-1} $$ is the inverse of the covariance matrix, directing the weight update in a way that minimizes changes to unrelated facts, and $$ \eta $$ is a constant.
<div id="rome">
    <div class="row mt-3">
        <div class="col-sm mt-3 mt-md-0">
            {% include figure.html path="assets/img/2025-04-28-engram/rome_v1.png" class="img-fluid" %}
        </div>
    </div>
  <div class="caption">
Figure 4. ROME (Rank-One Model Editing) method for memory editing in language models. The top section shows the original associations: "Eiffel Tower" with "Paris" (Memory A) and "Colosseum" with "Rome" (Memory B). ROME injects a false association, linking "Eiffel Tower" to "Rome." The bottom section illustrates the process: the original association $W_{AA} = v_A k_A^T$ is removed, and a new association  $W_{AB} = v_B k_A^T$ is added. This update produces the edited weight matrix $\hat{W}$, resulting in the model associating "Eiffel Tower" with "Rome" instead of "Paris."
</div>
</div>

In neuroscience, **Hebbian learning** is summarized by "neurons that fire together, wire together." Mathematically, this is often modeled as:

$$
\Delta w_{ij} = \eta x_i y_j
$$

where $$ w_{ij} $$ is the weight of the connection between neuron $$ i $$ and neuron $$ j $$, $$ x_i $$ and $$ y_j $$ are the activations of neurons $$ i $$ and $$ j $$, and $$ \eta $$ is a learning rate.

This rule is similar to the update rule in ROME. 

$$
\Delta w_{ij} \propto [v_B k^{T}_A - W k_A k^{T}_A]_{ij} = (v_B)_i (k_A)_j - (v_A)_i (k_A)_j
$$

In addition to injecting a new association ($$ v_B k_A^{T} $$), ROME also effectively "removes" the old association by subtracting $$ v_A k_A^{T} $$. This process resembles anti-Hebbian unlearning, as it eliminates the previous association to ensure it is fully replaced by the new one. This update rule aligns ROME with the principle of "engram cells that fire together, wire together" <d-cite key="josselyn2020memory"></d-cite>.

The ROME method’s theoretical underpinnings align with biological mechanisms demonstrated in neuroscience experiments. For instance, Ohkawa et al. (2015)<d-cite key="ohkawa2015artificial"></d-cite> explored how artificial associations between unrelated memories could be induced through synchronous activation of neuronal ensembles. Their work showed that hippocampal and amygdala engrams, representing neutral contexts and fear stimuli respectively, could be artificially linked in mice. By optogenetically co-activating these engrams, researchers created an artificial memory where the mice exhibited fear in a previously neutral context.

This mechanism resembles the ROME method’s approach of pairing a specific $$ k_A $$ (representing the “neutral context” of the Eiffel Tower in Paris) with a new $$ v_B $$ (representing the fear-associated memory, or in this case, Rome). Both rely on synchronizing distinct ensembles—whether neurons in the brain or patterns in an artificial model—to generate a qualitatively new association.

The findings from Ohkawa et al.<d-cite key="ohkawa2015artificial"></d-cite> provide a compelling biological analog to ROME’s editing principles, suggesting that artificial systems may mimic the brain’s capacity to update and reshape memory networks by associating novel information with pre-stored information.

This exploration of the underlying mathematical framework reveals that ROME is more than just an engineering technique; it represents a sophisticated application of associative memory principles in AI.

## Concluding Remarks: Let's connect AI research with neuroscience!

We interpreted two LLM studies through the lens of neuroscience concepts, particularly engrams. Interestingly, although the fields of neuroscience and AI have developed independently, LLM research can be reinterpreted using concepts from neuroscience.

LLMs are highly complex systems, making it challenging to understand what happens when they process or handle information and knowledge. Similarly, the brain is an equally complex system. Researchers in both domains may share similar goals: to understand highly intricate "black box" systems. If we can achieve this understanding, we could develop ways to edit or control LLMs to prevent harmful behaviors. In neuroscience, such understanding could contribute to treatments for Alzheimer's disease or other brain-related conditions.

We are among the first to attempt to connect these two streams of research. Furthermore, by bringing together decades of neuroscience insights with cutting-edge AI research, we could uncover new ways to make LLMs more transparent, adaptable, and safe for everyday use. This effort might bring us closer to a future where AI models are as understandable and trustworthy as we hope our own minds can be.
