---
layout: distill
title: Distilling LLMs into MiniLLMs
description: "In this blog post, we propose to investigate a technique to distill LLMs into smaller models. Relying on the paper <em>MiniLLM: Knowledge Distillation of Large Language Models</em>, published in March 2024 by Yuxian Gu et al., we summarize and discuss its key findings. We supplement their work by reproducing their results and exploring its generalizability."
date: 2025-04-28
future: true
htmlwidgets: true
hidden: false

# Anonymize when submitting
# authors:
#   - name: Anonymous

authors:
  - name: Param Damle
    url: "https://app.paramdamle.com/about"
    affiliations:
      name: Carnegie Mellon University
  - name: Clement Ou
    url: "https://clementou.com/"
    affiliations:
      name: Carnegie Mellon University
  - name: Ines Vignal
    url: "https://www.linkedin.com/in/ines-vignal/"
    affiliations:
      name: Carnegie Mellon University

# must be the exact same name as your blogpost
bibliography: 2025-04-28-miniLLM.bib  

# Add a table of contents to your post.
#   - make sure that TOC names match the actual section names
#     for hyperlinks within the post to work correctly. 
#   - please use this format rather than manually creating a markdown table of contents.
toc:
  - name: Background
  - name: Intention
  - name: Comparing Models
  - name: Divergence Feedback in Practice
  - name: Optimizations
  - name: Methods
  - name: Conclusion

# Below is an example of injecting additional post-specific styles.
# This is used in the 'Layouts' section of this post.
# If you use this post as a template, delete this _styles block.
_styles: >
  .fake-img {
    background: #bbb;
    border: 1px solid rgba(0, 0, 0, 0.1);
    box-shadow: 0 0px 4px rgba(0, 0, 0, 0.1);
    margin-bottom: 12px;
  }
  .fake-img p {
    font-family: monospace;
    color: white;
    text-align: left;
    margin: 12px 0;
    text-align: center;
    font-size: 16px;
  }
---

## MiniLLM: Knowledge Distillation of Large Language Models

### Background

**Large Language Models (LLMs)** have witnessed a surge in computational demand as they've grown in size.
<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.html path="assets/img/2025-04-28-miniLLM/fig1.png" class="img-fluid rounded z-depth-1" %}
    </div>
</div>
<div class="caption">
    Statistics of large language models (having a size larger than 10B in this survey) in recent years<d-cite key="Wayne2023Survey"></d-cite>
</div>
Techniques to reduce this computational load include **Knowledge Distillation (KD)**, in which a "smaller" student model is fine-tuned using data or feedback from a larger "teacher" model<d-cite key="Hinton2015Distilling"></d-cite>.

Types of KD include<d-cite key="Gou2021KDSurvey"></d-cite>:
- **Black-box KD**: only exposes the input-output pairs from the teacher to the student. This has shown promising results with prompt-response pairs from prominent LLM APIs, which most researchers can only query external APIs for, without any access to internal state. Prominent examples include Stanford's Alpaca<d-cite key="Alpaca"></d-cite> and Vicuna<d-cite key="Vicuna2023"></d-cite>, both of which use training data generated from queries to major LLMs.
- **White-box KD**: exposes the internals of a teacher model (e.g. hidden state data, a full distribution over predicted outputs) that allows student models to gain more insight into the full decision-making process of the teacher, rather than just its final verdict. The authors argue this method has only been applied to language _understanding_<d-cite key="Sanh2020DistilBERT"></d-cite><d-cite key="Wang2020MiniLM"></d-cite> (tasks that require interpreting pre-generated text) rather than _generation_ (an inherently more creative and open-ended task). <!--TODO: assess if there's really no other similar paper, otherwise a comparison is in order. Ines: I searched and didn't find any-->

### Intention
As LLMs are increasingly used to generate new content and the research community output more open source models, there is an opportunity to train smaller models using white-box knowledge of well-performing generators.
The intention of this paper is to optimize _white-box_ KD for _generative_ tasks. The central focuses include:
1. Distill larger teacher model into a smaller student model.
2. Generate relevant and generalizable responses to a prompt.
3. Disincentivize outputting obscure or nonsensical answers.
4. Maintain truthfulness (relevant to the teacher distribution).
5. Disincentivize brevity in responses.

### Comparing Models
With white-box access to the output distribution of the teacher model, _how well a student model has distilled knowledge_ can be measured by _how closely the student model's output distribution aligns with the teacher's_.

Let's say the teacher LLM's probability distribution over text output, $$p$$, closely resembles that of real documents and human-written text. Now, we want to train the parameters $$\theta$$ of our small student model so it outputs a distribution $$q_\theta$$ to best mimic $$p$$. One standard measure of how different these two distributions are is **(forward) KL Divergence**:

$$
D_{\text{KL}}(p||q_\theta) = \sum_{x\in \mathcal{X}} p(x) \log\left(\frac{p(x)}{q_\theta(x)}\right)
$$

Note the asymmetric definition&mdash;in other words, 

$$
D_{\text{KL}}(p||q_\theta) \neq D_{\text{KL}}(q_\theta||p)
$$

So what does this asymmetry mean in practice?

Say the distribution $$p$$ has many **modes** (the probability mass function looks hilly with several outputs that have a higher probability than most other outputs). An example of this is how an LLM is _far_ more likely to spit out a small set of outputs (sentences that make sense in English) compared to the vast majority of _possible_ outputs (all random sequences of words).

When we try to adjust distribution $$ q_\theta $$ to minimize the divergence $$ D_{\text{KL}}(p||q_\theta) 
$$, we try to get it to increase the probability it assigns to each of $$ p $$'s modes. This may work fine with a small, finite set of modes, such as in an understanding or classification task where we limit the possible set of class labels or insights a model can draw from an input so that we can better evaluate whether it's extracting the _proper_ understanding.

What about text generation? Think about how many modes would exist in a distribution over all possible valid outputs a model could generate given some prompt. Because forward KL wants to boost probabilities in $$ q_\theta $$, it dislikes lowering the probabilities of any output ("**zero-avoiding**"<d-cite key="Malinin2019ReverseKL"></d-cite>), so attempting to lower $$ D_{\text{KL}}(p||q_\theta)
$$ results in a distribution for $$ q_\theta $$ that spreads out across all possible outputs with not much precision for identifying the modes of $$ p $$. This is especially limiting if our $$ q_\theta $$ distribution is output by a student model whose small size limits its capacity to express large, complicated distributions.

In such cases, the authors argue a worthy tradeoff is having a $$q_\theta$$ that boosts the probabilities of _some_ of the modes in $$p$$, and diminishes the probability of all the other outputs, even if there's some other, less significant modes among the noisy garbage outputs that get silenced. The **Reverse KL Divergence** strategy argues that KD should minimize $$ D_{\text{KL}}(q_\theta||p) 
$$ instead. This objective will boost the probabilities in $$q_\theta$$ associated with the biggest modes of $$p$$, focusing on capturing the most frequent outputs and pushing all other probabilities down ("**zero-forcing**"). Diminishing the random, noisy, or garbage output is core to ensuring the output of $$q_\theta$$, and thereby our student model, is free of hallucinations<d-cite key="Huang2023Hallucination"></d-cite> or otherwise nonsensical content.

The differences between the result of minimizing different objectives is shown elegantly in this graph:
<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.html path="assets/img/2025-04-28-miniLLM/fig2.png" class="img-fluid rounded z-depth-1"  %}
    </div>
</div>
<div class="caption">
    Fitting a Gaussian mixture distribution using forward and reverse KLD.<d-cite key="Gu2023MiniLLM"></d-cite>
</div>

But wait&mdash;won't the student model lose the slightly smaller modes? In other words, won't our student model fail to learn more obscure but equally valid responses from our teacher distribution? The authors argue that:
1. They don't care about capturing all, or even multiple, valid responses; simply _one_ correct response from a student model in response to a prompt is sufficient.
2. A student model minimizing reverse KL still achieved linguistically complex and diverse responses, as evidenced by its Dist-4 (4-grams diveristy) score of 99.0, comparable to the teacher model's 99.3 on their test set.
3. A student model minimizing reverse KL is still able to output the full distribution of text from the source distribution, suffering only from a small relative increase in loss 3.95 compared to the teacher's 3.55.

### Divergence Feedback in Practice

If using forward KL Divergence, a **sequence-level KD** approach would take a prompt and feed it through both the student and teacher model. It would keep sampling outputs from the resulting teacher distribution to get an understanding of $$p$$, and extract the raw distribution $$q_\theta$$ from the student to calculate the loss $$ \mathcal{L}(\theta) = D_{\text{KL}}(p||q_\theta) 
$$, then propagate the gradient $$\nabla\mathcal{L}(\theta)$$ back to the student so it can update its model weights $$\theta$$ to lower the divergence on the next iteration.

With white-box access to the teacher model, the **miniLLM approach** can poll the _student_'s output distribution to mimic $$q_\theta$$, the frequency of responses from the student, and compare it with the true value of $$p$$ extracted from the teacher to propagate $$ \nabla\mathcal{L}(\theta) = \nabla D_{\text{KL}}(q_\theta||p) 
$$. In intuitive terms, this means instead of increasing the likelihood the student outputs the full range of samples frequently outputted by the teacher, we aim to train the student to output samples that resonate with the teacher's preference/distribution.

Let's say that we fed in an input prompt $$\boldsymbol{x}$$ and we want to predict the token at position $$t'$$ given the tokens up to that point, $$\boldsymbol{y}_{<t'}$$. Given this token, $$y_{t'}$$ we can examine how well the student model distills knowledge from the teacher model by comparing their probability of outputing this token: 
$$r_{t'} = \log \frac{p(y_{t'} | \boldsymbol{y_{<t'}, \boldsymbol{x}})}{q_\theta (y_{t'} | \boldsymbol{y_{<t'}, \boldsymbol{x}})}$$
Given our chosen tradeoffs, we want $$r$$ to be as close to 0 (probabilities align 1:1) but definitely not negative (we don't want $$q_\theta$$ to assign a larger probability to outputs that were not boosted in $$p$$). We can also calculate this alignment for the specific _sequence_ of tokens after some point $$t$$ (up to the full length of the output, $$T$$) by replacing the single probability with a product of probabilities:
$$R_t = \log\left( \frac
{\prod_{t'=t}^{T} p(y_{t'} | \boldsymbol{y_{<t'}, \boldsymbol{x}}) }
{\prod_{t'=t}^{T} q_\theta (y_{t'} | \boldsymbol{y_{<t'}, \boldsymbol{x}}) } 
\right) = \sum_{t'=t}^{T} r_{t'}$$

For our setup, say we repeatedly sample input prompts $$\boldsymbol{x}$$ from some distribution of input prompts $$p_{\boldsymbol{x}}$$ (labeled as $$p$$ because it is a natural language distribution that should closely mirror the teacher's language distribution of outputs). We then feed this through the student model with its current parameters $$\theta$$ and get an output distribution $$q_\theta(\cdot | \boldsymbol{x})
$$. 

We adapt this thinking into a **Policy Gradient**<d-cite key="Williams1992PolicyGradient"></d-cite> formulation to show the feedback we will give the student model (after giving it an input prompt $$\boldsymbol{x}$$ and receiving an output $$\boldsymbol{y}$$):
$$
\nabla \mathcal{L}(\theta) = - 
  \underset{\substack{
  \boldsymbol{x} \sim p_{\boldsymbol{x}} \\
  \boldsymbol{y} \sim q_\theta(\cdot | \boldsymbol{x}) }}
  {\mathbb{E}}
\sum_{t=1}^T (R_t - 1) \nabla\log q_\theta (y_t | \boldsymbol{y}_{<t} , \boldsymbol{x}) 
$$

Some things to note about this formulation:
- Instead of relying on single samples to calculate our loss, we can calculate the expected value across samples from the input distribution $$p_{\boldsymbol{x}}$$ and the corresponding output distribution $$q_\theta(\cdot | \boldsymbol{x})
$$ for each such sample $$\boldsymbol{x}$$.
- We accumulate the loss over each token in our $$T$$-length output.
- As an high-level example, if $$R_t - 1$$ is very low (very negative), this means probability $$p$$ was much lower than $$q$$ through point $$t$$, and our student is over-producing an output that is insignificant in the teacher's distribution. Multiplied with the negative sign from outside the $$\mathbb{E}$$, this attaches a high positive multiplier so that $$\nabla\mathcal{L}(\theta) \propto \nabla\log q_\theta (y_t | \boldsymbol{y}_{<t} , \boldsymbol{x})
$$. Thus, to minimize loss (and traverse in the direction of $$-\nabla\mathcal{L}(\theta)$$), we would want to decrease $$\log q_\theta (y_t)$$ and thus decrease $$q_\theta (y_t)$$ so it gets closer to $$p (y_t)$$ and is less likely to output the token $$y_t$$ in the given sequence.
- Intuitively, the negative loss gradient $$(-\nabla\mathcal{L}(\theta))$$ is positively correlated with $$p(y_t)$$ because we want to maximize the chance the student outputs tokens $$y_t$$ that the teacher favors, and negatively correlated with $$q_\theta(y_t)$$ because we want to minimize the probabilitiy peaks in $$q_\theta$$ so that the probability mass can be spread out over a diverse array of possible outputs.

### Optimizations

Some important optimizations are necessary to ensure KD training with this feedback structure goes smoothly:
<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.html path="assets/img/2025-04-28-miniLLM/fig3.png" class="img-fluid rounded z-depth-1" %}
    </div>
</div>
<div class="caption">
    The reverse KLD between the teacher and the students during MINILLM training when different optimization strategies are applied.<d-cite key="Gu2023MiniLLM"></d-cite>
</div>

1. **Single-Step Decomposition**: to reduce variance (spikes in the training graph), the authors sought to split the single-step loss variable $$r_t$$ (loss for the generation of token $$t$$), while still keeping the accumulated loss $$R_t$$ (cumulated loss up to token $$t$$) which helps accelerate and smoothen convergence. Indeed, as mistakes made in the early steps of a sequence generation can have a compounding effect on the rest of the generated text, computing $$r_t$$ individually allows the model to correct single-step errors and adjust its parameters accordingly, resulting in more stable and efficient training.<br><br>
$$
\nabla\mathcal{L}(\theta)
= \nabla\mathcal{L}(\theta)_{\text{single}} + \nabla\mathcal{L}(\theta)_{\text{long}}
$$
$$
\quad \quad \quad = -\mathbb{E} 
  \left( \sum_{t=1}^T 
    \nabla\mathbb{E}_{y_t \sim q_\theta(t)} r_t 
  \right)-\mathbb{E} 
  \left( \sum_{t=1}^T 
    R_{t+1} \nabla \log q_\theta (y_t | \boldsymbol{y}_{<t} , \boldsymbol{x}) 
  \right)
$$
<br><br>TODO: for the single term, how is $$\boldsymbol{y}$$ sampled from the student output but also $$y_t$$ is sampled separately in the nested sum? Isn't summing over the full vocabulary inefficient? how big is the vocabulary?


2. **Teacher-Mixed Sampling**: A common problem in training with reinforcement policies is **Reward Hacking**<d-cite key="Skalse2024RewardHacking"></d-cite>, wherein the student attempts to minimize its own loss function, which is just a _proxy_ for the true, abstract mission of just aligning with the natural language distribution however possible. This could lead to the student finding loopholes to minimize its loss by the "letter of the law", rather than the "spirit of the law". In practice, this could mean short, degenerate, or repeated phrases that happen to have a low loss value. To prevent this, rather than evaluating samples $$\boldsymbol{y}$$ drawn purely from the student's (potentially reward-hacking) distribution $$q_\theta$$, we draw them from a weighted combination
$$\tilde{p} = \alpha p + (1-\alpha) q_\theta$$, 
where $$\alpha = 1/5$$. With this modification, we need to adjust the loss propagated to the student so it only gets loss proportional to its share of the final distribution, so we change our formula to add a weighting of $$w_t = \frac{q_\theta (y_t | \boldsymbol{y}_{<t} , \boldsymbol{x})}{\tilde{p} (y_t | \boldsymbol{y}_{<t} , \boldsymbol{x})}$$:<br><br>
$$
\quad \quad \quad \nabla\mathcal{L}(\theta)
= - \underset{\substack{
  \boldsymbol{x} \sim p_{\boldsymbol{x}} \\ 
  \boldsymbol{y} \sim \tilde{p}(\cdot | \boldsymbol{x})}}
  {\mathbb{E}} 
\left( \sum_{t=1}^T w_t 
\nabla\underset{y_t \sim q_\theta(t)}{\mathbb{E}} 
r_t \right)
$$
$$ 
\quad \quad \quad \quad \quad \quad -\underset{\substack{
  \boldsymbol{x} \sim p_{\boldsymbol{x}} \\ 
  \boldsymbol{y} \sim \tilde{p}(\cdot | \boldsymbol{x})}}
  {\mathbb{E}}
\left( \sum_{t=1}^T w_t R_{t+1} \nabla \log q_\theta (y_t | \boldsymbol{y}_{<t} , \boldsymbol{x}) \right)
$$<br><br>
This setup allows the teacher to mix-in known valid responses, suppressing low-quality generation, while ensuring the student gets feedback proportional to its share of the generated output. Without this mixing, the reverse KL loss decreases, but this is usually due to prioritization of short, meaningless, or repeated strings. 

3. **Length Normalization**: Because long output sequences accumulate many $$r_{t'}$$ terms, they tend to have relatively more negative $$R_t$$ (more loss) than a similar quality output of shorter length. To eliminate this length bias, the authors normalize $$R$$ by the number of terms accumulated (by taking the arithmetic mean of $$r_{t'}$$):
$$
R_t^{\text{Norm}} = \frac{1}{T - t - 1} \sum_{t'=t}^{T} r_{t'}
$$
This promotes reducing the _average_ loss per token rather than _total_ loss of all tokens, so that a longer sequence with higher-quality output tokens is more favorable than a short sequence with a few really bad tokens.

4. **Penalizing Repeating Foundational Training**: To start the student model at a sufficient baseline, it was first pre-trained on a large corpus of long documents ($$\mathcal{D}_{\text{PT}}$$) to get the sensibility of language generation. Then, given a teacher model that performs well on a dataset we want to test ($$\mathcal{D}$$), we further fine tune the student model on $$\mathcal{D}$$. <!--TODO: is this a problem in terms of adding confounding variables?--> In the KD iterations, the authors added a feedback element $$\nabla\mathcal{L}_{\text{PT}}$$ to disincentivize the model simply repeating tokens it saw with high frequency in its pre-training, and instead to incentivize the model to generate new content relevant to its new dataset $$\mathcal{D}$$:<br><br>
$$
\nabla\mathcal{L}_{\text{PT}}(\theta) = -\underset{\boldsymbol{d} \sim \mathcal{D}_{\text{PT}}}{\mathbb{E}} \log q_\theta (\boldsymbol{d})
$$
$$
\nabla\mathcal{L}(\theta) = \nabla\mathcal{L}(\theta)_{\text{single}} + \nabla\mathcal{L}(\theta)_{\text{long}}^{\text{Norm}} + \nabla\mathcal{L}_{\text{PT}}(\theta)
$$



### Methods
The resulting student would be trained via the iterative KD loop described previously, with the addition of clipping<d-cite key="Schulman2017Proximal"></d-cite> for added stability. (Clipping is the action of limiting or "clipping" the magnitude of the gradients during training to prevent instability due to overly large updates.)

1. **Teacher/Student Models.**
The authors implemented MiniLLM with multiple models and sizes. The student models are pre-trained on a large long-document corpus $$\mathcal{D}_{\text{PT}}$$, OpenWeb<d-cite key="Gokaslan2019Openwebtext"></d-cite> for GPT2 and RoBERTa<d-cite key="Liu2019RoBERTa"></d-cite> training for other models.
- **GPT-2**<d-cite key="Radford2019GPT2"></d-cite>
  - Teacher size: 1.5B parameters
  - Student sizes: 120M, 340M, 760M parameters
- **OPT**<d-cite key="Zhang2022OPT"></d-cite>
  - Teacher size: 13B parameters
  - Student sizes: 1.3B, 2.7B, 6.7B parameters
- **LLaMa**<d-cite key="Touvron2023LLaMA"></d-cite>
  - Teacher size: 13B parameters
  - Student size: 7B parameters


2. **Comparison Baselines.**
For each student size listed above, comparison points for MiniLLM included equally-sized student models that were trained in the following ways:
- **SFT w/o KD**: Students fine-tuned directly on $$\mathcal{D}$$ without KD, but with the added feedback of **golden responses** (reference responses used as benchmark)
- **KD**: Students trained via word-level feedback from teacher's $$p$$  using forward KLD
- **SeqKD**: Students trained on input-output data generated by a teacher model using forward KLD (for students to better approximate the teacher output distribution rather than just ground truth labels)


3. **Assessement Datasets $$\mathcal{D}$$.**
The student models were assessed on the 5 following datasets:
- **Dolly**  <d-cite key="DatabricksDolly"></d-cite>: natural language tasks including brainstorming, classification, open/closed QA, generation, information extraction, and summarization.
- **SelfInst**  <d-cite key="Wang2023SelfInstruct"></d-cite>: natural language tasks including all of the above and rewriting/paraphrasing, instruction following, multi-turn dialogues, and code understanding/generation.
- **Vicuna** <d-cite key="Chiang2023Vicuna"></d-cite>: a custom set of 80 challenging  natural language tasks.
- **S-NI**  <d-cite key="Wang2022Benchmarking"></d-cite>: a set of natural language tasks aimed at assessing generalization via In-Context instructions.
- **UnNI**  <d-cite key="Honovich2023Unnatural"></d-cite>: natural language tasks including creative writing, text completion, and diverse generative tasks.


4. **Metrics.**
The knowledge-distilled student model was compared to several benchmarks, including:
- **RougeL**: precision for large scale instruction <d-cite key="Lin2004ROUGE"></d-cite>
- **Human judgement**: the authors asked humans to subjectively pick a winner between 2 outputs, one from MiniLLM and another from a comparison point (e.g. the teacher model)
- **GPT-4 evaluation**: the authors fed the outputs from MiniLLM and a comparison point to GPT-4 and asked the bigger model to score both outputs on a range of 1-10 for "helpfulness, relevance, accuracy, and level of detail of their responses".

Results: good generality
most precise alignment with ground truth, better than teacher
Teacher forcing, training inference discrepancy, exposure bias
MiniLLM improves consistently regardless of scale
better human preference



student model may not always proportionally scale with teacher size due to what? <d-cite key="Mirzadeh2019Improved"></d-cite>

Exposure bias due to difference between teacher forcing training and free run generation 
avoided since student sees its own responses in training

Models trained with policy optimization are poorly calibrated <d-cite key="OpenAI2023GPT4"></d-cite>
Test on classification tasks, with zero shot classification instructions

Differences in length show that MiniLLM outperforms regardless of golden length range, with no clear pattern
poor performance in ground truth < 5 tokens due to lack of representation in data
intuition: more tokens = bigger space = more nodes = better fit for reverse KL

metrics:
1. higher overall quality
2. lower exposure bias
3. better calibration
4. higher long text generation performance

Method scalable from 120M to 13B parameters

### Conclusion

Knowledge Distillation was previously successsfully applied to black-box models or white-box models with the task of language understanding. With a continued rise in open-source LLMs<d-cite key="Gu2023MiniLLM"></d-cite>, the authors of MiniLLM argue the exigency of KD techniques for white-box generative models. The paper makes a case for using reverse KL divergence as a metric for training the student model and presents optimizations to ensure the student model is incentivized to output sufficiently long, accurate content while simultaneously suppressing the chance of hallucination at the cost of pruning _some_ valid content from potentially being outputted.