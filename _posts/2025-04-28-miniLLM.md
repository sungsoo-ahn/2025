---
layout: distill
title: Distilling LLMs into MiniLLMss
description: "In this blog post, we propose to investigate a technique to distill LLMs into smaller models. Relying on the paper <em>MiniLLM: Knowledge Distillation of Large Language Models</em>, published in March 2024 by Yuxian Gu et al., we summarize and discuss key findings in the space and their implications for scalable language model training and deployment."
date: 2025-04-28
future: true
htmlwidgets: true
hidden: false

# Anonymize when submitting
authors:
  - name: Anonymous


# must be the exact same name as your blogpost
bibliography: 2025-04-28-minillm.bib  

# Add a table of contents to your post.
#   - make sure that TOC names match the actual section names
#     for hyperlinks within the post to work correctly. 
#   - please use this format rather than manually creating a markdown table of contents.
toc:
  - name: Background
  - name: Intention
  - name: Comparing Models
  - name: Divergence Feedback in Practice
  - name: Optimizations
  - name: Methods
  - name: Results
  - name: Practical Implications
  - name: Conclusion

# Below is an example of injecting additional post-specific styles.
# This is used in the 'Layouts' section of this post.
# If you use this post as a template, delete this _styles block.
_styles: >
  .fake-img {
    background: #bbb;
    border: 1px solid rgba(0, 0, 0, 0.1);
    box-shadow: 0 0px 4px rgba(0, 0, 0, 0.1);
    margin-bottom: 12px;
  }
  .fake-img p {
    font-family: monospace;
    color: white;
    text-align: left;
    margin: 12px 0;
    text-align: center;
    font-size: 16px;
  }
---

## MiniLLMs: Knowledge Distillation of Large Language Models

### Background

**Large Language Models (LLMs)** have witnessed a surge in computational demand as they've grown in size.
<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.html path="assets/img/2025-04-28-minillm/fig1.png" class="img-fluid rounded z-depth-1" %}
    </div>
</div>
<div class="caption">
    Statistics of large language models (having a size larger than 10B in this survey) in recent years<d-cite key="Wayne2023Survey"></d-cite>
</div>
Techniques to reduce this computational load include **Knowledge Distillation (KD)**, in which a "smaller" student model is fine-tuned using data or feedback from a larger "teacher" model<d-cite key="Hinton2015Distilling"></d-cite>.

Distilled models, with fewer than 1B parameters<d-cite key="Hinton2015Distilling"></d-cite>, are more efficient than larger models, using less computational power and energy. Although they may offer slightly reduced performance on tasks like text generation, translation, and sentiment analysis, their smaller size allows for faster inference and deployment on edge devices, making them especially valuable for resource-constrained applications (mobile apps, IoT) or those where privacy is critical (healthcare, finance).

Types of KD include<d-cite key="Gou2021KDSurvey"></d-cite>:
- **Black-box KD**: provides the student model with only the input-output pairs from the teacher model, without revealing its internal states. This has shown promising results with prompt-response pairs from prominent LLM APIs, as most researchers can only query them without access to their internal mechanisms. Notable examples include Stanford's Alpaca<d-cite key="Alpaca"></d-cite> and Vicuna<d-cite key="Chiang2023Vicuna"></d-cite>, both of which use training data generated from querying major LLMs.
- **White-box KD**: exposes the internal mechanisms of a teacher model (e.g. hidden state data, a full distribution over predicted outputs) to the student model, allowing it to gain more insight into the full decision-making process of the teacher, rather than just its final verdict. The authors argue this method has only been applied to language _understanding_<d-cite key="Sanh2020DistilBERT"></d-cite><d-cite key="Wang2020MiniLM"></d-cite> (tasks that require interpreting pre-generated text) rather than _generation_ (an inherently more creative and open-ended task). We note however, that comparisons between forward and reverse KD have at least already been studied in previous generative computer vision contexts<d-cite key="Lee2022SelfKnowledge"></d-cite>.


### Intention
As LLMs are increasingly used for content generation and the growing availability of open-source models, there is an opportunity to train smaller models using white-box knowledge of well-performing generators.
The paper we are investigating today, **MiniLLM: Knowledge Distillation of Large Language Models**<d-cite key="Gu2023MiniLLM"></d-cite>, aims to refine _white-box_ KD specifically for _generative_ tasks, focusing on:
1. Distilling larger teacher model into a smaller student model
2. Generating relevant, generalizable responses to a prompt
3. Avoiding obscure or nonsensical outputs
4. Upholding truthfulness aligned with the teacherâ€™s distribution
5. Encouraging detailed, non-brief responses

### Comparing Models
Given white-box access to the output distribution of the teacher model, _how well_ a student model has distilled knowledge can be measured by _how closely_ the student model's output distribution aligns with the teacher's.

Let's say the teacher LLM's probability distribution over text output, $$p$$, closely resembles that of real documents and human-written text. Now, we want to train the parameters $$\theta$$ of our small student model so it outputs a distribution $$q_\theta$$ to best mimic $$p$$. One standard measure of how different these two distributions are is **(forward) KL Divergence**:

$$
D_{\text{KL}}(p||q_\theta) = \sum_{x\in \mathcal{X}} p(x) \log\left(\frac{p(x)}{q_\theta(x)}\right)
$$

Note the asymmetric definition&mdash;in other words, 

$$
D_{\text{KL}}(p||q_\theta) \neq D_{\text{KL}}(q_\theta||p)
$$

What does this asymmetry mean in practice?

Say the distribution $$p$$ has many **modes** (the probability mass function looks hilly with several outputs that have a higher probability than most other outputs). An example of this is how an LLM is _far_ more likely to spit out a small set of outputs (sentences that make sense in English) compared to the vast majority of _possible_ outputs (all random sequences of words).

When we try to adjust distribution $$ q_\theta $$ to minimize the divergence $$ D_{\text{KL}}(p||q_\theta) 
$$, we try to get it to increase the probability it assigns to each of $$ p $$'s modes. This may work fine with a small, finite set of modes, such as in an understanding or classification task where we limit the possible set of class labels or insights a model can draw from an input so that we can better evaluate whether it's extracting the _proper_ understanding.

What about text generation? Think about how many modes would exist in a distribution over all possible valid outputs a model could generate given some prompt. Because forward KL wants to boost probabilities in $$ q_\theta $$, it dislikes lowering the probabilities of any output ("**zero-avoiding**"<d-cite key="Malinin2019ReverseKL"></d-cite>), so attempting to lower $$ D_{\text{KL}}(p||q_\theta)
$$ results in a distribution for $$ q_\theta $$ that spreads out across all possible outputs with not much precision for identifying the modes of $$ p $$. This is especially limiting if our $$ q_\theta $$ distribution is output by a student model whose small size limits its capacity to express large, complicated distributions.

In such cases, the authors argue a worthy tradeoff is having a $$q_\theta$$ that boosts the probabilities of _some_ of the modes in $$p$$, and diminishes the probability of all the other outputs, even if there's some other, less significant modes among the noisy garbage outputs that get silenced. The **Reverse KL Divergence** strategy argues that KD should minimize $$ D_{\text{KL}}(q_\theta||p) 
$$ instead. This objective will boost the probabilities in $$q_\theta$$ associated with the biggest modes of $$p$$, focusing on capturing the most frequent outputs and pushing all other probabilities down ("**zero-forcing**"). Diminishing the random, noisy, or garbage output is core to ensuring the output of $$q_\theta$$, and thereby our student model, is free of hallucinations<d-cite key="Huang2023Hallucination"></d-cite> or otherwise nonsensical content.

The differences between the result of minimizing different objectives is shown elegantly in this graph:
<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.html path="assets/img/2025-04-28-minillm/fig2.png" class="img-fluid rounded z-depth-1"  %}
    </div>
</div>
<div class="caption">
    Fitting a Gaussian mixture distribution using forward and reverse KLD.<d-cite key="Gu2023MiniLLM"></d-cite>
</div>

But wait&mdash;won't the student model lose the slightly smaller modes? In other words, won't our student model fail to learn more obscure (but equally valid) responses from our teacher distribution? The authors argue that:
1. They don't care about capturing all, or even multiple, valid responses; simply _one_ correct response from a student model in response to a prompt is sufficient.
2. A student model minimizing reverse KL still achieved linguistically complex and diverse responses, as evidenced by its 4-grams diveristy score of 99.0, comparable to the teacher model's 99.3 on their test set.<d-cite key="Gu2023MiniLLM"></d-cite>
3. A student model minimizing reverse KL is still able to output the full distribution of text from the source distribution, suffering only from a small relative loss of information with a loss of 3.95 compared to the teacher's 3.55.<d-cite key="Gu2023MiniLLM"></d-cite>

### Divergence Feedback in Practice
If using forward KL Divergence, a **sequence-level KD** approach would take a prompt and feed it through both the student and teacher model. It would keep sampling outputs from the resulting teacher distribution to get an understanding of $$p$$, and extract the raw distribution $$q_\theta$$ from the student to calculate the loss $$ \mathcal{L}(\theta) = D_{\text{KL}}(p||q_\theta) 
$$, then propagate the gradient $$\nabla\mathcal{L}(\theta)$$ back to the student so it can update its model weights $$\theta$$ to lower the divergence on the next iteration.

With white-box access to the teacher model, the **MiniLLMs approach** can poll the _student_'s output distribution to mimic $$q_\theta$$, the frequency of responses from the student, and compare it with the true value of $$p$$ extracted from the teacher to propagate $$ \nabla\mathcal{L}(\theta) = \nabla D_{\text{KL}}(q_\theta||p) 
$$. In intuitive terms, this means instead of increasing the likelihood the student outputs the full range of samples frequently outputted by the teacher, we aim to train the student to output samples that resonate with the teacher's preference/distribution.

Let's say that we fed in an input prompt $$\boldsymbol{x}$$ and we want to predict the token at position $$t'$$ given the tokens up to that point, $$\boldsymbol{y}_{<t'}$$. Given this token, $$y_{t'}$$ we can examine how well the student model distills knowledge from the teacher model by comparing their probability of outputing this token: 
$$r_{t'} = \log \frac{p(y_{t'} | \boldsymbol{y_{<t'}, \boldsymbol{x}})}{q_\theta (y_{t'} | \boldsymbol{y_{<t'}, \boldsymbol{x}})}$$
Given our chosen tradeoffs, we want $$r$$ to be as close to 0 (probabilities align 1:1) but definitely not negative (we don't want $$q_\theta$$ to assign a larger probability to outputs that were not boosted in $$p$$). We can also calculate this alignment for the specific _sequence_ of tokens after some point $$t$$ (up to the full length of the output, $$T$$) by replacing the single probability with a product of probabilities:
$$R_t = \log\left( \frac
{\prod_{t'=t}^{T} p(y_{t'} | \boldsymbol{y_{<t'}, \boldsymbol{x}}) }
{\prod_{t'=t}^{T} q_\theta (y_{t'} | \boldsymbol{y_{<t'}, \boldsymbol{x}}) } 
\right) = \sum_{t'=t}^{T} r_{t'}$$

For our setup, say we repeatedly sample input prompts $$\boldsymbol{x}$$ from some distribution of input prompts $$p_{\boldsymbol{x}}$$ (labeled as $$p$$ because it is a natural language distribution that should closely mirror the teacher's language distribution of outputs). We then feed this through the student model with its current parameters $$\theta$$ and get an output distribution $$q_\theta(\cdot | \boldsymbol{x})
$$. 

We adapt this thinking into a **Policy Gradient**<d-cite key="Williams1992PolicyGradient"></d-cite> formulation to show the feedback we will give the student model (after giving it an input prompt $$\boldsymbol{x}$$ and receiving an output $$\boldsymbol{y}$$):
$$
\nabla \mathcal{L}(\theta) = - 
  \underset{\substack{
  \boldsymbol{x} \sim p_{\boldsymbol{x}} \\
  \boldsymbol{y} \sim q_\theta(\cdot | \boldsymbol{x}) }}
  {\mathbb{E}}
\sum_{t=1}^T (R_t - 1) \nabla\log q_\theta (y_t | \boldsymbol{y}_{<t} , \boldsymbol{x}) 
$$

Some things to note about this formulation:
- Instead of relying on single samples to calculate our loss, we can calculate the expected value across samples from the input distribution $$p_{\boldsymbol{x}}$$ and the corresponding output distribution $$q_\theta(\cdot | \boldsymbol{x})
$$ for each such sample $$\boldsymbol{x}$$.
- We accumulate the loss over each token in our $$T$$-length output.
- As an high-level example, if $$R_t - 1$$ is very low (very negative), this means probability $$p$$ was much lower than $$q$$ through point $$t$$, and our student is over-producing an output that is insignificant in the teacher's distribution. Multiplied with the negative sign from outside the $$\mathbb{E}$$, this attaches a high positive multiplier so that $$\nabla\mathcal{L}(\theta) \propto \nabla\log q_\theta (y_t | \boldsymbol{y}_{<t} , \boldsymbol{x})
$$. Thus, to minimize loss (and traverse in the direction of $$-\nabla\mathcal{L}(\theta)$$), we would want to decrease $$\log q_\theta (y_t)$$ and thus decrease $$q_\theta (y_t)$$ so it gets closer to $$p (y_t)$$ and is less likely to output the token $$y_t$$ in the given sequence.
- Intuitively, the negative loss gradient $$(-\nabla\mathcal{L}(\theta))$$ is positively correlated with $$p(y_t)$$ because we want to maximize the chance the student outputs tokens $$y_t$$ that the teacher favors, and negatively correlated with $$q_\theta(y_t)$$ because we want to minimize the probabilitiy peaks in $$q_\theta$$ so that the probability mass can be spread out over a diverse array of possible outputs.

### Optimizations
Some important optimizations are necessary to ensure KD training with this feedback structure goes smoothly:
<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.html path="assets/img/2025-04-28-minillm/fig3.png" class="img-fluid rounded z-depth-1" %}
    </div>
</div>
<div class="caption">
    The reverse KLD between the teacher and the students during MiniLLMs training when different optimization strategies are applied.<d-cite key="Gu2023MiniLLM"></d-cite>
</div>

1. **Single-Step Decomposition**: to reduce variance (spikes in the training graph), the authors sought to split the single-step loss variable $$r_t$$ (loss for the generation of token $$t$$), while still keeping the accumulated loss $$R_t$$ (cumulated loss up to token $$t$$) which helps accelerate and smoothen convergence. Indeed, as mistakes made in the early steps of a sequence generation can have a compounding effect on the rest of the generated text, computing $$r_t$$ individually allows the model to correct single-step errors and adjust its parameters accordingly, resulting in more stable and efficient training.<br><br>
$$
\nabla\mathcal{L}(\theta)
= \nabla\mathcal{L}(\theta)_{\text{single}} + \nabla\mathcal{L}(\theta)_{\text{long}}
$$
$$
\quad \quad \quad = -\mathbb{E} 
  \left( \sum_{t=1}^T 
    \nabla\mathbb{E}_{y_t \sim q_\theta(t)} r_t 
  \right)-\mathbb{E} 
  \left( \sum_{t=1}^T 
    R_{t+1} \nabla \log q_\theta (y_t | \boldsymbol{y}_{<t} , \boldsymbol{x}) 
  \right)
$$


2. **Teacher-Mixed Sampling**: A common problem in training with reinforcement policies is **Reward Hacking**<d-cite key="Skalse2024RewardHacking"></d-cite>, wherein the student attempts to minimize its own loss function, which is just a _proxy_ for the true, abstract mission of just aligning with the natural language distribution however possible. This could lead to the student finding loopholes to minimize its loss by the "letter of the law", rather than the "spirit of the law". In practice, this could mean short, degenerate, or repeated phrases that happen to have a low loss value. To prevent this, rather than evaluating samples $$\boldsymbol{y}$$ drawn purely from the student's (potentially reward-hacking) distribution $$q_\theta$$, we draw them from a weighted combination
$$\tilde{p} = \alpha p + (1-\alpha) q_\theta$$, 
where $$\alpha = 1/5$$. With this modification, we need to adjust the loss propagated to the student so it only gets loss proportional to its share of the final distribution, so we change our formula to add a weighting of $$w_t = \frac{q_\theta (y_t | \boldsymbol{y}_{<t} , \boldsymbol{x})}{\tilde{p} (y_t | \boldsymbol{y}_{<t} , \boldsymbol{x})}$$:<br><br>
$$
\quad \quad \quad \nabla\mathcal{L}(\theta)
= - \underset{\substack{
  \boldsymbol{x} \sim p_{\boldsymbol{x}} \\ 
  \boldsymbol{y} \sim \tilde{p}(\cdot | \boldsymbol{x})}}
  {\mathbb{E}} 
\left( \sum_{t=1}^T w_t 
\nabla\underset{y_t \sim q_\theta(t)}{\mathbb{E}} 
r_t \right)
$$
$$ 
\quad \quad \quad \quad \quad \quad -\underset{\substack{
  \boldsymbol{x} \sim p_{\boldsymbol{x}} \\ 
  \boldsymbol{y} \sim \tilde{p}(\cdot | \boldsymbol{x})}}
  {\mathbb{E}}
\left( \sum_{t=1}^T w_t R_{t+1} \nabla \log q_\theta (y_t | \boldsymbol{y}_{<t} , \boldsymbol{x}) \right)
$$<br><br>
This setup allows the teacher to mix-in known valid responses, suppressing low-quality generation, while ensuring the student gets feedback proportional to its share of the generated output. Without this mixing, the reverse KL loss decreases, but this is usually due to prioritization of short, meaningless, or repeated strings. 

3. **Length Normalization**: Because long output sequences accumulate many $$r_{t'}$$ terms, they tend to have relatively more negative $$R_t$$ (more loss) than a similar quality output of shorter length. To eliminate this length bias, the authors normalize $$R$$ by the number of terms accumulated (by taking the arithmetic mean of $$r_{t'}$$):
$$
R_t^{\text{Norm}} = \frac{1}{T - t - 1} \sum_{t'=t}^{T} r_{t'}
$$
This promotes reducing the _average_ loss per token rather than _total_ loss of all tokens, so that a longer sequence with higher-quality output tokens is more favorable than a short sequence with a few really bad tokens.

4. **Penalizing Repeating Foundational Training**: To start the student model at a sufficient baseline, it was first pre-trained on a large corpus of long documents ($$\mathcal{D}_{\text{PT}}$$) to get the sensibility of language generation. Then, given a teacher model that performs well on a dataset we want to test ($$\mathcal{D}$$), we further fine tune the student model on $$\mathcal{D}$$. In the KD iterations, the authors added a feedback element $$\nabla\mathcal{L}_{\text{PT}}$$ to disincentivize the model simply repeating tokens it saw with high frequency in its pre-training, and instead to incentivize the model to generate new content relevant to its new dataset $$\mathcal{D}$$:<br><br>
$$
\nabla\mathcal{L}_{\text{PT}}(\theta) = -\underset{\boldsymbol{d} \sim \mathcal{D}_{\text{PT}}}{\mathbb{E}} \log q_\theta (\boldsymbol{d})
$$
$$
\nabla\mathcal{L}(\theta) = \nabla\mathcal{L}(\theta)_{\text{single}} + \nabla\mathcal{L}(\theta)_{\text{long}}^{\text{Norm}} + \nabla\mathcal{L}_{\text{PT}}(\theta)
$$

### Methods
The resulting student would be trained via the iterative KD loop described previously, with the addition of clipping<d-cite key="Schulman2017Proximal"></d-cite> for added stability. (Clipping is the action of limiting the magnitude of the gradients during training to prevent instability due to overly large updates.)

1. **Teacher/Student Models.**
The authors implemented MiniLLMs with multiple models and sizes. The student models are pre-trained on a large long-document corpus $$\mathcal{D}_{\text{PT}}$$, OpenWeb<d-cite key="Gokaslan2019Openwebtext"></d-cite> for GPT2 and RoBERTa<d-cite key="Liu2019RoBERTa"></d-cite> training for other models.
- **GPT-2**<d-cite key="Radford2019GPT2"></d-cite>
  - Teacher size: 1.5B parameters
  - Student sizes: 120M, 340M, 760M parameters
- **OPT**<d-cite key="Zhang2022OPT"></d-cite>
  - Teacher size: 13B parameters
  - Student sizes: 1.3B, 2.7B, 6.7B parameters
- **LLaMa**<d-cite key="Touvron2023LLaMA"></d-cite>
  - Teacher size: 13B parameters
  - Student size: 7B parameters

2. **Comparison Baselines.**
For each student size listed above, comparison points for MiniLLMs included equally-sized student models that were trained in the following ways:
- **SFT w/o KD**: Students fine-tuned directly on $$\mathcal{D}$$ without KD, but with the added feedback of **golden responses** (reference responses used as benchmark)
- **KD**: Students trained via word-level feedback from teacher's $$p$$  using forward KLD
- **SeqKD**: Students trained on input-output data generated by a teacher model using forward KLD (for students to better approximate the teacher output distribution rather than just ground truth labels)

3. **Assessement Datasets $$\mathcal{D}$$.**
The student models were assessed on the 5 following datasets:
- **Dolly**<d-cite key="DatabricksDolly"></d-cite>: natural language tasks including brainstorming, classification, open/closed QA, generation, information extraction, and summarization.
- **SelfInst**<d-cite key="Wang2023SelfInstruct"></d-cite>: natural language tasks including all of the above and rewriting/paraphrasing, instruction following, multi-turn dialogues, and code understanding/generation.
- **Vicuna**<d-cite key="Chiang2023Vicuna"></d-cite>: a custom set of 80 challenging  natural language tasks.
- **S-NI**<d-cite key="Wang2022Benchmarking"></d-cite>: a set of natural language tasks aimed at assessing generalization via In-Context instructions.
- **UnNI**<d-cite key="Honovich2023Unnatural"></d-cite>: natural language tasks including creative writing, text completion, and diverse generative tasks.

4. **Metrics.**
The knowledge-distilled student model was compared to several benchmarks, including:
- **RougeL**: precision for large scale instruction 
- **Human judgement**: the authors asked humans to subjectively pick a winner between 2 outputs, one from MiniLLMs and another from a comparison point (e.g. the teacher model)
- **GPT-4 evaluation**: the authors fed the outputs from MiniLLMs and a comparison point to GPT-4 and asked the bigger model to score both outputs on a range of 1-10 for "helpfulness, relevance, accuracy, and level of detail of their responses". It's worth noting that some researchers question the scientific validity of evaluations conducted by GPT-4.<d-cite key="2023OpenReview"></d-cite>

### Results 
MiniLLMs consistently outperforms baselines across diverse datasets and model sizes, showcasing strong **generalization**, from compact 120M models to large 13B architectures. It also achieves precise, high-quality responses which significant overlap with ground-truth answers. In certain instances, it even **surpasses** teacher models. The authors report that this behavior has been documented before<d-cite key="Furlanello2018BornAgain"></d-cite>, and is likely explained by the student model addressing exposure bias during training. 

**Exposure bias** occurs during the training of the teacher model when it is only exposed to perfect sequences in the ground truth data, leading to discrepancies during inference, where it must generate sequences based on its own predictions. The student model corrects exposure bias by sampling and learning from its own generated outputs during training, enabling it to better handle prediction errors and align closer to real-world inference conditions.

One other interesting observation is that while literature suggest that student model quality does not always **scale proportionally** with teacher size<d-cite key="Mirzadeh2019Improved"></d-cite>, MiniLLMs consistently improves as teacher model sizes increase, indicating MiniLLMs's potential to more effectively compress larger-scale models than previous historical attempts.

According to GPT-4's Technical Report<d-cite key="OpenAI2023GPT4"></d-cite>, models trained with policy optimization (which is the case of our student models) often suffer from **poor calibration** (the alignment between a model's predicted probabilities and the actual outcomes). The authors speculate that distilled models trained using _forward_-KLD pushes high probabilities to void regions of the target distribution, causing large distribution differences between student and teacher models. In contrast, MiniLLMs focuses on key parts of the distribution, narrowing the calibration gap.

Finally, the study examines model performance across different **expected response lengths**. It finds that all models perform poorly on short responses ($\leq 5$ tokens) due to a distribution shift between training and evaluation. However, for longer responses exceeding $6$ tokens, MiniLLMs outperforms forward-KD student models. The intuition is that longer responses (more tokens) create a larger output space with more modes, making reverse-KL a better fit for capturing the teacherâ€™s distribution compared to shorter responses.

### Practical Implications

#### Methodological Considerations
Reproducing these findings poses significant challenges. The original experiments required substantial computational resources (16 H100 GPUs running for 10 hours to distill LLaMA 13B to 7B) and large datasets like OpenWebText<d-cite key="Gokaslan2019Openwebtext"></d-cite>. Even distilling smaller models within the GPT-2 family would require considerable resources, potentially yielding different results than those observed between larger models.

The data efficiency of the distillation process raises important questions. The requirement for extensive pre-training data, rather than pure teacher model knowledge, suggests we might be "augmenting" rather than purely "distilling" knowledge<d-cite key="Hu2023TeacherStudent"></d-cite>.

#### Industrial Applications
Knowledge distillation is seeing rapid adoption in production environments. GPT-4o mini demonstrates this trend, achieving comparable performance to larger models while being 16x cheaper to run via API<d-cite key="OpenAI2024GPT4oMini"></d-cite>. While traditional distillation often degrades performance, recent work shows student models can sometimes match or exceed their teachers<d-cite key="Furlanello2018BornAgain"></d-cite>, particularly when:
- The student model has sufficient capacity for the task
- Additional techniques like data augmentation are employed
- The focus is on specific, well-defined capabilities rather than general performance

#### Democratizing AI
The ability to create efficient, smaller models has several key implications<d-cite key="Wayne2023Survey"></d-cite>:
- **Cost Reduction**: Lower operational costs for both training and inference
- **Accessibility**: Enables deployment on edge devices and resource-constrained environments
- **Environmental Impact**: Reduced energy consumption and carbon footprint<d-cite key="Gou2021KDSurvey"></d-cite>
- **Safety & Validation**: Requires careful testing to maintain safety guarantees while reducing model size

The authors report that MiniLLM's training time is approximately half that of traditional approaches<d-cite key="2023OpenReview"></d-cite>, further supporting its potential for widespread adoption. However, practitioners must carefully weigh the trade-offs between model compression, performance requirements, and reliability for their specific use cases.

### Conclusion
Knowledge Distillation was previously successsfully applied to black-box models or white-box models with the task of language understanding. With a continued rise in open-source LLMs<d-cite key="Gu2023MiniLLM"></d-cite>, the authors of MiniLLMs argue the exigency of KD techniques for white-box generative models. The paper makes a case for using reverse KL divergence as a metric for training the student model and presents optimizations to ensure the student model is incentivized to output sufficiently long, accurate content while simultaneously suppressing the chance of hallucination at the cost of pruning _some_ valid content from potentially being outputted.

#### Why This Matters
This work advances the field in several key ways:
1. Demonstrates that reverse KL divergence can effectively preserve model capabilities while reducing size
2. Provides empirical evidence across multiple model scales and benchmarks
3. Introduces practical optimizations for training stability and output quality
4. Reduces training time compared to traditional distillation approaches<d-cite key="2023OpenReview"></d-cite>

These contributions are particularly relevant as the field moves toward more efficient, accessible AI systems that maintain the capabilities of larger models while reducing computational and environmental costs.