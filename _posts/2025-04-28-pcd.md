---
layout: distill
title: Improving LLM’s Long Range Awareness via Positional Contrastive Decoding
description: As Large Language Models (LLMs) scale to support increasingly long contexts, we observe performance degradation within the claimed attention window as input length increases. Through a logits-level analysis, we identify and term the phenomenon Rank Collapse, where the average rank of correct tokens declines with increasing length but remains within the top decoding positions. To address Rank Collapse and enhance long-range awareness, the blog introduces Positional Contrastive Decoding (PCD)—a training-free, plug-and-play method that leverages low-frequency positional perturbations and contrastive decoding. PCD integrates seamlessly into existing architectures with just two lines of code, offering an efficient and effective solution to improve long-context performance within the claimed length of attention window.
date: 2025-04-28
future: true
htmlwidgets: true
hidden: false

# Anonymize when submitting
# authors:
#   - name: Anonymous

authors:
  - name: Anonymous

# must be the exact same name as your blogpost
bibliography: 2025-04-28-pcd.bib

# Add a table of contents to your post.
#   - make sure that TOC names match the actual section names
#     for hyperlinks within the post to work correctly. 
#   - please use this format rather than manually creating a markdown table of contents.
toc:
  - name: "Background"
  - name: "Rank Collapse For Long Context Models"
  - name: "Related Works"
  - name: "Improving Long Range Awareness by low-frequency Perturbation"
  - name: "Algorithm: Positional Contrastive Decoding (PCD)"
  - name: "Case Study: PCD effevtively mitigate Rank Collapse"
  - name: "Experiments"
  - name: "Conclusion"


  
_styles: >
  .fake-img {
    background: #bbb;
    border: 1px solid rgba(0, 0, 0, 0.1);
    box-shadow: 0 0px 4px rgba(0, 0, 0, 0.1);
    margin-bottom: 12px;
  }
  .fake-img p {
    font-family: monospace;
    color: white;
    text-align: left;
    margin: 12px 0;
    text-align: center;
    font-size: 16px;
  }
---

## Improving LLM’s Long Range Awareness via Positional Contrastive Decoding

## Background
The official claimed context length of large language models has gradually increased. Starting from the release of GPT-3.5-Turbo of 16k length on November 6, 2023, commercial models reached support for 128k tokens by early 2024. In the case of open-source models, the release of Llama 3.1 by July 2024 signified the 8-billion-parameter models that could reliably handle contexts of 128k tokens <d-cite key="dubey2024llama"></d-cite>. By mid-2024, nearly all cutting-edge models had adopted 128k tokens as the standard context length.

{% include figure.html path="assets/img/2025-04-28-pcd/image.png" class="img-fluid rounded z-depth-1" %}

However, the effective length of the models falls significantly short of the  claimed length. To evaluate the real capabilities of long-context models, benchmarks typically include retrieval tasks, multi-hop tracking tasks, information aggregation, multi-document question answering, multilingual tasks, and code comprehension. In the RULER benchmark <d-cite key="hsieh2024ruler"></d-cite>, it has been observed that open-source models claiming context lengths of 128k tokens struggle to maintain effective context lengths beyond 32k tokens. Similarly, in multilingual long-context benchmark <d-cite key="hengle2024multilingual"></d-cite>, open-source models such as Llama3-8b demonstrate effective context lengths between 4k and 8k tokens, falling significantly short of the 128k target. Moreover, the performance of existing models on the Infinite Bench <d-cite key="zhang2024bench"></d-cite> remains far below expectations across various tasks. For instance, in tasks such as Math.Calc and code debugging/running, most models achieve near-zero performance scores.

Several studies have delved into mechanisms and causes that lead to performance degradation within the claimed context length of long-context models. The "lost in the middle" <d-cite key="liu2024lost"></d-cite> reveals that models often focus on the beginning and end of corpus while neglecting middle parts. Based on it, InfiniteBench <d-cite key="zhang2024bench"></d-cite> findings that performance degradation in long-context models is not consistently linked to the middle of the input. Instead, it can occur at any location within the context. Furthermore, the "Know but Don’t Tell" hypothesis <d-cite key="lu2024insights"></d-cite> suggests a disconnect between information retrieval and generation. Probing analysis shows that models accurately identify key information in hidden representations but struggle to effectively use it during generation. In addition to these findings,  LongLLAMA <d-cite key="tworkowski2024focused"></d-cite> identifies that Transformers, when handling long context or multi-document inputs, struggle to distinguish important content from irrelevant information, limiting the effectiveness of the attention mechanism.

This blog presents a new perspective by examining logits dynamics across different input lengths. Through the token-level analysis, we identify a phenomenon termed "Rank Collapse," which indicates that the predicted ranking of the correct token tends to statistically deteriorate as the input length increases when task difficulty remains constant.

## Rank Collapse For Long Context Models
To investigate the effect of input length on logits and identify the patterns exhibited by the logits, we designed a key-value retrieval experiment, in which models are required to retrieve the corresponding value for a provided key from a large key-value dictionary.

The **Mean Reciprocal Rank (MRR)**, a statistical metric commonly used in retrieval systems to measure the effectiveness of retrieval results, is leveraged to quantify logits pattern. The formula for MRR is as follows:
$$
\operatorname{MRR}=\frac{1}{|Q|} \sum_{i=1}^{|Q|} \frac{1}{\operatorname{rank}_{i}}
$$
where $$Q$$ represents the number of retrieval attempts, and $$rank_i$$ denotes the rank of the correct answer.

**Rank Collapse** refers to a phenomenon where the predicted ranking of the correct token degrades statistically as the input text length increases, even though the task difficulty remains unchanged.  

The figure below illustrates the phenomenon of Rank Collapse, depicting the MRR for all incorrect cases. This observation is based on Gradient AI's Llama-3-8B model, which reportedly supports an input length of 262k tokens.
For each asigned input length, 100 test cases were evaluated.  Considering that the overall accuracy and the accuracy of the first token are nearly identical, the ranking of the first token is used to represent the entire response. For each test case, the ranking of the label's first token among the predicted probabilities is used for the MRR calculation.
{% include figure.html path="assets/img/2025-04-28-pcd/image_1.png" class="img-fluid rounded z-depth-1" %}
As the sample length increases from 1k to 26k, the MRR gradually decreases from 0.5 to approximately 1/3. This indicates that in incorrect predictions, the average rank of the correct token drops from 2nd to 3nd. 

The figure below shows the progression of the top 100 token values at each length. The x-axis represents the vocabulary IDs, where, for simplicity in plotting, the first 100 tokens are selected from the 128,256 IDs and transformed into a range from 1 to 100 in ascending order. It can be observed that, as the length increases, the probability associated with the correct token decreases, reaching its lowest value at 26k, where the correct token is ranked at most 5th.

{% include figure.html path="assets/img/2025-04-28-pcd/image_2.png" class="img-fluid rounded z-depth-1" %}

{% include figure.html path="assets/img/2025-04-28-pcd/image_3.png" class="img-fluid rounded z-depth-1" %}

{% include figure.html path="assets/img/2025-04-28-pcd/10.gif" class="img-fluid rounded z-depth-1" %}

Rank Collapse reveals that as the input length increases, even though the number of incorrect tokens rises (resulting in a decrease in MRR), the rank of the correct answer gradually declines yet still retains a prominent position within the decoding space.This observation suggests that although the predicted result may be incorrect, the model inherently prioritizes the correct answer, which is positioned near the top in the decoding space. However, due to insufficient confidence, the correct answer is not generated. **This raises the question of whether it is possible to design a decoding strategy that can elevate the rank of the correct token within the decoding space.**

## Related Works

To augment the long-range awareness of Large Language Models (LLMs), research has primarily focused on four strategies: input design, instruction design, model-driven methods, and data-driven methods. 

Input design, specifically, involves methods such as segment reranking <d-cite key="dsouza2024evaluating, peysakhovich2023attention"></d-cite>, which aims to mitigate the inherent location bias ("lost in the middle") of transformers by rearranging input segments to balance the attention across different positions during inference. However, rearranging suffers from disrupting semantic coherence and incurring computational expenses due to the necessity for multiple inferences and averaging. 
Instruction design, such as context recalling <d-cite key="zhang2024bench, yu2023paraphrasing"></d-cite>, involves prompting the model to recall and generate relevant information before completing a task. Despite its potential to remind the model to retrieve information, context recalling does not inherently enhance long-range awareness. Model-driven methods address long-range awareness by optimize attention mechanisms or positional encodings to mitigate location bias. For instance, multi-scale positional encoding is designed to balance local and global modeling capability <d-cite key="zhang2024found, hsieh2024found"></d-cite>. However, model driven methods are not only costly but also poses challenges to maintaining stability and generalizability. Data-driven methods, which involve training on synthetic or multi-document QA datasets, have demonstrated efficacy in improving retrieval accuracy <d-cite key="dataartificial, an2024make"></d-cite>. Nevertheless, the extensive time requirements, high difficulty, and significant costs associated with annotating long-form datasets present a major barrier to acquiring high-quality, large-scale training corpora.


Based on the preceding discussion of Rank Collapse, it can be observed that the correct tokens are generally ranked near the top of the decoding space. This observation holds potential for improving long-range awareness and could contribute to improving the ranking of these tokens within the decoding space by devising an appropriate decoding strategy.

## Improving Long Range Awareness by low-frequency Perturbation

Long-Distance Awareness is affected by models positional encodings <d-cite key="su2024roformer, press2021train, chi2022kerple"></d-cite>, which are designed with long-term decay: the farther a token is from the current position, the less relevant its information. The high-frequency encoding is primarily responsible for local modeling, while the low-frequency encoding is responsible for global modeling. **Improving the model's global modeling capacity requires enhancing the contribution of the low-frequency signal $$F_l$$.**

Assuming a function $$G$$ takes the high-frequency encoding $$F_h$$ and low-frequency encoding $$F_l$$ of the positional encoding as inputs, and outputs the model’s logit $$\mathbf{L} = G(F_h, F_l)$$. 

One way involves amplifying the influence of low-frequency encoding, which is accomplished by the standard long context training. The other approach takes an inverse perspective, introducing perturbations: 

(1) Directly perturbing high-frequency encoding to indirectly amplify long-distance awareness: A perturbation $$\epsilon_h$$ is added into high-frequency encoding as $$F_h' = F_h + \epsilon_h$$. Then the logits expressed as $$\mathbf{L} = G(F_h', F_l)$$. However, it is important to emphasize that introducing disturbances to the high-frequency encoding will seriously lead to model collapse.

(2) Initially involving a temporary perturbation of low-frequency encoding, followed by contrastive decoding to reversely amplify long-distance awareness: Perturbing the low-frequency encoding $$F_l$$, where a perturbation $$\epsilon_l$$ is added, as $$F_l\prime = F_l + \epsilon_l$$. The logits are expressed as $$\mathbf{L}_0 = G(F_h, F_l + \epsilon_l)$$, and the corrected logits $$\mathbf{L}_\alpha$$ are computed as $$\mathbf{L}_\alpha = (1 + \beta) \cdot \mathbf{L}_0 - \beta \cdot {\mathbf{L}}_\delta$$. 

Based on the analysis, directly enhancing long-distance capabilities requires training with long-text data, while perturbing the high-frequency encoding can lead to model collapse. However, perturbing the low-frequency encoding followed by contrastive decoding, as a prospective strategy, enhances long-distance awareness without compromising the model's base capabilities.

## Algorithm: Positional Contrastive Decoding (PCD)

### 1. Perturbative Positional Encoding in Low-Frequency encodings

The Rotary Position Embedding (RoPE) utilizes a rotation matrix $$R_{\Theta, m}^{d}$$ to encode relative positional information by rotating word embeddings based on their position index $$m$$. For a given dimension $$d$$, the rotation matrix $$R_{\Theta, m}^{d}$$ can be defined as follows:

$$
f_{\{q,k\}}(\mathbf{x}_m, m) = R_{\Theta, m}^{d} \mathbf{W}_{\{q,k\}} \mathbf{x}_m
$$

Since the inner product satisfies linear additivity, RoPE in any even-dimensional space can be represented as a concatenation of 2D cases:

$$
R_{\Theta, m}^{d} =
\begin{pmatrix}
\cos(m \theta_0) & -\sin(m \theta_0) & 0 & 0 & \cdots & 0 & 0 \\
\sin(m \theta_0) & \cos(m \theta_0) & 0 & 0 & \cdots & 0 & 0 \\
0 & 0 & \cos(m \theta_1) & -\sin(m \theta_1) & \cdots & 0 & 0 \\
0 & 0 & \sin(m \theta_1) & \cos(m \theta_1) & \cdots & 0 & 0 \\
\vdots & \vdots & \vdots & \vdots & \ddots & \vdots & \vdots \\
0 & 0 & 0 & 0 & \cdots & \cos(m \theta_{d/2-1}) & -\sin(m \theta_{d/2-1}) \\
0 & 0 & 0 & 0 & \cdots & \sin(m \theta_{d/2-1}) & \cos(m \theta_{d/2-1})
\end{pmatrix}
$$

where  $$\Theta = \{ \theta_i = 10000^{-2(i-1)/d}, i \in [1, 2, \ldots, d/2] \}$$. The high-frequency part predominantly contribute to local modeling, while the low-frequency part is mainly responsible for global modeling.

The positional perturbation is added to the low-frequency encodings (the first half) to obtain a noisy rotation matrix $$R_{\Theta, m}^{\prime d}$$. Let $$\sigma$$ be the standard deviation of the noise (e.g., $$\sigma = 0.03$$). We define random noise vectors $$\epsilon_{\cos}$$ and $$\epsilon_{\sin}$$ for the low-frequency encodings only: $$\epsilon_{\cos} \sim \mathcal{N}(0, \sigma^2), \quad \epsilon_{\sin} \sim \mathcal{N}(0, \sigma^2)$$. We add the perturbation to the first half of the cosine and sine encodings of the rotation matrix $$R_{\Theta, m}^{d}$$, thereby producing the perturbative rotation matrix:

$$
R_{\Theta, m}^{\prime d} = 
\begin{pmatrix}
R_{\Theta, m}^{d, \cos} + \epsilon_{\cos} & \text{if } i \leq d/2 \\
R_{\Theta, m}^{d, \sin} + \epsilon_{\sin} & \text{if } i \leq d/2 \\
R_{\Theta, m}^{d} & \text{otherwise}
\end{pmatrix}
$$

Here, $$\epsilon_{\cos}$$ and $$\epsilon_{\sin}$$ affect only the low-frequency encodings ($$i \leq d/2$$), while the rest of the encodings remain unchanged. $$R_{\Theta, m}^{d,\cos}$$ and $$R_{\Theta, m}^{d, \sin}$$ represent the cosine and sine parts of the positional encoding, respectively.

### 2. Calculating Perturbative Logits

Given input vectors $$\mathbf{x}_m$$ and $$\mathbf{x}_n$$, the perturbed query and key vectors are:

$$
\tilde{\mathbf{q}}_m = R_{\Theta, m}^{d} \mathbf{W}_q \mathbf{x}_m, \quad \tilde{\mathbf{k}}n = R_{\Theta, n}^{d} \mathbf{W}_k \mathbf{x}_n
$$

where $$\mathbf{W}_q$$ and $$\mathbf{W}_k$$ denote the query and key matrices.

Using these noisy positional embeddings, we perform a forward pass through the model to obtain the noisy logits $$\tilde{\mathbf{L}}_{\delta} = f(\tilde{\mathbf{q}}_m, \tilde{\mathbf{k}}_n) 
$$, where $$f(\cdot)$$ denotes the forward pass through the model.

### 3. Calculating Standard Logits

Without any perturbation, the positional embeddings for the query and key vectors are: $$\mathbf{q}_m = R_{\Theta, m}^{d} \mathbf{W}_q \mathbf{x}_m, \quad \mathbf{k}_n = R_{\Theta, n}^{d} \mathbf{W}_k \mathbf{x}_n$$. Then, we obtain the normal logits $$\mathbf{L}_0$$ by passing these embeddings through the model: $$\mathbf{L}_0 = f(\mathbf{q}_m, \mathbf{k}_n)$$.

### 4. Logits Calibration via Positional Contrastive Decoding

To calibrate the logits, we combine the normal logits and the mean of the noisy logits as follows, producing the calibrated logits $$\mathbf{L}_{\alpha} = (1 + \beta) \cdot \mathbf{L}_0 - \beta \mathbf{L}_{\delta}$$. Here $$\beta$$ is a hyperparameter (set to 1.5) that controls the weighting of the perturbative component relative to the original logits.

### 5. Next Token Prediction

Using the calibrated logits $$\mathbf{L}_{\alpha}$$, we predict the next token $$\hat{y} = \arg \max (\mathbf{L}_{\alpha})$$. Repeat these steps until the sequence generation is complete.

### Symbol Summary

- $$R_{\Theta, m}^{d}$$: Original rotation matrix
- $$R_{\Theta, m}^{d, \prime}$$: Perturbative rotation matrix
- $$\epsilon_{\cos}$$, $$\epsilon_{\sin}$$: Positional perturbation
- $$\mathbf{W}_q$$, $$\mathbf{W}_k$$: Wight of query and key
- $$\mathbf{x}_m$$, $$\mathbf{x}_n$$: Word vectors
- $$\mathbf{q}_m$$, $$\mathbf{k}_n$$: Query and key
- $$\tilde{\mathbf{q}}_m$$, $$\tilde{\mathbf{k}}_n$$: Perturbative query and key
- $$\mathbf{L}_0$$ Standard logits
- $$\tilde{\mathbf{L}}_{\delta}$$: Perturbative logits
- $$\mathbf{L}_{\alpha}$$: Calibrated logits
- $$\beta$$: Tuning coefficient

**Case Study: PCD effevtively mitigate Rank Collapse**
The case investigates the effectiveness of PCD in addressing rank collapse during a key-value retrieval task using the Llama3-8b-262k model at a sequence length of 16k. Given a specific input, the expected target label was `ffeae470-29ae-4a8c-9c56-9b97d9edf8ac`, while the model's standard decoding incorrectly predicted `cd501c0360f7`. The tokens for `"ffe"` (index 29069), `"FFE"` (index 80194), and `"ff"` (index 544) were chosen as candidate tokens.

{% include figure.html path="assets/img/2025-04-28-pcd/case_study.png" class="img-fluid rounded z-depth-1" %}

For standard decoding, we analyzed the Top-K (K=18) tokens sampled from the model's probability distribution: `4484, 1, 15, 25867, 66, 68, 4578, 791, 544, 69, 7047, 98046, 67, 291, 65, 58923, 29069, 16`. The unrelated token `"cd"` (index 4484) was preferred, while `"ff"` and `"ffe"` ranked at positions 9 and 17. The uppercase `"FFE"` (index 80194) did not appear in the top 18. This illustrates that standard decoding is vulnerable to noise, leading to rank collapse.

Using the proposed PCD, the decoding space included: `29069, 80194, 544, 73654, 89649, 3018, 44514, 84985, 68499, 18267, 38058, 78987, 81490, 73522, 14424, 34383, 93021, 1897`. The correct tokens (`"ffe"`, `"FFE"`, `"ff"`) were re-ranked to the top three positions. This intervention mitigated rank collapse by reducing distractions and improving long-range awareness.

## Experiments

To demonstrate the widely effectiveness of PCD, we conducted experiments using various models, strategies, and datasets. We selected the original Llama3-8b-8k model, as well as the Llama3-8b-262k and Llama3-8b-1048k models fine-tuned by Gradient AI on the basis of Llama3. The test sets includes key-value retrieval and variable tracking, as well as Longbench and RULER 16k. We employed decoding methods such as beam search and DoLa, along with training-free positional encoding correction algorithms like MsPoE.

In Table 1, on the task of key-value retrieval, PCD outperforms decoding methods such as beam search and DoLa, as well as MsPoE that utilizes multi-scale Positional IDs, achieving a maximum improvement of 7% without training.

{% include figure.html path="assets/img/2025-04-28-pcd/image_4.png" class="img-fluid rounded z-depth-1" %}

Table 2 presents the results for the variable tracking, which involves identifying the specific numerical value of a variable after a series of variable assignments mixed within noisy text. It is evident that PCD demonstrates consistent improvement on both the Llama-3-8B-8k and Llama-3-8B-262k models. On the Llama-3-8B-1048k model, the use of PCD may lead to a decrease in performance at 4k and 8k possibly due to the adequate training at short length, but a significant performance increase is observed at the 16k.

{% include figure.html path="assets/img/2025-04-28-pcd/image_5.png" class="img-fluid rounded z-depth-1" %}

Furthermore, we conducted tests on LongBench and RULER. It was observed that PCD enhances long-range awareness in the majority of subtasks and leads to an improvement in the average performance of the tasks.

{% include figure.html path="assets/img/2025-04-28-pcd/image_6.png" class="img-fluid rounded z-depth-1" %}

{% include figure.html path="assets/img/2025-04-28-pcd/image_7.png" class="img-fluid rounded z-depth-1" %}


## Conclusion
In this blog, the phenomenon of Rank Collapse is revealed, showing that as the input length increases, the Mean Reciprocal Rank (MRR) declines within the claimed length. The proposed Positional Contrastive Decoding is introduced to mitigate Rank Collapse.
PCD is a **plug-and-play** decoding method that can be seamlessly integrated with minimal effort by simply replacing the attention and sampling functions, requiring just two lines of code. Additionally, it is **training-free**, which enhances deployment efficiency while improving long-range awareness.
PCD has limitations in that it does not extend the model's maximum attention window length nor improve performance on short inputs.

### Future Directions
The fundamental insight of contrastive decoding is that **the context reattention can be achieved through the linear combination of logits**. There is also potential to design additional "necessary" or "disturbative" inputs to generate logits that enhance long-context capabilities. For example, logits from the long-context model can be adjusted by adding logits generated by Retrieval-Augmented Generation (RAG) models and subtracting those from short-claimed-length models. By linearly combining these different logits, the model can refine its ability to discern and prioritize relevant information within complex contexts.

## Code

{% highlight python %}
# Usage: 
# 1. insert pcd_enable in modeling_llama.py
# 2. replace LlamaFlashAttention2() with LlamaFlashAttention2_Positional_Contrastive_Decoding()
class LlamaFlashAttention2_Positional_Contrastive_Decoding(LlamaAttention):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self._flash_attn_uses_top_left_mask = not is_flash_attn_greater_or_equal_2_10()

    def forward(
        self,
        hidden_states: torch.Tensor,
        attention_mask: Optional[torch.LongTensor] = None,
        position_ids: Optional[torch.LongTensor] = None,
        past_key_value: Optional[Cache] = None,
        output_attentions: bool = False,
        use_cache: bool = False,
        cache_position: Optional[torch.LongTensor] = None,
        pcd_enable: Optional[bool] = None,  # new modified
    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:
        output_attentions = False
        bsz, q_len, _ = hidden_states.size()
        query_states = self.q_proj(hidden_states)
        key_states = self.k_proj(hidden_states)
        value_states = self.v_proj(hidden_states)
        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)
        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)
        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)

        cos, sin = self.rotary_emb(value_states, position_ids)
        if pcd_enable:
            start_idx = 0
            end_idx = int(self.head_dim / 2)
            noise_std = 0.03 
            noise_cos = torch.randn(bsz, q_len, end_idx - start_idx, device=cos.device) * noise_std
            noise_sin = torch.randn(bsz, q_len, end_idx - start_idx, device=sin.device) * noise_std
            cos[:, :, start_idx:end_idx].add_(noise_cos)
            sin[:, :, start_idx:end_idx].add_(noise_sin)
     
        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)

        if past_key_value is not None:
            # sin and cos are specific to RoPE models; cache_position needed for the static cache
            cache_kwargs = {"sin": sin, "cos": cos, "cache_position": cache_position}
            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)

        query_states = query_states.transpose(1, 2)
        key_states = key_states.transpose(1, 2)
        value_states = value_states.transpose(1, 2)

        dropout_rate = self.attention_dropout if self.training else 0.0

        input_dtype = query_states.dtype
        if input_dtype == torch.float32:
            if torch.is_autocast_enabled():
                target_dtype = torch.get_autocast_gpu_dtype()
            # Handle the case where the model is quantized
            elif hasattr(self.config, "_pre_quantization_dtype"):
                target_dtype = self.config._pre_quantization_dtype
            else:
                target_dtype = self.q_proj.weight.dtype
            query_states = query_states.to(target_dtype)
            key_states = key_states.to(target_dtype)
            value_states = value_states.to(target_dtype)

        attn_output = self._flash_attention_forward(
            query_states, key_states, value_states, attention_mask, q_len, dropout=dropout_rate
        )
        attn_output = attn_output.reshape(bsz, q_len, -1).contiguous()
        attn_output = self.o_proj(attn_output)
        if not output_attentions:
            attn_weights = None

        return attn_output, attn_weights, past_key_value
{% endhighlight %}

{% highlight python %}
# Usage: 
# transformers.generation.utils.GenerationMixin._sample = _sample
def _sample(
        self,
        input_ids: torch.LongTensor,
        logits_processor: LogitsProcessorList,
        stopping_criteria: StoppingCriteriaList,
        generation_config: GenerationConfig,
        synced_gpus: bool,
        streamer: Optional["BaseStreamer"],
        logits_warper: Optional[LogitsProcessorList],
        **model_kwargs,
    ) -> Union[GenerateNonBeamOutput, torch.LongTensor]:
        # init values
        pad_token_id = generation_config._pad_token_tensor
        output_attentions = generation_config.output_attentions
        output_hidden_states = generation_config.output_hidden_states
        output_scores = generation_config.output_scores
        output_logits = generation_config.output_logits
        return_dict_in_generate = generation_config.return_dict_in_generate
        has_eos_stopping_criteria = any(hasattr(criteria, "eos_token_id") for criteria in stopping_criteria)
        do_sample = generation_config.do_sample
        if do_sample is True and not isinstance(logits_warper, LogitsProcessorList):
            raise ValueError(
                "`do_sample` is set to `True`, `logits_warper` must be a `LogitsProcessorList` instance (it is "
                f"{logits_warper})."
            )

        # init attention / hidden states / scores tuples
        scores = () if (return_dict_in_generate and output_scores) else None
        raw_logits = () if (return_dict_in_generate and output_logits) else None
        decoder_attentions = () if (return_dict_in_generate and output_attentions) else None
        cross_attentions = () if (return_dict_in_generate and output_attentions) else None
        decoder_hidden_states = () if (return_dict_in_generate and output_hidden_states) else None

        # if model is an encoder-decoder, retrieve encoder attention weights and hidden states
        if return_dict_in_generate and self.config.is_encoder_decoder:
            encoder_attentions = model_kwargs["encoder_outputs"].get("attentions") if output_attentions else None
            encoder_hidden_states = (
                model_kwargs["encoder_outputs"].get("hidden_states") if output_hidden_states else None
            )

        # keep track of which sequences are already finished
        batch_size = input_ids.shape[0]
        this_peer_finished = False
        unfinished_sequences = torch.ones(batch_size, dtype=torch.long, device=input_ids.device)
        model_kwargs = self._get_initial_cache_position(input_ids, model_kwargs)

        calibration_enable = True
        prefilling_stage = True
        while self._has_unfinished_sequences(this_peer_finished, synced_gpus, device=input_ids.device):
            if calibration_enable:  
                # Step 1: prefilling
                if prefilling_stage:
                    if input_ids.shape[1] == 1:
                        raise ValueError("input_ids only contains one element.")
                    input_ids_next = input_ids[:, -1]
                    input_ids = input_ids[:, :-1]
                    model_kwargs['attention_mask'] = model_kwargs['attention_mask'][:, :-1]  
                    model_kwargs['cache_position'] = model_kwargs['cache_position'][:-1] 
                    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                    outputs = self(
                                    **model_inputs,
                                    return_dict=True,
                                    output_attentions=output_attentions,
                                    output_hidden_states=output_hidden_states,
                    )
                    input_ids = torch.cat([input_ids, input_ids_next[:, None]], dim=-1)
                    model_kwargs = self._update_model_kwargs_for_generation(
                        outputs,
                        model_kwargs,
                        is_encoder_decoder=self.config.is_encoder_decoder,
                    )
                    prefilling_stage = False

                # Step 2: Calculate noisy logits
                model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                model_inputs['position_ids'] += torch.randint(num, 10 * num, model_inputs['position_ids'].shape, device=model_inputs['position_ids'].device)
                model_inputs['position_ids'].clamp_(min=0)
                if synced_gpus and this_peer_finished: 
                    continue  # don't waste resources running the code we don't need
                with torch.no_grad():
                    outputs_chunk = self(
                        **model_inputs,
                        return_dict=True,
                        output_attentions=output_attentions,
                        output_hidden_states=output_hidden_states,
                        pcd_enable=True           # eable positional contrastive decoding
                    )
                # 裁剪kv cache
                model_inputs['past_key_values'].crop(model_inputs['attention_mask'].shape[1]-1)
                if synced_gpus and this_peer_finished:
                    continue  # don't waste resources running the code we don't need

                # Clone is needed to avoid keeping a hanging ref to outputs.logits which may be very large for first iteration
                # (the clone itself is always small)
                next_token_logits = outputs_chunk.logits[:, -1, :].clone()
                self.logits_pcd = next_token_logit
                
                # Step 3:
                # 3. formal next token predict
                model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                model_inputs["attention_mask"][0].fill_(1)
                with torch.no_grad():
                    outputs = self(
                        **model_inputs,
                        return_dict=True,
                        output_attentions=output_attentions,
                        output_hidden_states=output_hidden_states,
                    )

                if synced_gpus and this_peer_finished:
                    continue  # don't waste resources running the code we don't need

                # Clone is needed to avoid keeping a hanging ref to outputs.logits which may be very large for first iteration
                # (the clone itself is always small)
                next_token_logits_base = outputs.logits[:, -1, :].clone()                
                   
                calibrated_logits = (1 + 1.5) * next_token_logits_base - 1.5 * self.logits_pcd 
   
					      next_token_logits = calibrated_logits 
                
                top_k = 100
                _, topk_indices_ = torch.topk(next_token_logits_base, top_k, dim=-1)                
                mask = torch.full_like(next_token_logits, fill_value=-float('inf'))
                next_token_logits = mask.scatter(dim=-1, index=topk_indices_, src=next_token_logits.gather(dim=-1, index=topk_indices_))

            else:
                # prepare model inputs
                model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)

                # prepare variable output controls (note: some models won't accept all output controls)
                model_inputs.update({"output_attentions": output_attentions} if output_attentions else {})
                model_inputs.update({"output_hidden_states": output_hidden_states} if output_hidden_states else {})

                # forward pass to get next token
                outputs = self(**model_inputs, return_dict=True)

                if synced_gpus and this_peer_finished:
                    continue  # don't waste resources running the code we don't need

                # Clone is needed to avoid keeping a hanging ref to outputs.logits which may be very large for first iteration
                # (the clone itself is always small)
                next_token_logits = outputs.logits[:, -1, :].clone()
            
            # Remain Unchanged.
            # pre-process distribution
            next_token_scores = logits_processor(input_ids, next_token_logits)
            if do_sample:
                next_token_scores = logits_warper(input_ids, next_token_scores)

            # Store scores, attentions and hidden_states when required
            if return_dict_in_generate:
                if output_scores:
                    scores += (next_token_scores,)
                if output_logits:
                    raw_logits += (next_token_logits,)
                if output_attentions:
                    decoder_attentions += (
                        (outputs.decoder_attentions,) if self.config.is_encoder_decoder else (outputs.attentions,)
                    )
                    if self.config.is_encoder_decoder:
                        cross_attentions += (outputs.cross_attentions,)

                if output_hidden_states:
                    decoder_hidden_states += (
                        (outputs.decoder_hidden_states,)
                        if self.config.is_encoder_decoder
                        else (outputs.hidden_states,)
                    )

            # token selection
            if do_sample:
                probs = nn.functional.softmax(next_token_scores, dim=-1)
                next_tokens = torch.multinomial(probs, num_samples=1).squeeze(1)
            else:
                next_tokens = torch.argmax(next_token_scores, dim=-1)

            # finished sentences should have their next token be a padding token
            if has_eos_stopping_criteria:
                next_tokens = next_tokens * unfinished_sequences + pad_token_id * (1 - unfinished_sequences)

            # update generated ids, model inputs, and length for next step
            input_ids = torch.cat([input_ids, next_tokens[:, None]], dim=-1)
            if streamer is not None:
                streamer.put(next_tokens.cpu())
            model_kwargs = self._update_model_kwargs_for_generation(
                outputs,
                model_kwargs,
                is_encoder_decoder=self.config.is_encoder_decoder,
            )

            unfinished_sequences = unfinished_sequences & ~stopping_criteria(input_ids, scores)
            this_peer_finished = unfinished_sequences.max() == 0

            # This is needed to properly delete outputs.logits which may be very large for first iteration
            # Otherwise a reference to outputs is kept which keeps the logits alive in the next iteration
            del outputs

        if streamer is not None:
            streamer.end()

        if return_dict_in_generate:
            if self.config.is_encoder_decoder:
                return GenerateEncoderDecoderOutput(
                    sequences=input_ids,
                    scores=scores,
                    logits=raw_logits,
                    encoder_attentions=encoder_attentions,
                    encoder_hidden_states=encoder_hidden_states,
                    decoder_attentions=decoder_attentions,
                    cross_attentions=cross_attentions,
                    decoder_hidden_states=decoder_hidden_states,
                    past_key_values=model_kwargs.get("past_key_values"),
                )
            else:
                return GenerateDecoderOnlyOutput(
                    sequences=input_ids,
                    scores=scores,
                    logits=raw_logits,
                    attentions=decoder_attentions,
                    hidden_states=decoder_hidden_states,
                    past_key_values=model_kwargs.get("past_key_values"),
                )
        else:
            return input_ids
{% endhighlight %}