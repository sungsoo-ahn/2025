@book{nesterov2018lectures,
  title={Lectures on convex optimization},
  author={Nesterov, Yurii and others},
  volume={137},
  year={2018},
  publisher={Springer},
  url={https://link.springer.com/book/10.1007/978-3-319-91578-4}
}

@article{bubeck2015convex,
  title={Convex optimization: Algorithms and complexity},
  author={Bubeck, S{\'e}bastien and others},
  journal={Foundations and Trends in Machine Learning},
  volume={8},
  number={3-4},
  pages={231--357},
  year={2015},
  publisher={Now Publishers, Inc.},
  url={https://arxiv.org/abs/1405.4980}
}

@book{blair1985problem,
	author = {Nemirovski, Arkadi. S. and  Yudin, David. B.},
	title = {{Problem Complexity and Method Efficiency in Optimization}},
	publisher = {Wiley, New York},
	year = {1983}
}

@article{carmon2021lower,
  title={Lower bounds for finding stationary points II: first-order methods},
  author={Carmon, Yair and Duchi, John C and Hinder, Oliver and Sidford, Aaron},
  journal={Mathematical Programming},
  volume={185},
  number={1},
  pages={315--355},
  year={2021},
  publisher={Springer},
  url={https://link.springer.com/article/10.1007/s10107-019-01431-x}
}
@article{carmon2020lower,
  title={Lower bounds for finding stationary points I},
  author={Carmon, Yair and Duchi, John C and Hinder, Oliver and Sidford, Aaron},
  journal={Mathematical Programming},
  volume={184},
  number={1},
  pages={71--120},
  year={2020},
  publisher={Springer},
  url={https://link.springer.com/article/10.1007/s10107-019-01406-y}
}

@article{davis2018stochastic,
  title={Stochastic subgradient method converges at the rate $ O (k^{-1/4}) $ on weakly convex functions},
  author={Davis, Damek and Drusvyatskiy, Dmitriy},
  journal={arXiv preprint arXiv:1802.02988},
  year={2018},
  url={https://arxiv.org/abs/1802.02988}
}

@article{defazio2016simple,
  title={A simple practical accelerated method for finite sums},
  author={Defazio, Aaron},
  journal={Advances in neural information processing systems},
  volume={29},
  year={2016},
  url={https://proceedings.neurips.cc/paper_files/paper/2016/hash/4f6ffe13a5d75b2d6a3923922b3922e5-Abstract.html}
}

@article{allen2018katyusha,
  title={Katyusha: The first direct acceleration of stochastic gradient methods},
  author={Allen-Zhu, Zeyuan},
  journal={Journal of Machine Learning Research},
  volume={18},
  number={221},
  pages={1--51},
  year={2018},
  url={https://www.jmlr.org/papers/v18/16-410.html}
}

@article{woodworth2016tight,
  title={Tight complexity bounds for optimizing composite objectives},
  author={Woodworth, Blake E and Srebro, Nati},
  journal={Advances in neural information processing systems},
  volume={29},
  year={2016},
  url={https://proceedings.neurips.cc/paper/2016/hash/645098b086d2f9e1e0e939c27f9f2d6f-Abstract.html}
}

@inproceedings{allen2018katyusha,
  title={Katyusha x: Simple momentum method for stochastic sum-of-nonconvex optimization},
  author={Allen-Zhu, Zeyuan},
  booktitle={International Conference on Machine Learning},
  pages={179--185},
  year={2018},
  organization={PMLR},
  url={https://proceedings.mlr.press/v80/allen-zhu18a.html}
}

@article{xie2019general,
  title={A general analysis framework of lower complexity bounds for finite-sum optimization},
  author={Xie, Guangzeng and Luo, Luo and Zhang, Zhihua},
  journal={arXiv preprint arXiv:1908.08394},
  year={2019},
  url={https://arxiv.org/abs/1908.08394}
}

@inproceedings{zhou2019lower,
  title={Lower bounds for smooth nonconvex finite-sum optimization},
  author={Zhou, Dongruo and Gu, Quanquan},
  booktitle={International Conference on Machine Learning},
  pages={7574--7583},
  year={2019},
  organization={PMLR},
  url={https://proceedings.mlr.press/v97/zhou19b.html}
}

@article{wang2019spiderboost,
  title={Spiderboost and momentum: Faster variance reduction algorithms},
  author={Wang, Zhe and Ji, Kaiyi and Zhou, Yi and Liang, Yingbin and Tarokh, Vahid},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019},
  url={https://proceedings.neurips.cc/paper/2019/hash/512c5cad6c37edb98ae91c8a76c3a291-Abstract.html}
}

@article{fang2018spider,
  title={Spider: Near-optimal non-convex optimization via stochastic path-integrated differential estimator},
  author={Fang, Cong and Li, Chris Junchi and Lin, Zhouchen and Zhang, Tong},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018},
  url={https://proceedings.neurips.cc/paper_files/paper/2018/hash/1543843a4723ed2ab08e18053ae6dc5b-Abstract.html}
}

@article{ghadimi2012optimal,
  title={Optimal stochastic approximation algorithms for strongly convex stochastic composite optimization i: A generic algorithmic framework},
  author={Ghadimi, Saeed and Lan, Guanghui},
  journal={SIAM Journal on Optimization},
  volume={22},
  number={4},
  pages={1469--1492},
  year={2012},
  publisher={SIAM},
  url={https://epubs.siam.org/doi/abs/10.1137/110848864}
}

@inproceedings{rakhlin2012making,
  title={Making gradient descent optimal for strongly convex stochastic optimization},
  author={Rakhlin, Alexander and Shamir, Ohad and Sridharan, Karthik},
  booktitle={Proceedings of the 29th International Coference on International Conference on Machine Learning},
  pages={1571--1578},
  year={2012},
  url={https://icml.cc/2012/papers/261.pdf}
}

@article{lan2012optimal,
  title={An optimal method for stochastic composite optimization},
  author={Lan, Guanghui},
  journal={Mathematical Programming},
  volume={133},
  number={1},
  pages={365--397},
  year={2012},
  publisher={Springer},
  url={https://link.springer.com/article/10.1007/s10107-010-0434-y}
}

@article{woodworth2018graph,
  title={Graph oracle models, lower bounds, and gaps for parallel stochastic optimization},
  author={Woodworth, Blake E and Wang, Jialei and Smith, Adam and McMahan, Brendan and Srebro, Nati},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018},
  url={https://proceedings.neurips.cc/paper_files/paper/2018/hash/3ec27c2cff04bc5fd2586ca36c62044e-Abstract.html}
}

@article{hazan2014beyond,
  title={Beyond the regret minimization barrier: optimal algorithms for stochastic strongly-convex optimization},
  author={Hazan, Elad and Kale, Satyen},
  journal={The Journal of Machine Learning Research},
  volume={15},
  number={1},
  pages={2489--2512},
  year={2014},
  url={https://www.jmlr.org/papers/volume15/hazan14a/hazan14a.pdf}
}

@article{nemirovski2009robust,
  title={Robust stochastic approximation approach to stochastic programming},
  author={Nemirovski, Arkadi and Juditsky, Anatoli and Lan, Guanghui and Shapiro, Alexander},
  journal={SIAM Journal on optimization},
  volume={19},
  number={4},
  pages={1574--1609},
  year={2009},
  publisher={SIAM},
  url={https://epubs.siam.org/doi/abs/10.1137/070704277}
}

@article{agarwal2009information,
  title={Information-theoretic lower bounds on the oracle complexity of convex optimization},
  author={Agarwal, Alekh and Wainwright, Martin J and Bartlett, Peter and Ravikumar, Pradeep},
  journal={Advances in Neural Information Processing Systems},
  volume={22},
  year={2009},
  url={https://proceedings.neurips.cc/paper/2009/hash/2387337ba1e0b0249ba90f55b2ba2521-Abstract.html}
}

@inproceedings{foster2019complexity,
  title={The complexity of making the gradient small in stochastic convex optimization},
  author={Foster, Dylan J and Sekhari, Ayush and Shamir, Ohad and Srebro, Nathan and Sridharan, Karthik and Woodworth, Blake},
  booktitle={Conference on Learning Theory},
  pages={1319--1345},
  year={2019},
  organization={PMLR},
  url={https://proceedings.mlr.press/v99/foster19b.html}
}

@article{ghadimi2013stochastic,
  title={Stochastic first-and zeroth-order methods for nonconvex stochastic programming},
  author={Ghadimi, Saeed and Lan, Guanghui},
  journal={SIAM journal on optimization},
  volume={23},
  number={4},
  pages={2341--2368},
  year={2013},
  publisher={SIAM},
  url={https://epubs.siam.org/doi/abs/10.1137/120880811}
}

@article{arjevani2023lower,
  title={Lower bounds for non-convex stochastic optimization},
  author={Arjevani, Yossi and Carmon, Yair and Duchi, John C and Foster, Dylan J and Srebro, Nathan and Woodworth, Blake},
  journal={Mathematical Programming},
  volume={199},
  number={1},
  pages={165--214},
  year={2023},
  publisher={Springer},
  url={https://link.springer.com/article/10.1007/s10107-022-01822-7}
}

@article{chambolle2011first,
  title={A first-order primal-dual algorithm for convex problems with applications to imaging},
  author={Chambolle, Antonin and Pock, Thomas},
  journal={Journal of mathematical imaging and vision},
  volume={40},
  pages={120--145},
  year={2011},
  publisher={Springer},
  url={https://link.springer.com/article/10.1007/s10851-010-0251-1}
}

@article{ouyang2021lower,
  title={Lower complexity bounds of first-order methods for convex-concave bilinear saddle-point problems},
  author={Ouyang, Yuyuan and Xu, Yangyang},
  journal={Mathematical Programming},
  volume={185},
  number={1},
  pages={1--35},
  year={2021},
  publisher={Springer},
  url={https://link.springer.com/article/10.1007/s10107-019-01420-0}
}

@article{nemirovski2004prox,
  title={Prox-method with rate of convergence O (1/t) for variational inequalities with Lipschitz continuous monotone operators and smooth convex-concave saddle point problems},
  author={Nemirovski, Arkadi},
  journal={SIAM Journal on Optimization},
  volume={15},
  number={1},
  pages={229--251},
  year={2004},
  publisher={SIAM},
  url={https://epubs.siam.org/doi/abs/10.1137/S1052623403425629}
}

@article{mokhtari2020convergence,
  title={Convergence rate of O(1/k) for optimistic gradient and extragradient methods in smooth convex-concave saddle point problems},
  author={Mokhtari, Aryan and Ozdaglar, Asuman E and Pattathil, Sarath},
  journal={SIAM Journal on Optimization},
  volume={30},
  number={4},
  pages={3230--3251},
  year={2020},
  publisher={SIAM},
  url={https://epubs.siam.org/doi/abs/10.1137/19M127375X}
}

@inproceedings{xie2020lower,
  title={Lower complexity bounds for finite-sum convex-concave minimax optimization problems},
  author={Xie, Guangzeng and Luo, Luo and Lian, Yijiang and Zhang, Zhihua},
  booktitle={International Conference on Machine Learning},
  pages={10504--10513},
  year={2020},
  organization={PMLR},
  url={https://proceedings.mlr.press/v119/xie20d.html}
}

@article{yang2020catalyst,
  title={A catalyst framework for minimax optimization},
  author={Yang, Junchi and Zhang, Siqi and Kiyavash, Negar and He, Niao},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={5667--5678},
  year={2020},
  url={https://proceedings.neurips.cc/paper/2020/hash/3db54f5573cd617a0112d35dd1e6b1ef-Abstract.html}
}

@inproceedings{du2019linear,
  title={Linear convergence of the primal-dual gradient method for convex-concave saddle point problems without strong convexity},
  author={Du, Simon S and Hu, Wei},
  booktitle={The 22nd International Conference on Artificial Intelligence and Statistics},
  pages={196--205},
  year={2019},
  organization={PMLR},
  url={https://proceedings.mlr.press/v89/du19b.html}
}

@article{chambolle2016ergodic,
  title={On the ergodic convergence rates of a first-order primal--dual algorithm},
  author={Chambolle, Antonin and Pock, Thomas},
  journal={Mathematical Programming},
  volume={159},
  number={1},
  pages={253--287},
  year={2016},
  publisher={Springer},
  url={https://link.springer.com/article/10.1007/s10107-015-0957-3}
}

@article{zhang2022lower,
  title={On lower iteration complexity bounds for the convex concave saddle point problems},
  author={Zhang, Junyu and Hong, Mingyi and Zhang, Shuzhong},
  journal={Mathematical Programming},
  volume={194},
  number={1},
  pages={901--935},
  year={2022},
  publisher={Springer},
  url={https://link.springer.com/article/10.1007/s10107-021-01660-z}
}

@article{wang2020improved,
  title={Improved algorithms for convex-concave minimax optimization},
  author={Wang, Yuanhao and Li, Jian},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={4800--4810},
  year={2020},
  url={https://proceedings.neurips.cc/paper/2020/hash/331316d4efb44682092a006307b9ae3a-Abstract.html}
}

@inproceedings{liang2019interaction,
  title={Interaction matters: A note on non-asymptotic local convergence of generative adversarial networks},
  author={Liang, Tengyuan and Stokes, James},
  booktitle={The 22nd International Conference on Artificial Intelligence and Statistics},
  pages={907--915},
  year={2019},
  organization={PMLR},
  url={https://proceedings.mlr.press/v89/liang19b.html}
}

@inproceedings{ibrahim2020linear,
  title={Linear lower bounds and conditioning of differentiable games},
  author={Ibrahim, Adam and Azizian, Wa{\i}ss and Gidel, Gauthier and Mitliagkas, Ioannis},
  booktitle={International conference on machine learning},
  pages={4583--4593},
  year={2020},
  organization={PMLR},
  url={https://proceedings.mlr.press/v119/ibrahim20a.html}
}

@article{arjevani2016lower,
  title={On lower and upper bounds in smooth and strongly convex optimization},
  author={Arjevani, Yossi and Shalev-Shwartz, Shai and Shamir, Ohad},
  journal={Journal of Machine Learning Research},
  volume={17},
  number={126},
  pages={1--51},
  year={2016},
  url={https://www.jmlr.org/papers/v17/15-106.html}
}

@article{ruder2016overview,
  title={An overview of gradient descent optimization algorithms},
  author={Ruder, Sebastian},
  journal={arXiv preprint arXiv:1609.04747},
  year={2016},
  url={https://arxiv.org/abs/1609.04747}
}

@article{yazdandoost2023stochastic,
  title={A stochastic variance-reduced accelerated primal-dual method for finite-sum saddle-point problems},
  author={Yazdandoost Hamedani, Erfan and Jalilzadeh, Afrooz},
  journal={Computational Optimization and Applications},
  volume={85},
  number={2},
  pages={653--679},
  year={2023},
  publisher={Springer},
  url={https://link.springer.com/article/10.1007/s10589-023-00472-5}
}

@article{juditsky2011solving,
  title={Solving variational inequalities with stochastic mirror-prox algorithm},
  author={Juditsky, Anatoli and Nemirovski, Arkadi and Tauvel, Claire},
  journal={Stochastic Systems},
  volume={1},
  number={1},
  pages={17--58},
  year={2011},
  publisher={INFORMS},
  url={https://pubsonline.informs.org/doi/abs/10.1287/10-SSY011}
}

@article{palaniappan2016stochastic,
  title={Stochastic variance reduction methods for saddle-point problems},
  author={Palaniappan, Balamurugan and Bach, Francis},
  journal={Advances in Neural Information Processing Systems},
  volume={29},
  year={2016},
  url={https://proceedings.neurips.cc/paper_files/paper/2016/hash/1aa48fc4880bb0c9b8a3bf979d3b917e-Abstract.html}
}

@article{hsieh2019convergence,
  title={On the convergence of single-call stochastic extra-gradient methods},
  author={Hsieh, Yu-Guan and Iutzeler, Franck and Malick, J{\'e}r{\^o}me and Mertikopoulos, Panayotis},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019},
  url={https://proceedings.neurips.cc/paper/2019/hash/4625d8e31dad7d1c4c83399a6eb62f0c-Abstract.html}
}

@article{yan2020optimal,
  title={Optimal epoch stochastic gradient descent ascent methods for min-max optimization},
  author={Yan, Yan and Xu, Yi and Lin, Qihang and Liu, Wei and Yang, Tianbao},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={5789--5800},
  year={2020},
  url={https://proceedings.neurips.cc/paper_files/paper/2020/hash/3f8b2a81da929223ae025fcec26dde0d-Abstract.html}
}

@article{luo2019stochastic,
  title={A stochastic proximal point algorithm for saddle-point problems},
  author={Luo, Luo and Chen, Cheng and Li, Yujun and Xie, Guangzeng and Zhang, Zhihua},
  journal={arXiv preprint arXiv:1909.06946},
  year={2019},
  url={https://arxiv.org/abs/1909.06946}
}

@article{chavdarova2019reducing,
  title={Reducing noise in GAN training with variance reduced extragradient},
  author={Chavdarova, Tatjana and Gidel, Gauthier and Fleuret, Fran{\c{c}}ois and Lacoste-Julien, Simon},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019},
  url={https://proceedings.neurips.cc/paper/2019/hash/58a2fc6ed39fd083f55d4182bf88826d-Abstract.html}
}

@inproceedings{zhang2022beyond,
  title={Beyond Worst-Case Analysis in Stochastic Approximation: Moment Estimation Improves Instance Complexity},
  author={Zhang, Jingzhao and Lin, Hongzhou and Das, Subhro and Sra, Suvrit and Jadbabaie, Ali},
  booktitle={International Conference on Machine Learning},
  pages={26347--26361},
  year={2022},
  organization={PMLR},
  url={https://proceedings.mlr.press/v162/zhang22r}
}

@inproceedings{zhang2021complexity,
  title={The complexity of nonconvex-strongly-concave minimax optimization},
  author={Zhang, Siqi and Yang, Junchi and Guzm{\'a}n, Crist{\'o}bal and Kiyavash, Negar and He, Niao},
  booktitle={Uncertainty in Artificial Intelligence},
  pages={482--492},
  year={2021},
  organization={PMLR},
  url={https://proceedings.mlr.press/v161/zhang21c.html}
}

@inproceedings{lin2020near,
  title={Near-optimal algorithms for minimax optimization},
  author={Lin, Tianyi and Jin, Chi and Jordan, Michael I},
  booktitle={Conference on Learning Theory},
  pages={2738--2779},
  year={2020},
  organization={PMLR},
  url={https://proceedings.mlr.press/v125/lin20a.html}
}

@inproceedings{lin2020gradient,
  title={On gradient descent ascent for nonconvex-concave minimax problems},
  author={Lin, Tianyi and Jin, Chi and Jordan, Michael},
  booktitle={International Conference on Machine Learning},
  pages={6083--6093},
  year={2020},
  organization={PMLR},
  url={https://proceedings.mlr.press/v119/lin20a.html}
}

@article{boct2023alternating,
  title={Alternating proximal-gradient steps for (stochastic) nonconvex-concave minimax problems},
  author={Bo{\c{t}}, Radu Ioan and B{\"o}hm, Axel},
  journal={SIAM Journal on Optimization},
  volume={33},
  number={3},
  pages={1884--1913},
  year={2023},
  publisher={SIAM},
  url={https://epubs.siam.org/doi/full/10.1137/21M1465470}
}

@article{thekumparampil2019efficient,
  title={Efficient algorithms for smooth minimax optimization},
  author={Thekumparampil, Kiran K and Jain, Prateek and Netrapalli, Praneeth and Oh, Sewoong},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019},
  url={https://proceedings.neurips.cc/paper_files/paper/2019/hash/05d0abb9a864ae4981e933685b8b915c-Abstract.html}
}

@article{zhang2022sapd+,
  title={Sapd+: An accelerated stochastic method for nonconvex-concave minimax problems},
  author={Zhang, Xuan and Aybat, Necdet Serhat and Gurbuzbalaban, Mert},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={21668--21681},
  year={2022},
  url={https://proceedings.neurips.cc/paper_files/paper/2022/hash/880d8999c07a8efc9bbbeb0c38f50765-Abstract-Conference.html}
}

@article{zhang2020single,
  title={A single-loop smoothed gradient descent-ascent algorithm for nonconvex-concave min-max problems},
  author={Zhang, Jiawei and Xiao, Peijun and Sun, Ruoyu and Luo, Zhiquan},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={7377--7389},
  year={2020},
  url={https://proceedings.neurips.cc/paper/2020/hash/52aaa62e71f829d41d74892a18a11d59-Abstract.html}
}

@article{yang2020global,
  title={Global convergence and variance reduction for a class of nonconvex-nonconcave minimax problems},
  author={Yang, Junchi and Kiyavash, Negar and He, Niao},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={1153--1165},
  year={2020},
  url={https://proceedings.neurips.cc/paper/2020/hash/0cc6928e741d75e7a92396317522069e-Abstract.html}
}

@inproceedings{yang2022faster,
  title={Faster single-loop algorithms for minimax optimization without strong concavity},
  author={Yang, Junchi and Orvieto, Antonio and Lucchi, Aurelien and He, Niao},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={5485--5517},
  year={2022},
  organization={PMLR},
  url={https://proceedings.mlr.press/v151/yang22b.html}
}

@article{xu2023unified,
  title={A unified single-loop alternating gradient projection algorithm for nonconvex--concave and convex--nonconcave minimax problems},
  author={Xu, Zi and Zhang, Huiling and Xu, Yang and Lan, Guanghui},
  journal={Mathematical Programming},
  volume={201},
  number={1},
  pages={635--706},
  year={2023},
  publisher={Springer},
  url={,
  url={https://proceedings.mlr.press/v151/yang22b.html}}
}

@inproceedings{xu2019slides,
  title={(Slides) Lower complexity bounds of first-order methods for convex-concave bilinear saddle-point problems},
  author={Xu, Yangyang},
  year={2019},
  url={https://xu-yangyang.github.io/slides/LowerBnd.pdf}
}

@article{johnson2013accelerating,
  title={Accelerating stochastic gradient descent using predictive variance reduction},
  author={Johnson, Rie and Zhang, Tong},
  journal={Advances in neural information processing systems},
  volume={26},
  year={2013},
  url={https://proceedings.neurips.cc/paper/2013/file/ac1dd209cbcc5e5d1c6e28598e8cbbe8-Paper.pdf}
}

@article{garrigos2023handbook,
  title={Handbook of convergence theorems for (stochastic) gradient methods},
  author={Garrigos, Guillaume and Gower, Robert M},
  journal={arXiv preprint arXiv:2301.11235},
  year={2023},
  url={https://arxiv.org/abs/2301.11235}
}

@article{nemirovskij1983problem,
  title={Problem complexity and method efficiency in optimization},
  author={Nemirovskij, Arkadij Semenovi{\v{c}} and Yudin, David Borisovich},
  year={1983},
  publisher={Wiley-Interscience}
}

@incollection{danilova2022recent,
  title={Recent theoretical advances in non-convex optimization},
  author={Danilova, Marina and Dvurechensky, Pavel and Gasnikov, Alexander and Gorbunov, Eduard and Guminov, Sergey and Kamzolov, Dmitry and Shibaev, Innokentiy},
  booktitle={High-Dimensional Optimization and Probability: With a View Towards Data Science},
  pages={79--163},
  year={2022},
  publisher={Springer},
  url={https://link.springer.com/chapter/10.1007/978-3-031-00832-0_3}
}

@article{duchi2011adaptive,
  title={Adaptive subgradient methods for online learning and stochastic optimization.},
  author={Duchi, John and Hazan, Elad and Singer, Yoram},
  journal={Journal of machine learning research},
  volume={12},
  number={7},
  year={2011}
}

@inproceedings{kingma2014adam,
  title={Adam: A method for stochastic optimization},
  author={Diederik P. Kingma and Jimmy Ba},
  booktitle={International Conference on Learning Representations},
  year={2015}
}

@inproceedings{boyd2024text,
  title={EE364a: Convex Optimization I},
  author={Boyd, Stephen},
  year={2024},
  url={https://web.stanford.edu/class/ee364a/}
}

@inproceedings{boyd2024video,
  title={Stanford EE364A Convex Optimization Course Video (Stanford Online)},
  author={Boyd, Stephen},
  year={2024},
  url={https://www.youtube.com/playlist?list=PLoROMvodv4rMJqxxviPa4AmDClvcbHi6h}
}

@article{sun2019optimization,
  title={Optimization for deep learning: theory and algorithms},
  author={Sun, Ruoyu},
  journal={arXiv preprint arXiv:1912.08957},
  year={2019}
}

@article{dvurechensky2021first,
  title={First-order methods for convex optimization},
  author={Dvurechensky, Pavel and Shtern, Shimrit and Staudigl, Mathias},
  journal={EURO Journal on Computational Optimization},
  volume={9},
  pages={100015},
  year={2021},
  publisher={Elsevier}
}

@inproceedings{sun2021list,
  title={Provable Nonconvex Methods/Algorithms},
  author={Sun, Ju},
  year={2021},
  url={https://sunju.org/research/nonconvex/}
}

@article{sun2019survey,
  title={A survey of optimization methods from a machine learning perspective},
  author={Sun, Shiliang and Cao, Zehui and Zhu, Han and Zhao, Jing},
  journal={IEEE transactions on cybernetics},
  volume={50},
  number={8},
  pages={3668--3681},
  year={2019},
  publisher={IEEE}
}

@article{defazio2019ineffectiveness,
  title={On the ineffectiveness of variance reduced optimization for deep learning},
  author={Defazio, Aaron and Bottou, L{\'e}on},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}

@article{bottou2018optimization,
  title={Optimization methods for large-scale machine learning},
  author={Bottou, L{\'e}on and Curtis, Frank E and Nocedal, Jorge},
  journal={SIAM review},
  volume={60},
  number={2},
  pages={223--311},
  year={2018},
  publisher={SIAM}
}

@article{wang2017stochastic,
  title={Stochastic compositional gradient descent: algorithms for minimizing compositions of expected-value functions},
  author={Wang, Mengdi and Fang, Ethan X and Liu, Han},
  journal={Mathematical Programming},
  volume={161},
  pages={419--449},
  year={2017},
  publisher={Springer}
}

@article{ghadimi2018approximation,
  title={Approximation methods for bilevel programming},
  author={Ghadimi, Saeed and Wang, Mengdi},
  journal={arXiv preprint arXiv:1802.02246},
  year={2018}
}

@article{hong2020two,
  title={A two-timescale stochastic algorithm framework for bilevel optimization: Complexity analysis and application to actor-critic},
  author={Hong, Mingyi and Wai, Hoi-To and Wang, Zhaoran and Yang, Zhuoran},
  journal={SIAM Journal on Optimization},
  volume={33},
  number={1},
  pages={147--180},
  year={2023},
  publisher={SIAM}
}

@inproceedings{chen2021tighter,
  title={Closing the Gap: Tighter Analysis of Alternating Stochastic Gradient Methods for Bilevel Problems},
  author={Chen, Tianyi and Sun, Yuejiao and Yin, Wotao},
  booktitle={Advances in Neural Information Processing Systems},
  year={2021}
}

@article{guo2021randomized,
  title={Randomized Stochastic Variance-Reduced Methods for Stochastic Bilevel Optimization},
  author={Guo, Zhishuai and Yang, Tianbao},
  journal={arXiv preprint arXiv:2105.02266},
  year={2021}
}

@inproceedings{kwon2023fully,
  title={A fully first-order method for stochastic bilevel optimization},
  author={Kwon, Jeongyeol and Kwon, Dohyun and Wright, Stephen and Nowak, Robert D},
  booktitle={International Conference on Machine Learning},
  pages={18083--18113},
  year={2023},
  organization={PMLR}
}

@article{xiao2024unlocking,
  title={Unlocking Global Optimality in Bilevel Optimization: A Pilot Study},
  author={Xiao, Quan and Chen, Tianyi},
  journal={arXiv preprint arXiv:2408.16087},
  year={2024}
}

@article{hu2024contextual,
  title={Contextual stochastic bilevel optimization},
  author={Hu, Yifan and Wang, Jie and Xie, Yao and Krause, Andreas and Kuhn, Daniel},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@inproceedings{chen2022adaptive,
  title={Adaptive model design for Markov decision process},
  author={Chen, Siyu and Yang, Donglin and Li, Jiayang and Wang, Senmiao and Yang, Zhuoran and Wang, Zhaoran},
  booktitle={International Conference on Machine Learning},
  pages={3679--3700},
  year={2022},
  organization={PMLR}
}
@inproceedings{thoma2024contextual,
  title={Contextual Bilevel Reinforcement Learning for Incentive Alignment},
  author={Thoma, Vinzenz and P{\'a}sztor, Barna and Krause, Andreas and Ramponi, Giorgia and Hu, Yifan},
  booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},
  year={2024},
}

@inproceedings{chakraborty2024parl,
  title={PARL: A unified framework for policy alignment in reinforcement learning from human feedback},
  author={Chakraborty, Souradip and Bedi, Amrit and Koppel, Alec and Wang, Huazheng and Manocha, Dinesh and Wang, Mengdi and Huang, Furong},
  booktitle={The Twelfth International Conference on Learning Representations},
  year={2024}
}

@article{jiang2024barrier,
  title={Barrier Function for Bilevel Optimization with Coupled Lower-Level Constraints: Formulation, Approximation and Algorithms},
  author={Jiang, Xiaotian and Li, Jiaxiang and Hong, Mingyi and Zhang, Shuzhong},
  journal={arXiv preprint arXiv:2410.10670},
  year={2024}
}

@article{khanduri2021near,
  title={A near-optimal algorithm for stochastic bilevel optimization via double-momentum},
  author={Khanduri, Prashant and Zeng, Siliang and Hong, Mingyi and Wai, Hoi-To and Wang, Zhaoran and Yang, Zhuoran},
  journal={Advances in neural information processing systems},
  volume={34},
  pages={30271--30283},
  year={2021}
}

  @article{hu2020sample,
  title={Sample complexity of sample average approximation for conditional stochastic optimization},
  author={Hu, Yifan and Chen, Xin and He, Niao},
  journal={SIAM Journal on Optimization},
  volume={30},
  number={3},
  pages={2103--2133},
  year={2020},
  publisher={SIAM}
}

@article{kuhn2024distributionally,
  title={Distributionally robust optimization},
  author={Kuhn, Daniel and Wiesemann, Wolfram and  and Sim, Melvyn},
  journal={Operations research},
  volume={62},
  number={6},
  pages={1358--1376},
  year={2014},
  publisher={INFORMS}
}

@article{kuhn2024distributionally,
  title={Distributionally robust optimization},
  author={Kuhn, Daniel and Shafiee, Soroosh and Wiesemann, Wolfram},
  journal={arXiv preprint arXiv:2411.02549},
  year={2024}
}

@article{chen2024efficient,
  title={Efficient Algorithms for a Class of Stochastic Hidden Convex Optimization and Its Applications in Network Revenue Management},
  author={Chen, Xin and He, Niao and Hu, Yifan and Ye, Zikun},
  journal={Operations Research},
  year={2024},
  publisher={INFORMS}
}

@article{zhang2020variational,
  title={Variational policy gradient method for reinforcement learning with general utilities},
  author={Zhang, Junyu and Koppel, Alec and Bedi, Amrit Singh and Szepesvari, Csaba and Wang, Mengdi},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={4572--4583},
  year={2020}
}

@article{anderson2019system,
  title={System level synthesis},
  author={Anderson, James and Doyle, John C and Low, Steven H and Matni, Nikolai},
  journal={Annual Reviews in Control},
  volume={47},
  pages={364--393},
  year={2019},
  publisher={Elsevier}
}

@article{chen2023network,
  title={Network revenue management with online inverse batch gradient descent method},
  author={Chen, Yiwei and Shi, Cong},
  journal={Production and Operations Management},
  volume={32},
  number={7},
  pages={2123--2137},
  year={2023},
  publisher={SAGE Publications Sage CA: Los Angeles, CA}
}

@article{fatkhullin2023stochastic,
  title={Stochastic optimization under hidden convexity},
  author={Fatkhullin, Ilyas and He, Niao and Hu, Yifan},
  journal={arXiv preprint arXiv:2401.00108},
  year={2023}
}

@article{zhang2021convergence,
  title={On the convergence and sample efficiency of variance-reduced policy gradient method},
  author={Zhang, Junyu and Ni, Chengzhuo and Szepesvari, Csaba and Wang, Mengdi and others},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={2228--2240},
  year={2021}
}

@inproceedings{karimi2016linear,
  title={Linear convergence of gradient and proximal-gradient methods under the polyak-{\l}ojasiewicz condition},
  author={Karimi, Hamed and Nutini, Julie and Schmidt, Mark},
  booktitle={Machine Learning and Knowledge Discovery in Databases: European Conference, ECML PKDD 2016, Riva del Garda, Italy, September 19-23, 2016, Proceedings, Part I 16},
  pages={795--811},
  year={2016},
  organization={Springer},
  url={https://arxiv.org/pdf/1608.04636}
}

@article{chen2024landscape,
  title={Landscape of Policy Optimization for Finite Horizon MDPs with General State and Action},
  author={Chen, Xin and Hu, Yifan and Zhao, Minda},
  journal={arXiv preprint arXiv:2409.17138},
  year={2024}
}

@inproceedings{fatkhullin2023stochastic,
  title={Stochastic policy gradient methods: Improved sample complexity for fisher-non-degenerate policies},
  author={Fatkhullin, Ilyas and Barakat, Anas and Kireeva, Anastasia and He, Niao},
  booktitle={International Conference on Machine Learning},
  pages={9827--9869},
  year={2023},
  organization={PMLR}
}

@book{nesterov2013introductory,
  title={Introductory lectures on convex optimization: A basic course},
  author={Nesterov, Yurii},
  volume={87},
  year={2013},
  publisher={Springer Science \& Business Media}
}

@inproceedings{chewi2023complexity,
  title={On the complexity of finding stationary points of smooth functions in one dimension},
  author={Chewi, Sinho and Bubeck, S{\'e}bastien and Salim, Adil},
  booktitle={International Conference on Algorithmic Learning Theory},
  pages={358--374},
  year={2023},
  organization={PMLR}
}

@article{fatkhullin2022sharp,
  title={Sharp analysis of stochastic optimization under global Kurdyka-Lojasiewicz inequality},
  author={Fatkhullin, Ilyas and Etesami, Jalal and He, Niao and Kiyavash, Negar},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={15836--15848},
  year={2022}
}

@article{agarwal2009information,
  title={Information-theoretic lower bounds on the oracle complexity of convex optimization},
  author={Agarwal, Alekh and Wainwright, Martin J and Bartlett, Peter and Ravikumar, Pradeep},
  journal={Advances in Neural Information Processing Systems},
  volume={22},
  year={2009}
}

@article{nemirovski2009robust,
  title={Robust stochastic approximation approach to stochastic programming},
  author={Nemirovski, Arkadi and Juditsky, Anatoli and Lan, Guanghui and Shapiro, Alexander},
  journal={SIAM Journal on optimization},
  volume={19},
  number={4},
  pages={1574--1609},
  year={2009},
  publisher={SIAM}
}

@article{bertsimas2020predictive,
  title={From predictive to prescriptive analytics},
  author={Bertsimas, Dimitris and Kallus, Nathan},
  journal={Management Science},
  volume={66},
  number={3},
  pages={1025--1044},
  year={2020},
  publisher={INFORMS}
}

@article{sadana2024survey,
  title={A survey of contextual optimization methods for decision-making under uncertainty},
  author={Sadana, Utsav and Chenreddy, Abhilash and Delage, Erick and Forel, Alexandre and Frejinger, Emma and Vidal, Thibaut},
  journal={European Journal of Operational Research},
  year={2024},
  publisher={Elsevier}
}

@article{sun2020global,
  title={The global landscape of neural networks: An overview},
  author={Sun, Ruoyu and Li, Dawei and Liang, Shiyu and Ding, Tian and Srikant, Rayadurgam},
  journal={IEEE Signal Processing Magazine},
  volume={37},
  number={5},
  pages={95--108},
  year={2020},
  publisher={IEEE}
}

@inproceedings{zhang2019gradient,
  title={Why Gradient Clipping Accelerates Training: A Theoretical Justification for Adaptivity},
  author={Zhang, Jingzhao and He, Tianxing and Sra, Suvrit and Jadbabaie, Ali},
  booktitle={International Conference on Learning Representations},
  year={2019}
}

@article{zhang2020adaptive,
  title={Why are adaptive methods good for attention models?},
  author={Zhang, Jingzhao and Karimireddy, Sai Praneeth and Veit, Andreas and Kim, Seungyeon and Reddi, Sashank and Kumar, Sanjiv and Sra, Suvrit},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={15383--15393},
  year={2020}
}

@inproceedings{zhang2019citation,
  title={Why gradient clipping accelerates training: A theoretical justification for adaptivity},
  author={Zhang, Jingzhao and He, Tianxing and Sra, Suvrit and Jadbabaie, Ali},
  year={2019},
  url={https://scholar.google.com/citations?view_op=view_citation&hl=en&user=8NudxYsAAAAJ&citation_for_view=8NudxYsAAAAJ:Se3iqnhoufwC}
}

@inproceedings{zhang2020citation,
  title={Why are adaptive methods good for attention models?},
  author={Zhang, Jingzhao and Karimireddy, Sai Praneeth and Veit, Andreas and Kim, Seungyeon and Reddi, Sashank and Kumar, Sanjiv and Sra, Suvrit},
  year={2020},
  url={https://scholar.google.com/citations?view_op=view_citation&hl=en&user=8NudxYsAAAAJ&citation_for_view=8NudxYsAAAAJ:M3ejUd6NZC8C}
}

@article{drusvyatskiy2023stochastic,
  title={Stochastic optimization with decision-dependent distributions},
  author={Drusvyatskiy, Dmitriy and Xiao, Lin},
  journal={Mathematics of Operations Research},
  volume={48},
  number={2},
  pages={954--998},
  year={2023},
  publisher={INFORMS}
}

@inproceedings{perdomo2020performative,
  title={Performative prediction},
  author={Perdomo, Juan and Zrnic, Tijana and Mendler-D{\"u}nner, Celestine and Hardt, Moritz},
  booktitle={International Conference on Machine Learning},
  pages={7599--7609},
  year={2020},
  organization={PMLR}
}

@inproceedings{pedregosa2020acceleration,
  title={Acceleration through spectral density estimation},
  author={Pedregosa, Fabian and Scieur, Damien},
  booktitle={International Conference on Machine Learning},
  pages={7553--7562},
  year={2020},
  organization={PMLR}
}

@inproceedings{paquette2021sgd,
  title={Sgd in the large: Average-case analysis, asymptotics, and stepsize criticality},
  author={Paquette, Courtney and Lee, Kiwon and Pedregosa, Fabian and Paquette, Elliot},
  booktitle={Conference on Learning Theory},
  pages={3548--3626},
  year={2021},
  organization={PMLR}
}

@inproceedings{doikov2023second,
  title={Second-order optimization with lazy hessians},
  author={Doikov, Nikita and Jaggi, Martin and others},
  booktitle={International Conference on Machine Learning},
  pages={8138--8161},
  year={2023},
  organization={PMLR}
}

@article{konevcny2016federated,
  title={Federated optimization: Distributed machine learning for on-device intelligence},
  author={Kone{\v{c}}n{\`y}, Jakub and McMahan, H Brendan and Ramage, Daniel and Richt{\'a}rik, Peter},
  journal={arXiv preprint arXiv:1610.02527},
  year={2016}
}

@inproceedings{mcmahan2017communication,
  title={Communication-efficient learning of deep networks from decentralized data},
  author={McMahan, Brendan and Moore, Eider and Ramage, Daniel and Hampson, Seth and y Arcas, Blaise Aguera},
  booktitle={Artificial intelligence and statistics},
  pages={1273--1282},
  year={2017},
  organization={PMLR}
}

@article{kairouz2021advances,
  title={Advances and open problems in federated learning},
  author={Kairouz, Peter and McMahan, H Brendan and Avent, Brendan and Bellet, Aur{\'e}lien and Bennis, Mehdi and Bhagoji, Arjun Nitin and Bonawitz, Kallista and Charles, Zachary and Cormode, Graham and Cummings, Rachel and others},
  journal={Foundations and trends{\textregistered} in machine learning},
  volume={14},
  number={1--2},
  pages={1--210},
  year={2021},
  publisher={Now Publishers, Inc.}
}

@inproceedings{karimireddy2020scaffold,
  title={Scaffold: Stochastic controlled averaging for federated learning},
  author={Karimireddy, Sai Praneeth and Kale, Satyen and Mohri, Mehryar and Reddi, Sashank and Stich, Sebastian and Suresh, Ananda Theertha},
  booktitle={International conference on machine learning},
  pages={5132--5143},
  year={2020},
  organization={PMLR}
}

@inproceedings{stich2019local,
  title={Local SGD Converges Fast and Communicates Little},
  author={Stich, Sebastian Urban},
  booktitle={ICLR 2019-International Conference on Learning Representations},
  year={2019}
}

@inproceedings{mishchenko2022proxskip,
  title={Proxskip: Yes! local gradient steps provably lead to communication acceleration! finally!},
  author={Mishchenko, Konstantin and Malinovsky, Grigory and Stich, Sebastian and Richt{\'a}rik, Peter},
  booktitle={International Conference on Machine Learning},
  pages={15750--15769},
  year={2022},
  organization={PMLR}
}

@article{grimmer2024provably,
  title={Provably faster gradient descent via long steps},
  author={Grimmer, Benjamin},
  journal={SIAM Journal on Optimization},
  volume={34},
  number={3},
  pages={2588--2608},
  year={2024},
  publisher={SIAM}
}

@article{kornowski2024open,
  title={Open Problem: Anytime Convergence Rate of Gradient Descent},
  author={Kornowski, Guy and Shamir, Ohad},
  journal={arXiv preprint arXiv:2406.13888},
  year={2024}
}

@article{altschuler2023acceleration1,
  title={Acceleration by stepsize hedging i: Multi-step descent and the silver stepsize schedule},
  author={Altschuler, Jason M and Parrilo, Pablo A},
  journal={arXiv preprint arXiv:2309.07879},
  year={2023}
}

@article{altschuler2023acceleration2,
  title={Acceleration by stepsize hedging ii: Silver stepsize schedule for smooth convex optimization},
  author={Altschuler, Jason M and Parrilo, Pablo A},
  journal={arXiv preprint arXiv:2309.16530},
  year={2023}
}

@inproceedings{yue2023lower,
  title={On the lower bound of minimizing polyak-{\L}ojasiewicz functions},
  author={Yue, Pengyun and Fang, Cong and Lin, Zhouchen},
  booktitle={The Thirty Sixth Annual Conference on Learning Theory},
  pages={2948--2968},
  year={2023},
  organization={PMLR},
  url={https://proceedings.mlr.press/v195/yue23a.html}
}

@inproceedings{karimi2016linear,
  title={Linear convergence of gradient and proximal-gradient methods under the polyak-{\l}ojasiewicz condition},
  author={Karimi, Hamed and Nutini, Julie and Schmidt, Mark},
  booktitle={Machine Learning and Knowledge Discovery in Databases: European Conference, ECML PKDD 2016, Riva del Garda, Italy, September 19-23, 2016, Proceedings, Part I 16},
  pages={795--811},
  year={2016},
  organization={Springer}
}

@inproceedings{zhang2021complexity,
  title={The complexity of nonconvex-strongly-concave minimax optimization},
  author={Zhang, Siqi and Yang, Junchi and Guzm{\'a}n, Crist{\'o}bal and Kiyavash, Negar and He, Niao},
  booktitle={Uncertainty in Artificial Intelligence},
  pages={482--492},
  year={2021},
  organization={PMLR}
}

@article{yang2020catalyst,
  title={A catalyst framework for minimax optimization},
  author={Yang, Junchi and Zhang, Siqi and Kiyavash, Negar and He, Niao},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={5667--5678},
  year={2020}
}

@article{zhang2022sapd+,
  title={Sapd+: An accelerated stochastic method for nonconvex-concave minimax problems},
  author={Zhang, Xuan and Aybat, Necdet Serhat and Gurbuzbalaban, Mert},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={21668--21681},
  year={2022}
}

@article{li2021complexity,
  title={Complexity lower bounds for nonconvex-strongly-concave min-max optimization},
  author={Li, Haochuan and Tian, Yi and Zhang, Jingzhao and Jadbabaie, Ali},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={1792--1804},
  year={2021}
}

@article{yang2020global,
  title={Global convergence and variance reduction for a class of nonconvex-nonconcave minimax problems},
  author={Yang, Junchi and Kiyavash, Negar and He, Niao},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={1153--1165},
  year={2020}
}

@inproceedings{daskalakis2021complexity,
  title={The complexity of constrained min-max optimization},
  author={Daskalakis, Constantinos and Skoulakis, Stratis and Zampetakis, Manolis},
  booktitle={Proceedings of the 53rd Annual ACM SIGACT Symposium on Theory of Computing},
  pages={1466--1478},
  year={2021}
}

@article{nesterov2012make,
  title={How to make the gradients small},
  author={Nesterov, Yurii},
  journal={Optima. Mathematical Optimization Society Newsletter},
  number={88},
  pages={10--11},
  year={2012},
  url={https://www.mathopt.org/Optima-Issues/optima88.pdf}
}

@article{zhang2024anytime,
  title={Anytime Acceleration of Gradient Descent}, 
  author={Zihan Zhang and Jason D. Lee and Simon S. Du and Yuxin Chen},
  journal={arXiv preprint arXiv:2411.17668},
  year={2024},
  url={https://arxiv.org/abs/2411.17668}
}

@article{liu2020primer,
  title={A primer on zeroth-order optimization in signal processing and machine learning: Principals, recent advances, and applications},
  author={Liu, Sijia and Chen, Pin-Yu and Kailkhura, Bhavya and Zhang, Gaoyuan and Hero III, Alfred O and Varshney, Pramod K},
  journal={IEEE Signal Processing Magazine},
  volume={37},
  number={5},
  pages={43--54},
  year={2020},
  publisher={IEEE}
}

test test