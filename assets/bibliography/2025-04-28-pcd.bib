@article{dubey2024llama,
  title={The llama 3 herd of models},
  author={Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Yang, Amy and Fan, Angela and others},
  journal={arXiv preprint arXiv:2407.21783},
  year={2024}
}
@article{hsieh2024ruler,
  title={RULER: What's the Real Context Size of Your Long-Context Language Models?},
  author={Hsieh, Cheng-Ping and Sun, Simeng and Kriman, Samuel and Acharya, Shantanu and Rekesh, Dima and Jia, Fei and Zhang, Yang and Ginsburg, Boris},
  journal={arXiv preprint arXiv:2404.06654},
  year={2024}
}
@article{hengle2024multilingual,
  title={Multilingual needle in a haystack: Investigating long-context behavior of multilingual large language models},
  author={Hengle, Amey and Bajpai, Prasoon and Dan, Soham and Chakraborty, Tanmoy},
  journal={arXiv preprint arXiv:2408.10151},
  year={2024}
}
@inproceedings{zhang2024bench,
  title={∞ Bench: Extending long context evaluation beyond 100k tokens},
  author={Zhang, Xinrong and Chen, Yingfa and Hu, Shengding and Xu, Zihang and Chen, Junhao and Hao, Moo and Han, Xu and Thai, Zhen and Wang, Shuo and Liu, Zhiyuan and others},
  booktitle={Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={15262--15277},
  year={2024}
}
@article{liu2024lost,
  title={Lost in the middle: How language models use long contexts},
  author={Liu, Nelson F and Lin, Kevin and Hewitt, John and Paranjape, Ashwin and Bevilacqua, Michele and Petroni, Fabio and Liang, Percy},
  journal={Transactions of the Association for Computational Linguistics},
  volume={12},
  pages={157--173},
  year={2024},
  publisher={MIT Press One Broadway, 12th Floor, Cambridge, Massachusetts 02142, USA~…}
}
@article{lu2024insights,
  title={Insights into LLM Long-Context Failures: When Transformers Know but Don't Tell},
  author={Lu, Taiming and Gao, Muhan and Yu, Kuai and Byerly, Adam and Khashabi, Daniel},
  journal={arXiv preprint arXiv:2406.14673},
  year={2024}
}
@article{tworkowski2024focused,
  title={Focused transformer: Contrastive training for context scaling},
  author={Tworkowski, Szymon and Staniszewski, Konrad and Pacek, Miko{\l}aj and Wu, Yuhuai and Michalewski, Henryk and Mi{\l}o{\'s}, Piotr},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}
@article{dsouza2024evaluating,
  title={Evaluating Language Model Context Windows: A" Working Memory" Test and Inference-time Correction},
  author={Dsouza, Amanda and Glaze, Christopher and Shin, Changho and Sala, Frederic},
  journal={arXiv preprint arXiv:2407.03651},
  year={2024}
}
@article{peysakhovich2023attention,
  title={Attention sorting combats recency bias in long context language models},
  author={Peysakhovich, Alexander and Lerer, Adam},
  journal={arXiv preprint arXiv:2310.01427},
  year={2023}
}
@article{yu2023paraphrasing,
  title={" Paraphrasing The Original Text" Makes High Accuracy Long-Context QA},
  author={Yu, Yijiong},
  journal={arXiv preprint arXiv:2312.11193},
  year={2023}
}
@article{zhang2024found,
  title={Found in the middle: How language models use long contexts better via plug-and-play positional encoding},
  author={Zhang, Zhenyu and Chen, Runjin and Liu, Shiwei and Yao, Zhewei and Ruwase, Olatunji and Chen, Beidi and Wu, Xiaoxia and Wang, Zhangyang},
  journal={arXiv preprint arXiv:2403.04797},
  year={2024}
}
@article{hsieh2024found,
  title={Found in the middle: Calibrating positional attention bias improves long context utilization},
  author={Hsieh, Cheng-Yu and Chuang, Yung-Sung and Li, Chun-Liang and Wang, Zifeng and Le, Long T and Kumar, Abhishek and Glass, James and Ratner, Alexander and Lee, Chen-Yu and Krishna, Ranjay and others},
  journal={arXiv preprint arXiv:2406.16008},
  year={2024}
}
@article{dataartificial,
  title={FROM ARTIFICIAL NEEDLES TO REAL HAYSTACKS: IM-PROVING RETRIEVAL CAPABILITIES IN LLMS BY FINE-TUNING ON SYNTHETIC DATA},
  author={DATA, TUNING ON SYNTHETIC}
}
@article{an2024make,
  title={Make Your LLM Fully Utilize the Context},
  author={An, Shengnan and Ma, Zexiong and Lin, Zeqi and Zheng, Nanning and Lou, Jian-Guang},
  journal={arXiv preprint arXiv:2404.16811},
  year={2024}
}
@article{su2024roformer,
  title={Roformer: Enhanced transformer with rotary position embedding},
  author={Su, Jianlin and Ahmed, Murtadha and Lu, Yu and Pan, Shengfeng and Bo, Wen and Liu, Yunfeng},
  journal={Neurocomputing},
  volume={568},
  pages={127063},
  year={2024},
  publisher={Elsevier}
}
@article{press2021train,
  title={Train short, test long: Attention with linear biases enables input length extrapolation},
  author={Press, Ofir and Smith, Noah A and Lewis, Mike},
  journal={arXiv preprint arXiv:2108.12409},
  year={2021}
}
@article{chi2022kerple,
  title={Kerple: Kernelized relative positional embedding for length extrapolation},
  author={Chi, Ta-Chung and Fan, Ting-Han and Ramadge, Peter J and Rudnicky, Alexander},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={8386--8399},
  year={2022}
}
