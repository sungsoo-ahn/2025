@article{gregor2015draw,
  title={DRAW: A recurrent neural network for image generation},
  author={Gregor, Karol and Danihelka, Ivo and Graves, Alex and Rezende, Danilo Jimenez and Wierstra, Daan},
  journal={arXiv preprint, arXiv:1502.04623},
  year={2015},
  url={https://arxiv.org/pdf/1502.04623.pdf}
}

@INPROCEEDINGS{def_exp,
  author={Verma, Sahil and Rubin, Julia},
  booktitle={2018 IEEE/ACM International Workshop on Software Fairness (FairWare)}, 
  title={Fairness Definitions Explained}, 
  year={2018},
  volume={},
  number={},
  pages={1-7},
  keywords={Artificial intelligence;Software algorithms;Software;Employment;Conferences;Software engineering;History;definitions of fairness;survey;machine learning},
  doi={10.1145/3194770.3194776}}
  
  @article{alg_fair_choices,
   author = "Mitchell, Shira and Potash, Eric and Barocas, Solon and D&apos;Amour, Alexander and Lum, Kristian",
   title = "Algorithmic Fairness: Choices, Assumptions, and Definitions", 
   journal= "Annual Review of Statistics and Its Application",
   year = "2021",
   volume = "8",
   number = "Volume 8, 2021",
   pages = "141-163",
   doi = "https://doi.org/10.1146/annurev-statistics-042720-125902",
   url = "https://www.annualreviews.org/content/journals/10.1146/annurev-statistics-042720-125902",
   publisher = "Annual Reviews",
   issn = "2326-831X"}
   
   @inproceedings{JointOpt,
	author = {Zhang, Yunfeng and Bellamy, Rachel and Varshney, Kush},
	title = {Joint Optimization of AI Fairness and Utility: A Human-Centered Approach},
	year = {2020},
	isbn = {9781450371100},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3375627.3375862},
	doi = {10.1145/3375627.3375862},
	abstract = {Today, AI is increasingly being used in many high-stakes decision-making applications in which fairness is an important concern. Already, there are many examples of AI being biased and making questionable and unfair decisions. The AI research community has proposed many methods to measure and mitigate unwanted biases, but few of them involve inputs from human policy makers. We argue that because different fairness criteria sometimes cannot be simultaneously satisfied, and because achieving fairness often requires sacrificing other objectives such as model accuracy, it is key to acquire and adhere to human policy makers' preferences on how to make the tradeoff among these objectives. In this paper, we propose a framework and some exemplar methods for eliciting such preferences and for optimizing an AI model according to these preferences.},
	booktitle = {Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society},
	pages = {400â€“406},
	numpages = {7},
	keywords = {policy elicitation, multi-criteria decision making, algorithmic fairness},
	location = {New York, NY, USA},
	series = {AIES '20}
}

@misc{aif360,
    title = "{AI Fairness} 360:  An Extensible Toolkit for Detecting, Understanding, and Mitigating Unwanted Algorithmic Bias",
    author = {Rachel K. E. Bellamy and Kuntal Dey and Michael Hind and
	Samuel C. Hoffman and Stephanie Houde and Kalapriya Kannan and
	Pranay Lohia and Jacquelyn Martino and Sameep Mehta and
	Aleksandra Mojsilovic and Seema Nagar and Karthikeyan Natesan Ramamurthy and
	John Richards and Diptikalyan Saha and Prasanna Sattigeri and
	Moninder Singh and Kush R. Varshney and Yunfeng Zhang},
    month = oct,
    year = {2018},
    url = {https://arxiv.org/abs/1810.01943}
}

@article{Mittelstadt2019PrinciplesAC,
  title={Principles alone cannot guarantee ethical AI},
  author={Brent Daniel Mittelstadt},
  journal={Nature Machine Intelligence},
  year={2019},
  volume={1},
  pages={501 - 507},
  url={https://api.semanticscholar.org/CorpusID:207888555}
}

