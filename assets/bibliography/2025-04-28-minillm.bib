@article{Gou2021KDSurvey,
  title={Knowledge Distillation: A Survey},
  author={Gou, Jianping and Yu, Baosheng and Maybank, Stephen J. and Tao, Dacheng},
  journal={International Journal of Computer Vision},
  volume = 129,
  year={2021},
  pages = {1789--1819},
  url={https://doi.org/10.1007/s11263-021-01453-z}
}

@misc{Alpaca,
  author = {Rohan Taori and Ishaan Gulrajani and Tianyi Zhang and Yann Dubois and Xuechen Li and Carlos Guestrin and Percy Liang and Tatsunori B. Hashimoto },
  title = {Stanford Alpaca: An Instruction-following LLaMA model},
  year = {2023},
  publisher = {GitHub},
  journal = {GitHub repository},
  url = {https://github.com/tatsu-lab/stanford_alpaca},
}

@article{Sanh2020DistilBERT,
  title={DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter},
  author={Victor Sanh and Lysandre Debut and Julien Chaumond and Thomas Wolf},
  journal={ArXiv},
  year={2019},
  volume={abs/1910.01108},
  url={https://api.semanticscholar.org/CorpusID:203626972}
}

@inproceedings{Wang2020MiniLM,
author = {Wang, Wenhui and Wei, Furu and Dong, Li and Bao, Hangbo and Yang, Nan and Zhou, Ming},
title = {MINILM: deep self-attention distillation for task-agnostic compression of pre-trained transformers},
year = {2020},
isbn = {9781713829546},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
booktitle = {Proceedings of the 34th International Conference on Neural Information Processing Systems},
articleno = {485},
numpages = {13},
location = {Vancouver, BC, Canada},
series = {NIPS '20}
}

@inbook{Malinin2019ReverseKL,
author = {Malinin, Andrey and Gales, Mark},
title = {Reverse KL-divergence training of prior networks: improved uncertainty and adversarial robustness},
year = {2019},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Ensemble approaches for uncertainty estimation have recently been applied to the tasks of misclassification detection, out-of-distribution input detection and adversarial attack detection. Prior Networks have been proposed as an approach to efficiently emulate an ensemble of models for classification by parameterising a Dirichlet prior distribution over output distributions. These models have been shown to outperform alternative ensemble approaches, such as Monte-Carlo Dropout, on the task of out-of-distribution input detection. However, scaling Prior Networks to complex datasets with many classes is difficult using the training criteria originally proposed. This paper makes two contributions. First, we show that the appropriate training criterion for Prior Networks is the reverse KL-divergence between Dirichlet distributions. This addresses issues in the nature of the training data target distributions, enabling prior networks to be successfully trained on classification tasks with arbitrarily many classes, as well as improving out-of-distribution detection performance. Second, taking advantage of this new training criterion, this paper investigates using Prior Networks to detect adversarial attacks and proposes a generalized form of adversarial training. It is shown that the construction of successful adaptive whitebox attacks, which affect the prediction and evade detection, against Prior Networks trained on CIFAR-10 and CIFAR-100 using the proposed approach requires a greater amount of computational effort than against networks defended using standard adversarial training or MC-dropout.},
booktitle = {Proceedings of the 33rd International Conference on Neural Information Processing Systems},
articleno = {1303},
numpages = {12}
}

@Article{Williams1992PolicyGradient,
author={Williams, Ronald J.},
title={Simple statistical gradient-following algorithms for connectionist reinforcement learning},
journal={Machine Learning},
year={1992},
month={May},
day={01},
volume={8},
number={3},
pages={229-256},
issn={1573-0565},
doi={10.1007/BF00992696},
url={https://doi.org/10.1007/BF00992696}
}

@inproceedings{Skalse2024RewardHacking,
author = {Skalse, Joar and Howe, Nikolaus H. R. and Krasheninnikov, Dmitrii and Krueger, David},
title = {Defining and characterizing reward hacking},
year = {2024},
isbn = {9781713871088},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
booktitle = {Proceedings of the 36th International Conference on Neural Information Processing Systems},
articleno = {687},
numpages = {12},
location = {New Orleans, LA, USA},
series = {NIPS '22}
}

@article{Hinton2015Distilling,
  title={Distilling the Knowledge in a Neural Network},
  author={Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff},
  journal={arXiv preprint arXiv:1503.02531},
  year={2015},
  url={https://arxiv.org/abs/1503.02531}
}

@article{Huang2023Hallucination,
  title={A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions},
  author={Huang, Lei and Yu, Weijiang and Ma, Weitao and Zhong, Weihong and Feng, Zhangyin and Wang, Haotian and Chen, Qianglong and Peng, Weihua and Feng, Xiaocheng and Qin, Bing and Liu, Ting},
  journal={arXiv preprint arXiv:2311.05232},
  year={2023}
}


@article{Wayne2023Survey,
  title = {A Survey of Large Language Models},
  author = {Zhao, Wayne Xin and Zhou, Kun and Li, Junyi and Tang, Tianyi and Wang, Xiaolei and Hou, Yupeng and Min, Yingqian and Zhang, Beichen and Zhang, Junjie and Dong, Zican and Du, Yifan and Yang, Chen and Chen, Yushuo and Chen, Zhipeng and Jiang, Jinhao and Ren, Ruiyang and Li, Yifan and Tang, Xinyu and Liu, Zikang and Liu, Peiyu and Nie, Jian-Yun and Wen, Ji-Rong},
  journal = {arXiv preprint arXiv:2303.18223},
  year = {2023},
  url = {https://arxiv.org/abs/2303.18223}
}



@article{Gu2023MiniLLM,
  title = {MiniLLM: Knowledge Distillation of Large Language Models},
  author = {Gu, Yuxian and Dong, Li and Wei, Furu and Huang, Minlie},
  journal = {arXiv preprint arXiv:2306.08543},
  year = {2023},
  url = {https://arxiv.org/pdf/2306.08543}
}



@article{Radford2019GPT2,
  title = {Language models are unsupervised multitask learners},
  author = { Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever,Ilya
},
  journal = {OpenAI Technical report},
  year = {2019},
  url = {https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf}
}


@article{Zhang2022OPT,
  title = {OPT: Open pre-trained transformer language models},
  author = {Zhang, Susan and Roller, Stephen and Goyal, Naman and Artetxe, Mikel and Chen, Moya and Chen, Shuohui and Dewan, Christopher and Diab, Mona and Li, Xian and Lin, Xi Victoria and et al.},
  journal = {arXiv preprint arXiv:2205.01068},
  year = {2022},
  url = {https://arxiv.org/abs/2205.01068}
}

@article{Touvron2023LLaMA,
  title = {LLaMA: Open and efficient foundation language models},
  author = {Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timothée and Rozière, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and Rodriguez, Aurelien and Joulin, Armand and Grave, Edouard and Lample, Guillaume},
  journal = {arXiv preprint arXiv:2302.13971},
  year = {2023},
  url = {https://arxiv.org/abs/2302.13971}
}


@inproceedings{Lin2004ROUGE,
  title = {ROUGE: A Package for Automatic Evaluation of Summaries},
  author = {Lin, Chin-Yew},
  booktitle = {Proceedings of the Workshop on Text Summarization Branches Out},
  year = {2004},
  publisher = {Association for Computational Linguistics},
  url = {https://aclanthology.org/W04-1013.pdf}
}


@article{Schulman2017Proximal,
  title = {Proximal Policy Optimization Algorithms},
  author = {Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
  journal = {arXiv preprint arXiv:1707.06347},
  year = {2017},
  url = {https://arxiv.org/pdf/1707.06347}
}


@article{OpenAI2023GPT4,
  title = {GPT-4 Technical Report},
  author = {OpenAI},
  year = {2023},
  eprint = {arXiv:2303.08774},
  url = {https://arxiv.org/abs/2303.08774}
}


@article{Mirzadeh2019Improved,
  author = {Seyed-Iman Mirzadeh and Mehrdad Farajtabar and Ang Li and Nir Levine and Akihiro Matsukawa and Hassan Ghasemzadeh},
  title = {Improved Knowledge Distillation via Teacher Assistant},
  year = {2019},
  eprint = {arXiv:1902.03393},
  url = {https://arxiv.org/abs/1902.03393}
}


@misc{DatabricksDolly,
  title = {Dolly: Instruction-Following Large Language Model},
  author = {Databricks},
  url = {https://github.com/databrickslabs/dolly/tree/master},
  note = {Databricks Dolly is an instruction-following large language model trained on the Databricks machine learning platform. The model family includes dolly-v2-12b, available on Hugging Face as databricks/dolly-v2-12b.},
  year = {2023}
}

@inproceedings{Wang2023SelfInstruct,
  author = {Yizhong Wang and Yeganeh Kordi and Swaroop Mishra and Alisa Liu and Noah A. Smith and Daniel Khashabi and Hannaneh Hajishirzi},
  title = {Self-Instruct: Aligning Language Models with Self-Generated Instructions},
  booktitle = {Proceedings of the ACL},
  year = {2023},
  url = {https://arxiv.org/abs/2212.10560}
}


@misc{Chiang2023Vicuna,
  author = {Wei-Lin Chiang and Zhuohan Li and Zi Lin and Ying Sheng and Zhanghao Wu and Hao Zhang and Lianmin Zheng and Siyuan Zhuang and Yonghao Zhuang and Joseph E. Gonzalez and Ion Stoica and Eric P. Xing},
  title = {Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90% ChatGPT Quality},
  year = {2023},
  month = {March},
  url = {https://lmsys.org/blog/2023-03-30-vicuna/}
}


@inproceedings{Wang2022Benchmarking,
  author = {Yizhong Wang and Swaroop Mishra and Pegah Alipoormolabashi and Yeganeh Kordi and Amirreza Mirzaei and Anjana Arunkumar and Arjun Ashok and Arut Selvan Dhanasekaran and Atharva Naik and David Stap and others},
  title = {Benchmarking Generalization via In-Context Instructions on 1,600+ Language Tasks},
  booktitle = {Proceedings of EMNLP},
  year = {2022},
  url = {https://arxiv.org/abs/2204.07705}
}

@inproceedings{Honovich2023Unnatural,
  author = {Or Honovich and Thomas Scialom and Omer Levy and Timo Schick},
  title = {Unnatural Instructions: Tuning Language Models with (Almost) No Human Labor},
  booktitle = {Proceedings of ACL},
  year = {2023},
  url = {https://arxiv.org/abs/2212.09689}
}


@article{Liu2019RoBERTa,
  author = {Yinhan Liu and Myle Ott and Naman Goyal and Jingfei Du and Mandar Joshi and Danqi Chen and Omer Levy and Mike Lewis and Luke Zettlemoyer and Veselin Stoyanov},
  title = {RoBERTa: A robustly optimized BERT pretraining approach},
  journal = {arXiv preprint arXiv:1907.11692},
  year = {2019},
  url = {https://arxiv.org/abs/1907.11692}
}


@misc{Gokaslan2019Openwebtext,
  author = {Aaron Gokaslan and Vanya Cohen and Ellie Pavlick and Stefanie Tellex},
  title = {Openwebtext Corpus},
  year = {2019},
  url = {https://huggingface.co/datasets/Skylion007/openwebtext}
}


@inproceedings{Furlanello2018BornAgain,
  author = {Tommaso Furlanello and Zachary Lipton and Michael Tschannen and Laurent Itti and Anima Anandkumar},
  title = {Born Again Neural Networks},
  booktitle = {Proceedings of the International Conference on Machine Learning (ICML)},
  year = {2018},
  url = {https://proceedings.mlr.press/v80/furlanello18a/furlanello18a.pdf}
}


@article{Lee2022SelfKnowledge,
  author = {Hyoje Lee and Yeachan Park and Hyun Seo and Myungjoo Kang},
  title = {Self-Knowledge Distillation via Dropout},
  journal = {arXiv preprint arXiv:2208.05642},
  year = {2022},
  url = {https://arxiv.org/abs/2208.05642}
}


@inproceedings{2023OpenReview,
  title = {MiniLLM: Knowledge Distillation of Large Language Models},
  author = {Gu, Yuxian and Dong, Li and Wei, Furu and Huang, Minlie},
  booktitle = {OpenReview},
  year = {2023},
  url = {https://openreview.net/forum?id=5h0qf7IBZZ}
}

@misc{OpenAI2024GPT4oMini,
  title = {GPT-4o Mini: Advancing Cost-Efficient Intelligence},
  author = {OpenAI},
  year = {2024},
  url = {https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/},
}

@article{Hu2023TeacherStudent,
  title={Teacher-Student Architecture for Knowledge Distillation: A Survey},
  author={Hu, Chengming and Li, Xuan and Liu, Dan and Wu, Haolun and Chen, Xi and Wang, Ju and Liu, Xue},
  journal={arXiv preprint arXiv:2308.04268},
  year={2023},
  url={https://arxiv.org/abs/2308.04268}
}