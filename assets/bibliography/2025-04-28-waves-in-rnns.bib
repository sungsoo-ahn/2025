
%%% Neurips2023 - submission


@InProceedings{pmlr-v202-karuvally23a,
  title = 	 {General Sequential Episodic Memory Model},
  author =       {Karuvally, Arjun and Sejnowski, Terrence and Siegelmann, Hava T},
  booktitle = 	 {Proceedings of the 40th International Conference on Machine Learning},
  pages = 	 {15900--15910},
  year = 	 {2023},
  editor = 	 {Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan},
  volume = 	 {202},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {23--29 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v202/karuvally23a/karuvally23a.pdf},
  url = 	 {https://proceedings.mlr.press/v202/karuvally23a.html},
  abstract = 	 {The state-of-the-art memory model is the General Associative Memory Model, a generalization of the classical Hopfield network. Like its ancestor, the general associative memory has a well-defined state-dependant energy surface, and its memories correlate with its fixed points. This is unlike human memories, which are commonly sequential rather than separated fixed points. In this paper, we introduce a class of General Sequential Episodic Memory Models (GSEMM) that, in the adiabatic limit, exhibit a dynamic energy surface, leading to a series of meta-stable states capable of encoding memory sequences. A multiple-timescale architecture enables the dynamic nature of the energy surface with newly introduced asymmetric synapses and signal propagation delays. We demonstrate its dense capacity under polynomial activation functions. GSEMM combines separate memories, short and long sequential episodic memories, under a unified theoretical framework, demonstrating how energy-based memory modeling can provide richer, human-like episodes.}
}


@article{Rumelhart1986LearningRB,
  title={Learning representations by back-propagating errors},
  author={David E. Rumelhart and Geoffrey E. Hinton and Ronald J. Williams},
  journal={Nature},
  year={1986},
  volume={323},
  pages={533-536}
}

@article{Sussillo2009GeneratingCP,
  title={Generating Coherent Patterns of Activity from Chaotic Neural Networks},
  author={David Sussillo and L. F. Abbott},
  journal={Neuron},
  year={2009},
  volume={63},
  pages={544-557}
}

 @book{Moir_2022, address={Cham}, title={Rudiments of Signal Processing and Systems}, rights={https://www.springer.com/tdm}, ISBN={9783030769468}, url={https://link.springer.com/10.1007/978-3-030-76947-5}, DOI={10.1007/978-3-030-76947-5}, publisher={Springer International Publishing}, author={Moir, Tom J.}, year={2022}, language={en} }

 @book{Jacquot_2019, edition={2}, title={Modern Digital Control Systems}, ISBN={9780203746721}, url={https://www.taylorfrancis.com/books/9781351430586}, DOI={10.1201/9780203746721}, publisher={Routledge}, author={Jacquot, Raymond G.}, year={2019}, month=jan, language={en} }

@inproceedings{Hespanha2009LinearST,
  title={Linear Systems Theory},
  author={Jo{\~a}o Pedro Hespanha},
  year={2009},
  url={https://api.semanticscholar.org/CorpusID:45378226}
}

@misc{keles2022computationalcomplexityselfattention,
      title={On The Computational Complexity of Self-Attention},
      author={Feyza Duman Keles and Pruthuvi Mahesakya Wijewardena and Chinmay Hegde},
      year={2022},
      eprint={2209.04881},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2209.04881},
}

@article{Dosovitskiy2020AnII,
  title={An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
  author={Alexey Dosovitskiy and Lucas Beyer and Alexander Kolesnikov and Dirk Weissenborn and Xiaohua Zhai and Thomas Unterthiner and Mostafa Dehghani and Matthias Minderer and Georg Heigold and Sylvain Gelly and Jakob Uszkoreit and Neil Houlsby},
  journal={ArXiv},
  year={2020},
  volume={abs/2010.11929},
  url={https://api.semanticscholar.org/CorpusID:225039882}
}

@inproceedings{Radford2018ImprovingLU,
  title={Improving Language Understanding by Generative Pre-Training},
  author={Alec Radford and Karthik Narasimhan},
  year={2018},
  url={https://api.semanticscholar.org/CorpusID:49313245}
}

@inproceedings{Martens2011LearningRN,
  title={Learning Recurrent Neural Networks with Hessian-Free Optimization},
  author={James Martens and Ilya Sutskever},
  booktitle={International Conference on Machine Learning},
  year={2011}
}

@article{Sussillo2013OpeningTB,
  title={Opening the Black Box: Low-Dimensional Dynamics in High-Dimensional Recurrent Neural Networks},
  author={David Sussillo and Omri Barak},
  journal={Neural Computation},
  year={2013},
  volume={25},
  pages={626-649}
}

@article{Strogatz1994NonlinearDA,
  title={Nonlinear Dynamics and Chaos: With Applications to Physics, Biology, Chemistry and Engineering},
  author={Steven H. Strogatz},
  journal={Physics Today},
  year={1994},
  volume={48},
  pages={93-94}
}

@article{JohnsonLaird2010MentalMA,
  title={Mental models and human reasoning},
  author={Philip N. Johnson-Laird},
  journal={Proceedings of the National Academy of Sciences},
  year={2010},
  volume={107},
  pages={18243 - 18250}
}

@inproceedings{WhiteheadSymbolismIM,
  title={Symbolism: its meaning and effect},
  author={Alfred North Whitehead}
}

@article{Newell1956TheLT,
  title={The logic theory machine-A complex information processing system},
  author={Allen Newell and Herbert A. Simon},
  journal={IRE Trans. Inf. Theory},
  year={1956},
  volume={2},
  pages={61-79}
}

@inproceedings{Newell1995GPSAP,
  title={GPS, a program that simulates human thought},
  author={Allen Newell and Herbert A. Simon},
  year={1995}
}

@article{Shortliffe1990AMO,
  title={A model of inexact reasoning in medicine},
  author={Edward H. Shortliffe and Bruce G. Buchanan},
  journal={Bellman Prize in Mathematical Biosciences},
  year={1990},
  volume={23},
  pages={259-275}
}

@inproceedings{Buchanan1984RuleBE,
  title={Rule Based Expert Systems: The Mycin Experiments of the Stanford Heuristic Programming Project (The Addison-Wesley series in artificial intelligence)},
  author={Bruce G. Buchanan and Edward H. Shortliffe},
  year={1984}
}

@inproceedings{Lindsay1980ApplicationsOA,
  title={Applications of Artificial Intelligence for Organic Chemistry: The DENDRAL Project},
  author={Robert K. Lindsay and Bruce G. Buchanan and Edward A. Feigenbaum and Joshua Lederberg},
  year={1980}
}

@article{Lindsay1993DENDRALAC,
  title={DENDRAL: A Case Study of the First Expert System for Scientific Hypothesis Formation},
  author={Robert K. Lindsay and Bruce G. Buchanan and Edward A. Feigenbaum and Joshua Lederberg},
  journal={Artif. Intell.},
  year={1993},
  volume={61},
  pages={209-261}
}

@article{Kautz2022TheTA,
  title={The Third AI Summer: AAAI Robert S. Engelmore Memorial Lecture},
  author={Henry A. Kautz},
  journal={AI Mag.},
  year={2022},
  volume={43},
  pages={93-104}
}

@article{Davis2015CommonsenseRA,
  title={Commonsense reasoning and commonsense knowledge in artificial intelligence},
  author={Ernest Davis and Gary F. Marcus},
  journal={Communications of the ACM},
  year={2015},
  volume={58},
  pages={92 - 103}
}

@article{Rosenblatt1958ThePA,
  title={The perceptron: a probabilistic model for information storage and organization in the brain.},
  author={Frank Rosenblatt},
  journal={Psychological review},
  year={1958},
  volume={65 6},
  pages={
          386-408
        }
}


@InProceedings{pmlr-v202-orvieto23a,
  title = 	 {Resurrecting Recurrent Neural Networks for Long Sequences},
  author =       {Orvieto, Antonio and Smith, Samuel L and Gu, Albert and Fernando, Anushan and Gulcehre, Caglar and Pascanu, Razvan and De, Soham},
  booktitle = 	 {Proceedings of the 40th International Conference on Machine Learning},
  pages = 	 {26670--26698},
  year = 	 {2023},
  editor = 	 {Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan},
  volume = 	 {202},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {23--29 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v202/orvieto23a/orvieto23a.pdf},
  url = 	 {https://proceedings.mlr.press/v202/orvieto23a.html},
  abstract = 	 {Recurrent Neural Networks (RNNs) offer fast inference on long sequences but are hard to optimize and slow to train. Deep state-space models (SSMs) have recently been shown to perform remarkably well on long sequence modeling tasks, and have the added benefits of fast parallelizable training and RNN-like fast inference. However, while SSMs are superficially similar to RNNs, there are important differences that make it unclear where their performance boost over RNNs comes from. We show that careful design of deep RNNs using standard signal propagation arguments can recover the impressive performance of deep SSMs on long-range reasoning tasks, while matching their training speed. To achieve this, we analyze and ablate a series of changes to standard RNNs including linearizing and diagonalizing the recurrence, using better parameterizations and initializations, and ensuring careful normalization of the forward pass. Our results provide new insights on the origins of the impressive performance of deep SSMs, and introduce an RNN block called the Linear Recurrent Unit (or LRU) that matches both their performance on the Long Range Arena benchmark and their computational efficiency.}
}


@inproceedings{Russell1995ArtificialIA,
  title={Artificial Intelligence: A Modern Approach},
  author={Stuart J. Russell and Peter Norvig},
  year={1995}
}

@article{Graves2014NeuralTM,
  title={Neural Turing Machines},
  author={Alex Graves and Greg Wayne and Ivo Danihelka},
  journal={ArXiv},
  year={2014},
  volume={abs/1410.5401}
}

@article{Turing2021OnCN,
  title={On computable numbers, with an application to the Entscheidungsproblem},
  author={A. Turing},
  journal={Proc. London Math. Soc.},
  year={2021},
  volume={s2-42},
  pages={230-265}
}

@article{Graves2016HybridCU,
  title={Hybrid computing using a neural network with dynamic external memory},
  author={Alex Graves and Greg Wayne and Malcolm Reynolds and Tim Harley and Ivo Danihelka and Agnieszka Grabska-Barwinska and Sergio Gomez Colmenarejo and Edward Grefenstette and Tiago Ramalho and John P. Agapiou and Adri{\`a} Puigdom{\`e}nech Badia and Karl Moritz Hermann and Yori Zwols and Georg Ostrovski and Adam Cain and Helen King and Christopher Summerfield and Phil Blunsom and Koray Kavukcuoglu and Demis Hassabis},
  journal={Nature},
  year={2016},
  volume={538},
  pages={471-476}
}

@inproceedings{Vaswani2017AttentionIA,
  title={Attention is All you Need},
  author={Ashish Vaswani and Noam M. Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
  booktitle={NIPS},
  year={2017}
}

@article{Liang2016NeuralSM,
  title={Neural Symbolic Machines: Learning Semantic Parsers on Freebase with Weak Supervision},
  author={Chen Liang and Jonathan Berant and Quoc V. Le and Kenneth D. Forbus and N. Lao},
  journal={ArXiv},
  year={2016},
  volume={abs/1612.01197}
}

@article{Greff2020OnTB,
  title={On the Binding Problem in Artificial Neural Networks},
  author={Klaus Greff and Sjoerd van Steenkiste and J{\"u}rgen Schmidhuber},
  journal={ArXiv},
  year={2020},
  volume={abs/2012.05208}
}

@article{Vankov2019TrainingNN,
  title={Training neural networks to encode symbols enables combinatorial generalization},
  author={Ivan I. Vankov and Jeffrey S. Bowers},
  journal={Philosophical Transactions of the Royal Society B},
  year={2019},
  volume={375}
}

@inproceedings{Panigrahi2021LearningAG,
  title={Learning and Generalization in RNNs},
  author={Abhishek Panigrahi and Navin Goyal},
  booktitle={Neural Information Processing Systems},
  year={2021}
}

@article{Dasgupta2021MemoryAA,
  title={Memory as a Computational Resource},
  author={Ishita Dasgupta and Samuel J. Gershman},
  journal={Trends in Cognitive Sciences},
  year={2021},
  volume={25},
  pages={240-251}
}

@article{Hopfield1982NeuralNA,
  title={Neural networks and physical systems with emergent collective computational abilities.},
  author={John J. Hopfield},
  journal={Proceedings of the National Academy of Sciences of the United States of America},
  year={1982},
  volume={79 8},
  pages={
          2554-8
        }
}

@article{Dennis2015TheEO,
  title={The effects of item familiarity on the neural correlates of successful associative memory encoding},
  author={Nancy A. Dennis and Indira C. Turney and Christina E. Webb and Amy A. Overman},
  journal={Cognitive, Affective, \& Behavioral Neuroscience},
  year={2015},
  volume={15},
  pages={889-900}
}

@article{Ramsauer2020HopfieldNI,
  title={Hopfield Networks is All You Need},
  author={Hubert Ramsauer and Bernhard Schafl and Johannes Lehner and Philipp Seidl and Michael Widrich and Lukas Gruber and Markus Holzleitner and Milena Pavlovi'c and Geir Kjetil Sandve and Victor Greiff and David P. Kreil and Michael Kopp and G{\"u}nter Klambauer and Johannes Brandstetter and Sepp Hochreiter},
  journal={ArXiv},
  year={2020},
  volume={abs/2008.02217}
}

@article{Krotov2016DenseAM,
  title={Dense Associative Memory for Pattern Recognition},
  author={Dmitry Krotov and John J. Hopfield},
  journal={ArXiv},
  year={2016},
  volume={abs/1606.01164}
}

@article{Krotov2020LargeAM,
  title={Large Associative Memory Problem in Neurobiology and Machine Learning},
  author={Dmitry Krotov and John J. Hopfield},
  journal={ArXiv},
  year={2020},
  volume={abs/2008.06996}
}

@article{Hopfield1984NeuronsWG,
  title={Neurons with graded response have collective computational properties like those of two-state neurons.},
  author={John J. Hopfield},
  journal={Proceedings of the National Academy of Sciences of the United States of America},
  year={1984},
  volume={81 10},
  pages={
          3088-92
        }
}

@article{Koiran1994DynamicsOD,
  title={Dynamics of Discrete Time, Continuous State Hopfield Networks},
  author={Pascal Koiran},
  journal={Neural Computation},
  year={1994},
  volume={6},
  pages={459-468}
}

@article{Kleinfeld1986SequentialSG,
  title={Sequential state generation by model neural networks.},
  author={David Kleinfeld},
  journal={Proceedings of the National Academy of Sciences of the United States of America},
  year={1986},
  volume={83 24},
  pages={
          9469-73
        }
}

@article{Kleinfeld1988AssociativeNN,
  title={Associative neural network model for the generation of temporal patterns. Theory and application to central pattern generators.},
  author={David Kleinfeld and Haim Sompolinsky},
  journal={Biophysical journal},
  year={1988},
  volume={54 6},
  pages={
          1039-51
        }
}

@ARTICLE{Personnaz1986,
  title    = "Collective computational properties of neural networks: New
              learning mechanisms",
  author   = "Personnaz, L and Guyon, I I and Dreyfus, G",
  journal  = "Phys Rev A Gen Phys",
  volume   =  34,
  number   =  5,
  pages    = "4217--4228",
  month    =  nov,
  year     =  1986,
  address  = "United States",
  language = "en"
}

@article{olah2020zoom,
  author = {Olah, Chris and Cammarata, Nick and Schubert, Ludwig and Goh, Gabriel and Petrov, Michael and Carter, Shan},
  title = {Zoom In: An Introduction to Circuits},
  journal = {Distill},
  year = {2020},
  note = {https://distill.pub/2020/circuits/zoom-in},
  doi = {10.23915/distill.00024.001}
}

@InProceedings{pmlr-v44-li15convergent,
  title = 	 {Convergent Learning: Do different neural networks learn the same representations?},
  author = 	 {Li, Yixuan and Yosinski, Jason and Clune, Jeff and Lipson, Hod and Hopcroft, John},
  booktitle = 	 {Proceedings of the 1st International Workshop on Feature Extraction: Modern Questions and Challenges at NIPS 2015},
  pages = 	 {196--212},
  year = 	 {2015},
  editor = 	 {Storcheus, Dmitry and Rostamizadeh, Afshin and Kumar, Sanjiv},
  volume = 	 {44},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Montreal, Canada},
  month = 	 {11 Dec},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v44/li15convergent.pdf},
  url = 	 {https://proceedings.mlr.press/v44/li15convergent.html},
  abstract = 	 {Recent successes in training large, deep neural networks (DNNs) have prompted active investigation into the underlying representations learned on their intermediate layers. Such research is difficult because it requires making sense of non-linear computations performed by millions of learned parameters. However, despite the difficulty, such research is valuable because it increases our ability to understand current models and training algorithms and thus create improved versions of them. We argue for the value of investigating whether neural networks exhibit what we call convergent learning, which is when separately trained DNNs learn features that converge to span similar spaces. We further begin research into this question by introducing two techniques to approximately align neurons from two networks: a bipartite matching approach that makes one-to-one assignments between neurons and a spectral clustering approach that finds many-to-many mappings. Our initial approach to answering this question reveals many interesting, previously unknown properties of neural networks, and we argue that future research into the question of convergent learning will yield many more. The insights described here include (1) that some features are learned reliably in multiple networks, yet other features are not consistently learned; and (2) that units learn to span low-dimensional subspaces and, while these subspaces are common to multiple networks, the specific basis vectors learned are not; (3) that the  average activation values of neurons vary considerably within a network, yet the mean activation values across different networks converge to an almost identical distribution.}
}

@article{cammarata2020thread:,
  author = {Cammarata, Nick and Carter, Shan and Goh, Gabriel and Olah, Chris and Petrov, Michael and Schubert, Ludwig and Voss, Chelsea and Egan, Ben and Lim, Swee Kiat},
  title = {Thread: Circuits},
  journal = {Distill},
  year = {2020},
  note = {https://distill.pub/2020/circuits},
  doi = {10.23915/distill.00024}
}

@article{elhage2021mathematical,
   title={A Mathematical Framework for Transformer Circuits},
   author={Elhage, Nelson and Nanda, Neel and Olsson, Catherine and Henighan, Tom and Joseph, Nicholas and Mann, Ben and Askell, Amanda and Bai, Yuntao and Chen, Anna and Conerly, Tom and DasSarma, Nova and Drain, Dawn and Ganguli, Deep and Hatfield-Dodds, Zac and Hernandez, Danny and Jones, Andy and Kernion, Jackson and Lovitt, Liane and Ndousse, Kamal and Amodei, Dario and Brown, Tom and Clark, Jack and Kaplan, Jared and McCandlish, Sam and Olah, Chris},
   year={2021},
   journal={Transformer Circuits Thread},
   note={https://transformer-circuits.pub/2021/framework/index.html}
}

@article{olsson2022context,
   title={In-context Learning and Induction Heads},
   author={Olsson, Catherine and Elhage, Nelson and Nanda, Neel and Joseph, Nicholas and DasSarma, Nova and Henighan, Tom and Mann, Ben and Askell, Amanda and Bai, Yuntao and Chen, Anna and Conerly, Tom and Drain, Dawn and Ganguli, Deep and Hatfield-Dodds, Zac and Hernandez, Danny and Johnston, Scott and Jones, Andy and Kernion, Jackson and Lovitt, Liane and Ndousse, Kamal and Amodei, Dario and Brown, Tom and Clark, Jack and Kaplan, Jared and McCandlish, Sam and Olah, Chris},
   year={2022},
   journal={Transformer Circuits Thread},
   note={https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html}
}



@InProceedings{pmlr-v202-chughtai23a,
  title = 	 {A Toy Model of Universality: Reverse Engineering how Networks Learn Group Operations},
  author =       {Chughtai, Bilal and Chan, Lawrence and Nanda, Neel},
  booktitle = 	 {Proceedings of the 40th International Conference on Machine Learning},
  pages = 	 {6243--6267},
  year = 	 {2023},
  editor = 	 {Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan},
  volume = 	 {202},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {23--29 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v202/chughtai23a/chughtai23a.pdf},
  url = 	 {https://proceedings.mlr.press/v202/chughtai23a.html},
  abstract = 	 {Universality is a key hypothesis in mechanistic interpretability – that different models learn similar features and circuits when trained on similar tasks. In this work, we study the universality hypothesis by examining how small networks learn to implement group compositions. We present a novel algorithm by which neural networks may implement composition for any finite group via mathematical representation theory. We then show that these networks consistently learn this algorithm by reverse engineering model logits and weights, and confirm our understanding using ablations. By studying networks trained on various groups and architectures, we find mixed evidence for universality: using our algorithm, we can completely characterize the family of circuits and features that networks learn on this task, but for a given network the precise circuits learned – as well as the order they develop – are arbitrary.}
}

@article{Elhage2022ToyMO,
  title={Toy Models of Superposition},
  author={Nelson Elhage and Tristan Hume and Catherine Olsson and Nicholas Schiefer and T. J. Henighan and Shauna Kravec and Zac Hatfield-Dodds and Robert Lasenby and Dawn Drain and Carol Chen and Roger Baker Grosse and Sam McCandlish and Jared Kaplan and Dario Amodei and Martin Wattenberg and Christopher Olah},
  journal={ArXiv},
  year={2022},
  volume={abs/2209.10652},
  url={https://api.semanticscholar.org/CorpusID:252439050}
}

@article{Gu2021EfficientlyML,
  title={Efficiently Modeling Long Sequences with Structured State Spaces},
  author={Albert Gu and Karan Goel and Christopher R'e},
  journal={ArXiv},
  year={2021},
  volume={abs/2111.00396},
  url={https://api.semanticscholar.org/CorpusID:240354066}
}


@InProceedings{pmlr-v235-karuvally24a,
  title = 	 {Hidden Traveling Waves bind Working Memory Variables in Recurrent Neural Networks},
  author =       {Karuvally, Arjun and Sejnowski, Terrence and Siegelmann, Hava T},
  booktitle = 	 {Proceedings of the 41st International Conference on Machine Learning},
  pages = 	 {23266--23290},
  year = 	 {2024},
  editor = 	 {Salakhutdinov, Ruslan and Kolter, Zico and Heller, Katherine and Weller, Adrian and Oliver, Nuria and Scarlett, Jonathan and Berkenkamp, Felix},
  volume = 	 {235},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {21--27 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://raw.githubusercontent.com/mlresearch/v235/main/assets/karuvally24a/karuvally24a.pdf},
  url = 	 {https://proceedings.mlr.press/v235/karuvally24a.html},
  abstract = 	 {Traveling waves are a fundamental phenomenon in the brain, playing a crucial role in short-term information storage. In this study, we leverage the concept of traveling wave dynamics within a neural lattice to formulate a theoretical model of neural working memory in Recurrent Neural Networks (RNNs), study its properties, and its real world implications in AI. The proposed model diverges from traditional approaches, which assume information storage in static, register-like locations updated by interference. Instead, the model stores data as waves that is updated by the wave’s boundary conditions. We rigorously examine the model’s capabilities in representing and learning state histories, which are vital for learning history-dependent dynamical systems. The findings reveal that the model reliably stores external information and enhances the learning process by addressing the diminishing gradient problem of RNNs. To understand the model’s real-world applicability, we explore two cases: linear boundary condition and non-linear, self-attention-driven boundary condition. The experiments reveal that the linear scenario is effectively <em>learned</em> by RNNs through backpropagation when modeling history-dependent dynamical systems. Conversely, the non-linear scenario parallels an attention-only transformer. Collectively, our findings suggest the broader relevance of traveling waves in AI and its potential in advancing neural network architectures.}
}


@article{Arora2016LinearAS,
  title={Linear Algebraic Structure of Word Senses, with Applications to Polysemy},
  author={Sanjeev Arora and Yuanzhi Li and Yingyu Liang and Tengyu Ma and Andrej Risteski},
  journal={Transactions of the Association for Computational Linguistics},
  year={2016},
  volume={6},
  pages={483-495},
  url={https://api.semanticscholar.org/CorpusID:9285053}
}

@inproceedings{Radford2019LanguageMA,
  title={Language Models are Unsupervised Multitask Learners},
  author={Alec Radford and Jeff Wu and Rewon Child and David Luan and Dario Amodei and Ilya Sutskever},
  year={2019},
  url={https://api.semanticscholar.org/CorpusID:160025533}
}

@inproceedings{Zhao2021CalibrateBU,
  title={Calibrate Before Use: Improving Few-Shot Performance of Language Models},
  author={Tony Zhao and Eric Wallace and Shi Feng and Dan Klein and Sameer Singh},
  booktitle={International Conference on Machine Learning},
  year={2021},
  url={https://api.semanticscholar.org/CorpusID:231979430}
}

@article{Chaudhry2023LongSH,
  title={Long Sequence Hopfield Memory},
  author={Hamza Tahir Chaudhry and Jacob A. Zavatone-Veth and Dmitry Krotov and Cengiz Pehlevan},
  journal={ArXiv},
  year={2023},
  volume={abs/2306.04532},
  url={https://api.semanticscholar.org/CorpusID:259095470}
}

@article{Cheung2019SuperpositionOM,
  title={Superposition of many models into one},
  author={Brian Cheung and Alex Terekhov and Yubei Chen and Pulkit Agrawal and Bruno A. Olshausen},
  journal={ArXiv},
  year={2019},
  volume={abs/1902.05522},
  url={https://api.semanticscholar.org/CorpusID:61153603}
}

@article{Hochreiter1997LongSM,
  title={Long Short-Term Memory},
  author={Sepp Hochreiter and J{\"u}rgen Schmidhuber},
  journal={Neural Computation},
  year={1997},
  volume={9},
  pages={1735-1780},
  url={https://api.semanticscholar.org/CorpusID:1915014}
}

@inproceedings{Cruse1996NeuralNA,
  title={Neural networks as cybernetic systems},
  author={Holk Cruse},
  year={1996},
  url={https://api.semanticscholar.org/CorpusID:60958322}
}

@article{doi:10.1146/annurev-neuro-092619-094115,
author = {Vyas, Saurabh and Golub, Matthew D. and Sussillo, David and Shenoy, Krishna V.},
title = {Computation Through Neural Population Dynamics},
journal = {Annual Review of Neuroscience},
volume = {43},
number = {1},
pages = {249-275},
year = {2020},
doi = {10.1146/annurev-neuro-092619-094115},
    note ={PMID: 32640928},

URL = {

        https://doi.org/10.1146/annurev-neuro-092619-094115



},
eprint = {

        https://doi.org/10.1146/annurev-neuro-092619-094115



}
,
    abstract = { Significant experimental, computational, and theoretical work has identified rich structure within the coordinated activity of interconnected neural populations. An emerging challenge now is to uncover the nature of the associated computations, how they are implemented, and what role they play in driving behavior. We term this computation through neural population dynamics. If successful, this framework will reveal general motifs of neural population activity and quantitatively describe how neural population dynamics implement computations necessary for driving goal-directed behavior. Here, we start with a mathematical primer on dynamical systems theory and analytical tools necessary to apply this perspective to experimental data. Next, we highlight some recent discoveries resulting from successful application of dynamical systems. We focus on studies spanning motor control, timing, decision-making, and working memory. Finally, we briefly discuss promising recent lines of investigation and future directions for the computation through neural population dynamics framework. }
}

@misc{Olah_2022, title={Mechanistic Interpretability, Variables, and the Importance of Interpretable Bases}, url={https://www.transformer-circuits.pub/2022/mech-interp-essay}, journal={Transformer Circuits Thread}, publisher={www.transformer-circuits.pub}, author={Olah, Chris}, year={2022}, month={Jun}}

@inproceedings{Bostrom2014SuperintelligencePD,
  title={Superintelligence: Paths, Dangers, Strategies},
  author={Nick Bostrom},
  year={2014},
  url={https://api.semanticscholar.org/CorpusID:106762840}
}

@article{Christian2021TheAP,
  title={The Alignment Problem: Machine Learning and Human Values},
  author={Brian R. Christian},
  journal={Perspectives on Science and Christian Faith},
  year={2021},
  url={https://api.semanticscholar.org/CorpusID:253505740}
}

@inproceedings{Mller2013FuturePI,
  title={Future Progress in Artificial Intelligence: A Survey of Expert Opinion},
  author={Vincent C. M{\"u}ller and Nick Bostrom},
  booktitle={Conference on Philosophy and Theory of Artificial Intelligence},
  year={2013},
  url={https://api.semanticscholar.org/CorpusID:111337952}
}

@article{Sears2021ThePE,
  title={The precipice: Existential risk and the future of humanity, TobyOrd, Hachette Books, New York, NY, 2020. 480pp. \$30.00 (cloth)},
  author={Nathan Alexander Sears},
  journal={Governance},
  year={2021},
  volume={34},
  pages={937-941},
  url={https://api.semanticscholar.org/CorpusID:236260716}
}

@article{Alishahi2019AnalyzingAI,
  title={Analyzing and interpreting neural networks for NLP: A report on the first BlackboxNLP workshop},
  author={A. Alishahi and Grzegorz Chrupała and Tal Linzen},
  journal={Natural Language Engineering},
  year={2019},
  volume={25},
  pages={543 - 557},
  url={https://api.semanticscholar.org/CorpusID:102350595}
}

@article{Brown2020LanguageMA,
  title={Language Models are Few-Shot Learners},
  author={Tom B. Brown and Benjamin Mann and Nick Ryder and Melanie Subbiah and Jared Kaplan and Prafulla Dhariwal and Arvind Neelakantan and Pranav Shyam and Girish Sastry and Amanda Askell and Sandhini Agarwal and Ariel Herbert-Voss and Gretchen Krueger and Tom Henighan and Rewon Child and Aditya Ramesh and Daniel M. Ziegler and Jeff Wu and Clemens Winter and Christopher Hesse and Mark Chen and Eric Sigler and Ma-teusz Litwin and Scott Gray and Benjamin Chess and Jack Clark and Christopher Berner and Sam McCandlish and Alec Radford and Ilya Sutskever and Dario Amodei},
  journal={ArXiv},
  year={2020},
  volume={abs/2005.14165},
  url={https://api.semanticscholar.org/CorpusID:218971783}
}

@article{Fong2017InterpretableEO,
  title={Interpretable Explanations of Black Boxes by Meaningful Perturbation},
  author={Ruth C. Fong and Andrea Vedaldi},
  journal={2017 IEEE International Conference on Computer Vision (ICCV)},
  year={2017},
  pages={3449-3457},
  url={https://api.semanticscholar.org/CorpusID:1633753}
}

@article{Buhrmester2019AnalysisOE,
  title={Analysis of Explainers of Black Box Deep Neural Networks for Computer Vision: A Survey},
  author={Vanessa Buhrmester and David M{\"u}nch and Michael Arens},
  journal={ArXiv},
  year={2019},
  volume={abs/1911.12116},
  url={https://api.semanticscholar.org/CorpusID:208309956}
}

@article{Raukur2022TowardTA,
  title={Toward Transparent AI: A Survey on Interpreting the Inner Structures of Deep Neural Networks},
  author={Tilman Raukur and An Chang Ho and Stephen Casper and Dylan Hadfield-Menell},
  journal={2023 IEEE Conference on Secure and Trustworthy Machine Learning (SaTML)},
  year={2022},
  pages={464-483},
  url={https://api.semanticscholar.org/CorpusID:251104722}
}

@article{Marschall2023ProbingLT,
  title={Probing learning through the lens of changes in circuit dynamics},
  author={Owen Marschall and Cristina Savin},
  journal={bioRxiv},
  year={2023},
  url={https://api.semanticscholar.org/CorpusID:261967145}
}

@article{Rowley2009SpectralAO,
  title={Spectral analysis of nonlinear flows},
  author={Clarence W. Rowley and Igor Mezi{\'c} and Shervin Bagheri and Philipp Schlatter and Dan S. Henningson},
  journal={Journal of Fluid Mechanics},
  year={2009},
  volume={641},
  pages={115 - 127},
  url={https://api.semanticscholar.org/CorpusID:1546931}
}

@article{Kim1996QUASIPERIODICRA,
  title={QUASI-PERIODIC RESPONSE AND STABILITY ANALYSIS FOR NON-LINEAR SYSTEMS: A GENERAL APPROACH},
  author={Youngbae Kim},
  journal={Journal of Sound and Vibration},
  year={1996},
  volume={192},
  pages={821-833},
  url={https://api.semanticscholar.org/CorpusID:119374962}
}

@article{Conmy2023TowardsAC,
  title={Towards Automated Circuit Discovery for Mechanistic Interpretability},
  author={Arthur Conmy and Augustine N. Mavor-Parker and Aengus Lynch and Stefan Heimersheim and Adri{\`a} Garriga-Alonso},
  journal={ArXiv},
  year={2023},
  volume={abs/2304.14997},
  url={https://api.semanticscholar.org/CorpusID:258418244}
}

@article{Wang2022InterpretabilityIT,
  title={Interpretability in the Wild: a Circuit for Indirect Object Identification in GPT-2 small},
  author={Kevin Wang and Alexandre Variengien and Arthur Conmy and Buck Shlegeris and Jacob Steinhardt},
  journal={ArXiv},
  year={2022},
  volume={abs/2211.00593},
  url={https://api.semanticscholar.org/CorpusID:253244237}
}

@inproceedings{Li2015ConvergentLD,
  title={Convergent Learning: Do different neural networks learn the same representations?},
  author={Yixuan Li and Jason Yosinski and Jeff Clune and Hod Lipson and John E. Hopcroft},
  booktitle={FE@NIPS},
  year={2015},
  url={https://api.semanticscholar.org/CorpusID:12060927}
}

@Inbook{Tulving1972,
author={Tulving, Endel},
title={Episodic and semantic memory.},
series={Organization of memory.},
year={1972},
publisher={Academic Press},
address={Oxford,  England},
pages={xiii, 423-xiii, 423},
keywords={*Cognitive Processes; *Experiences (Events); *Memory; Semantics},
abstract={In this chapter I discuss the possibility that semantic memory, among other things, is not the kind of memory that psychologists have been studying since the time of Ebbinghaus. I will suggest that there are sufficiently fundamental differences between the two forms of memory to recommend that we consider, at least for the time being, the two categories separately. To facilitate subsequent discussion, I will refer to this other kind of memory, the one that semantic memory is not, as 'episodic' memory. (PsycINFO Database Record (c) 2016 APA, all rights reserved)}
}

@article{DBLP:journals/corr/GravesWD14,
  author       = {Alex Graves and
                  Greg Wayne and
                  Ivo Danihelka},
  title        = {Neural Turing Machines},
  journal      = {CoRR},
  volume       = {abs/1410.5401},
  year         = {2014},
  url          = {http://arxiv.org/abs/1410.5401},
  eprinttype    = {arXiv},
  eprint       = {1410.5401},
  timestamp    = {Mon, 13 Aug 2018 16:46:28 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/GravesWD14.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{Keller2023TravelingWE,
  title={Traveling Waves Encode the Recent Past and Enhance Sequence Learning},
  author={Thomas Anderson Keller and Lyle E. Muller and Terrence J. Sejnowski and Max Welling},
  journal={ArXiv},
  year={2023},
  volume={abs/2309.08045},
  url={https://api.semanticscholar.org/CorpusID:262013982}
}

@article{Maheswaranathan2019UniversalityAI,
  title={Universality and individuality in neural dynamics across large populations of recurrent networks},
  author={Niru Maheswaranathan and Alex H. Williams and Matthew D. Golub and Surya Ganguli and David Sussillo},
  journal={Advances in neural information processing systems},
  year={2019},
  volume={2019},
  pages={
          15629-15641
        },
  url={https://api.semanticscholar.org/CorpusID:197935221}
}

@article{Graves2013,
author = {Graves, A. and Mohamed, A. and Hinton, G. E.},
title = {Speech recognition with deep recurrent neural networks},
journal = {2013 IEEE International Conference on Acoustics, Speech and Signal Processing},
year = {2013},
doi = {10.1109/icassp.2013.6638947}
}

@article{Che2018,
  author = {Che, Z. and Purushotham, S. and Cho, K. and Sontag, D. and Liu, Y.},
  title = {Recurrent neural networks for multivariate time series with missing values},
  journal = {Scientific Reports},
  year = {2018},
  volume = {8},
  issue = {1},
  doi = {10.1038/s41598-018-24271-9}
}

@inproceedings{Marcus2001TheAM,
  title={The Algebraic Mind: Integrating Connectionism and Cognitive Science},
  author={Gary F. Marcus},
  year={2001},
  url={https://api.semanticscholar.org/CorpusID:142639115}
}

@article{Umbach2020TimeCI,
  title={Time cells in the human hippocampus and entorhinal cortex support episodic memory},
  author={Gray Umbach and Pranish A. Kantak and Joshua Jacobs and Michael J. Kahana and Brad E. Pfeiffer and Michael R. Sperling and Bradley C. Lega},
  journal={Proceedings of the National Academy of Sciences of the United States of America},
  year={2020},
  volume={117},
  pages={28463 - 28474},
  url={https://api.semanticscholar.org/CorpusID:213411329}
}

@article{Davis2020SpontaneousTC,
  title={Spontaneous Traveling Cortical Waves Gate Perception in Behaving Primates},
  author={Zachary W. Davis and Lyle E. Muller and Julio C. Martinez-Trujillo and Terrence J. Sejnowski and John H. Reynolds},
  journal={Nature},
  year={2020},
  volume={587},
  pages={432 - 436},
  url={https://api.semanticscholar.org/CorpusID:222214431}
}

@article{Davis2020SpontaneousTW,
  title={Spontaneous traveling waves naturally emerge from horizontal fiber time delays and travel through locally asynchronous-irregular states},
  author={Zachary W. Davis and Gabriel B. Benigno and Charlee Fletterman and Theo Desbordes and Christopher Steward and Terrence J. Sejnowski and John H Reynolds and Lyle E. Muller},
  journal={Nature Communications},
  year={2020},
  volume={12},
  url={https://api.semanticscholar.org/CorpusID:239026549}
}

@article{Muller2018CorticalTW,
  title={Cortical travelling waves: mechanisms and computational principles},
  author={Lyle E. Muller and Fr{\'e}d{\'e}ric Chavane and John H. Reynolds and Terrence J. Sejnowski},
  journal={Nature Reviews Neuroscience},
  year={2018},
  volume={19},
  pages={255-268},
  url={https://api.semanticscholar.org/CorpusID:4358777}
}

@article{Perrard2016WaveBasedTM,
  title={Wave-Based Turing Machine: Time Reversal and Information Erasing.},
  author={St{\'e}phane Perrard and Emmanuel Fort and Yves Couder},
  journal={Physical review letters},
  year={2016},
  volume={117 9},
  pages={
          094502
        },
  url={https://api.semanticscholar.org/CorpusID:26025006}
}

@article{Chumbley2008AttractorMO,
  title={Attractor models of working memory and their modulation by reward},
  author={Justin R. Chumbley and Raymond J. Dolan and Karl J. Friston},
  journal={Biological Cybernetics},
  year={2008},
  volume={98},
  pages={11-18},
  url={https://api.semanticscholar.org/CorpusID:15357209}
}

@article{Compte2000SynapticMA,
  title={Synaptic mechanisms and network dynamics underlying spatial working memory in a cortical network model.},
  author={Albert Compte and Nicolas J.-B. Brunel and Patricia S. Goldman-Rakic and Xiao-Jing Wang},
  journal={Cerebral cortex},
  year={2000},
  volume={10 9},
  pages={
          910-23
        },
  url={https://api.semanticscholar.org/CorpusID:7239548}
}

@article{Sreenivasan2014RevisitingTR,
  title={Revisiting the role of persistent neural activity during working memory},
  author={Kartik K. Sreenivasan and Clayton E. Curtis and Mark D’Esposito},
  journal={Trends in Cognitive Sciences},
  year={2014},
  volume={18},
  pages={82-89},
  url={https://api.semanticscholar.org/CorpusID:8631053}
}

@article{Barrouillet2011FurtherEF,
  title={Further evidence for temporal decay in working memory: reply to Lewandowsky and Oberauer (2009).},
  author={Pierre Barrouillet and Sophie Portrat and Evie Vergauwe and Kevin Diependaele and Val{\'e}rie Camos},
  journal={Journal of experimental psychology. Learning, memory, and cognition},
  year={2011},
  volume={37 5},
  pages={
          1302-17
        },
  url={https://api.semanticscholar.org/CorpusID:15384577}
}

@article{McGeoch1932ForgettingAT,
  title={Forgetting and the law of disuse.},
  author={John Alexander McGeoch},
  journal={Psychological Review},
  year={1932},
  volume={39},
  pages={352-370},
  url={https://api.semanticscholar.org/CorpusID:144896090}
}

@inproceedings{Hochreiter2001GradientFI,
  title={Gradient Flow in Recurrent Nets: the Difficulty of Learning Long-Term Dependencies},
  author={Sepp Hochreiter and Yoshua Bengio},
  booktitle={A Field Guide to Dynamical Recurrent Networks},
  year={2001},
  url={https://api.semanticscholar.org/CorpusID:17278462}
}

@inproceedings{Pascanu2012OnTD,
  title={On the difficulty of training recurrent neural networks},
  author={Razvan Pascanu and Tomas Mikolov and Yoshua Bengio},
  booktitle={International Conference on Machine Learning},
  year={2012},
  url={https://api.semanticscholar.org/CorpusID:14650762}
}

@inproceedings{Arjovsky2015UnitaryER,
  title={Unitary Evolution Recurrent Neural Networks},
  author={Mart{\'i}n Arjovsky and Amar Shah and Yoshua Bengio},
  booktitle={International Conference on Machine Learning},
  year={2015},
  url={https://api.semanticscholar.org/CorpusID:812047}
}

@article{Benigno2023WavesTO,
  title={Waves traveling over a map of visual space can ignite short-term predictions of sensory input},
  author={Gabriel B. Benigno and Roberto C. Budzinski and Zachary W. Davis and John H. Reynolds and Lyle E. Muller},
  journal={Nature Communications},
  year={2023},
  volume={14},
  url={https://api.semanticscholar.org/CorpusID:259130051}
}

@article{Dao2022HungryHH,
  title={Hungry Hungry Hippos: Towards Language Modeling with State Space Models},
  author={Tri Dao and Daniel Y. Fu and Khaled Kamal Saab and A. Waldmann Thomas and Atri Rudra and Christopher R{\'e}},
  journal={ArXiv},
  year={2022},
  volume={abs/2212.14052},
  url={https://api.semanticscholar.org/CorpusID:255340454}
}
@article{Hansen1995TIMESA,
  title={TIME SERIES ANALYSIS James D. Hamilton Princeton University Press, 1994},
  author={Bruce E. Hansen},
  journal={Econometric Theory},
  year={1995},
  volume={11},
  pages={625 - 630},
  url={https://api.semanticscholar.org/CorpusID:122102593}
}
@article{Gu2023MambaLS,
  title={Mamba: Linear-Time Sequence Modeling with Selective State Spaces},
  author={Albert Gu and Tri Dao},
  journal={ArXiv},
  year={2023},
  volume={abs/2312.00752},
  url={https://api.semanticscholar.org/CorpusID:265551773}
}

@article{Gu2022HowTT,
  title={How to Train Your HiPPO: State Space Models with Generalized Orthogonal Basis Projections},
  author={Albert Gu and Isys Johnson and Aman Timalsina and Atri Rudra and Christopher R{\'e}},
  journal={ArXiv},
  year={2022},
  volume={abs/2206.12037},
  url={https://api.semanticscholar.org/CorpusID:250048824}
}

@article{Wechsler1945ASM,
  title={A Standardized Memory Scale for Clinical Use},
  author={David Wechsler},
  journal={The Journal of Psychology},
  year={1945},
  volume={19},
  pages={87-95},
  url={https://api.semanticscholar.org/CorpusID:144877299}
}

@inproceedings{Chelune1990TheWM,
  title={The Wechsler Memory Scale—Revised},
  author={Gordon Chelune and Robert A. Bornstein and Aurelio Prifitera},
  year={1990},
  url={https://api.semanticscholar.org/CorpusID:141102472}
}

@article{Cabbage2017AssessingWM,
  title={Assessing Working Memory in Children: The Comprehensive Assessment Battery for Children – Working Memory (CABC-WM)},
  author={Kathryn L. Cabbage and Shara Brinkley and Shelley I Gray and Mary Alt and Nelson Cowan and Samuel Green and Trudy Y. Kuo and Tiffany P. Hogan},
  journal={Journal of Visualized Experiments : JoVE},
  year={2017},
  url={https://api.semanticscholar.org/CorpusID:5044978}
}

@inproceedings{karuvally2023variable,
  title={Variable Memory: Beyond the Fixed Memory Assumption in Memory Modeling},
  author={Karuvally, Arjun and Siegelmann, Hava T},
  booktitle={Associative Memory \& Hopfield Networks in 2023},
  year={2023}
}

@inproceedings{karuvally2023episodic,
  title={Episodic Memory Theory of Recurrent Neural Networks: Insights into Long-Term Information Storage and Manipulation},
  author={Karuvally, Arjun and DelMastro, Peter and Siegelmann, Hava T},
  booktitle={Topological, Algebraic and Geometric Learning Workshops 2023},
  pages={371--383},
  year={2023},
  organization={PMLR}
}
@inproceedings{Katharopoulos2020TransformersAR,
  title={Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention},
  author={Angelos Katharopoulos and Apoorv Vyas and Nikolaos Pappas and Franccois Fleuret},
  booktitle={International Conference on Machine Learning},
  year={2020},
  url={https://api.semanticscholar.org/CorpusID:220250819}
}

@article{Elman1990FindingSI,
  title={Finding Structure in Time},
  author={Jeffrey L. Elman},
  journal={Cogn. Sci.},
  year={1990},
  volume={14},
  pages={179-211},
  url={https://api.semanticscholar.org/CorpusID:2763403}
}

@inproceedings{Cho2014LearningPR,
  title={Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation},
  author={Kyunghyun Cho and Bart van Merrienboer and Çaglar G{\"u}lçehre and Dzmitry Bahdanau and Fethi Bougares and Holger Schwenk and Yoshua Bengio},
  booktitle={Conference on Empirical Methods in Natural Language Processing},
  year={2014},
  url={https://api.semanticscholar.org/CorpusID:5590763}
}

@inproceedings{Devlin2019BERTPO,
  title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  author={Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
  booktitle={North American Chapter of the Association for Computational Linguistics},
  year={2019},
  url={https://api.semanticscholar.org/CorpusID:52967399}
}
