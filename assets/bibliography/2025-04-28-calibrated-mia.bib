@article{das2024blind,
  title={Blind Baselines Beat Membership Inference Attacks for Foundation Models},
  author={Das, Debeshee and Zhang, Jie and Tram{\`e}r, Florian},
  journal={arXiv preprint arXiv:2406.16201},
  year={2024}
}

@inproceedings{maini2024llm,
title={LLM Dataset Inference: Did you train on my dataset?},
author={Pratyush Maini and Hengrui Jia and Nicolas Papernot and Adam Dziedzic},
booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},
year={2024},
url={https://openreview.net/forum?id=Fr9d1UMc37}
}

@inproceedings{duan2024membership,
  title={Do Membership Inference Attacks Work on Large Language Models?},
  author={Duan, Michael and Suri, Anshuman and Mireshghallah, Niloofar and Min, Sewon and Shi, Weijia and Zettlemoyer, Luke and Tsvetkov, Yulia and Choi, Yejin and Evans, David and Hajishirzi, Hannaneh},
  booktitle={Conference on Language Modeling},
  year={2024}
}

@inproceedings{zhang2024pretraining,
  title={Pretraining Data Detection for Large Language Models: A Divergence-based Calibration Method},
  author={Zhang, Weichao and Zhang, Ruqing and Guo, Jiafeng and Rijke, Maarten and Fan, Yixing and Cheng, Xueqi},
  booktitle={Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing},
  pages={5263--5274},
  year={2024}
}

@inproceedings{carlini2022membership,
  title={Membership inference attacks from first principles},
  author={Carlini, Nicholas and Chien, Steve and Nasr, Milad and Song, Shuang and Terzis, Andreas and Tramer, Florian},
  booktitle={2022 IEEE Symposium on Security and Privacy (SP)},
  pages={1897--1914},
  year={2022},
  organization={IEEE}
}

@inproceedings{carlini2021extracting,
  title={Extracting training data from large language models},
  author={Carlini, Nicholas and Tramer, Florian and Wallace, Eric and Jagielski, Matthew and Herbert-Voss, Ariel and Lee, Katherine and Roberts, Adam and Brown, Tom and Song, Dawn and Erlingsson, Ulfar and others},
  booktitle={30th USENIX Security Symposium (USENIX Security 21)},
  pages={2633--2650},
  year={2021}
}

@misc{shi2023detecting,
    title={Detecting Pretraining Data from Large Language Models},
    author={Weijia Shi and Anirudh Ajith and Mengzhou Xia and Yangsibo Huang and Daogao Liu and Terra Blevins and Danqi Chen and Luke Zettlemoyer},
    year={2023},
    eprint={2310.16789},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}