@article{ainslie2023gqa,
  author = {Ainslie, Joshua and others},
  title = {GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints},
  year = {2023},
  journal = {arXiv preprint},
  url = {https://arxiv.org/abs/2305.13245}
}

@misc{bannier2023rolling,
  author = {Bannier, P.A.},
  title = {Rolling buffer cache},
  year = {2023},
  howpublished = {\url{https://github.com/ggerganov/llama.cpp/discussions/3581}}
}

@misc{biderman2021rotary,
  author = {Biderman, Stella and others},
  title = {Rotary Embeddings: A Relative Revolution},
  year = {2021},
  howpublished = {\url{https://blog.eleuther.ai/rotary-embeddings/}}
}

@article{brandon2024reducing,
  author = {Brandon, William and Mishra, Mayank and Nrusimha, Aniruddha and Panda, Rameswar and Kelley, Jonathan Ragan},
  title = {Reducing Transformer Key-Value Cache Size with Cross-Layer Attention},
  year = {2024},
  journal = {arXiv preprint},
  url = {https://arxiv.org/abs/2405.12981}
}

@article{chowdhery2022palm,
  author = {Chowdhery, Aakanksha and others},
  title = {PaLM: Scaling Language Modeling with Pathways},
  year = {2022},
  journal = {arXiv preprint},
  url = {https://arxiv.org/abs/2204.02311}
}

@article{dao2022flashattention,
  author = {Dao, Tri and Fu, Daniel Y. and Ermon, Stefano and Rudra, Atri and Ré, Christopher},
  title = {FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness},
  year = {2022},
  journal = {arXiv preprint},
  url = {https://arxiv.org/abs/2205.14135}
}

@article{dehghani2023scaling,
  author = {Dehghani, Mostafa and others},
  title = {Scaling Vision Transformers to 22 Billion Parameters},
  year = {2023},
  journal = {arXiv preprint},
  url = {https://arxiv.org/abs/2302.05442}
}

@article{gu2023mamba,
  author = {Gu, Albert and Dao, Tri},
  title = {Mamba: Linear-Time Sequence Modeling with Selective State Spaces},
  year = {2023},
  journal = {arXiv preprint},
  url = {https://arxiv.org/abs/2312.00752}
}

@article{leech2024ten,
  author = {Leech, Gavin and Garfinkel, Simson and Yagudin, Misha and Briand, Alexander and Zhuralev, Aleksandr},
  title = {Ten Hard Problems in Artificial Intelligence We Must Get Right},
  year = {2024},
  journal = {arXiv preprint},
  url = {https://arxiv.org/abs/2402.04464}
}

@article{liu2024deepseek,
  author = {Liu, Aixin and others},
  title = {DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model},
  year = {2024},
  journal = {arXiv preprint},
  url = {https://arxiv.org/abs/2405.04434}
}

@misc{liu2024mixture,
  author = {Liu, Yuxi},
  title = {Mixture of Experts},
  year = {2024},
  howpublished = {\url{https://yuxi-liu-wired.github.io/essays/posts/mixture-of-experts/}}
}

@article{loshchilov2019decoupled,
  author = {Loshchilov, Ilya and Hutter, Frank},
  title = {Decoupled Weight Decay Regularization},
  year = {2019},
  journal = {arXiv preprint},
  url = {https://arxiv.org/abs/1711.05101v3}
}

@misc{miller2023attention,
  author = {Miller, Evan},
  title = {Attention Is Off By One},
  year = {2023},
  howpublished = {\url{https://www.evanmiller.org/attention-is-off-by-one.html}}
}

@misc{press2023alibi,
  author = {Press, Ofir},
  title = {Train Short, Test Long: Attention with Linear Biases (ALiBi) Enables Input Length Extrapolation},
  year = {2023},
  howpublished = {\url{https://github.com/ofirpress/attention_with_linear_biases/#faq}}
}

@misc{radford2019language,
  author = {Radford, Alec and Wu, Jeffrey and Amodei, Dario and Sutskever, Ilya and others},
  title = {Language Models are Unsupervised Multitask Learners},
  year = {2019},
  howpublished = {\url{https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf}}
}

@misc{roldan2024swiglu,
  author = {Roldán, J. Carlos},
  title = {What is SwiGLU?},
  year = {2024},
  howpublished = {\url{https://jcarlosroldan.com/post/348/what-is-swiglu}}
}

@article{shazeer2019fast,
  author = {Shazeer, Noam},
  title = {Fast Transformer Decoding: One Write-Head is All You Need},
  year = {2019},
  journal = {arXiv preprint},
  url = {https://arxiv.org/abs/1911.02150}
}

@misc{stackoverflow2024swiglu,
  author = {Stack Overflow},
  title = {How to implement SwiGLU activation? Why does SwiGLU takes in two tensors?},
  year = {2024},
  howpublished = {\url{https://stackoverflow.com/questions/79047727/how-to-implement-swiglu-activation-why-does-swiglu-takes-in-two-tensors}}
}

@article{sun2024hunyuan,
  author = {Sun, Xingwu and others},
  title = {Hunyuan-Large: An Open-Source MoE Model with 52 Billion Activated Parameters by Tencent},
  year = {2024},
  journal = {arXiv preprint},
  url = {https://arxiv.org/abs/2411.02265}
}

@article{vaswani2017attention,
  author = {Vaswani, Ashish and others},
  title = {Attention Is All You Need},
  year = {2017},
  journal = {arXiv preprint},
  url = {https://arxiv.org/abs/1706.03762}
}

@article{vyas2024soap,
  author = {Vyas, Nikhil and others},
  title = {SOAP: Improving and Stabilizing Shampoo using Adam},
  year = {2024},
  journal = {arXiv preprint},
  url = {https://arxiv.org/abs/2409.11321}
}

@article{xiong2020layernorm,
  author = {Xiong, Riubin and others},
  title = {On Layer Normalization in the Transformer Architecture},
  year = {2020},
  journal = {arXiv preprint},
  url = {https://arxiv.org/abs/2002.04745}
}

@article{zhang2019rmsnorm,
  author = {Zhang, Biao and Sennrich, Rico},
  title = {Root Mean Square Layer Normalization},
  year = {2019},
  journal = {arXiv preprint},
  url = {https://arxiv.org/abs/1910.07467}
}
