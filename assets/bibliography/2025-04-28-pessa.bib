@article{hu2021lora,
  title={Lora: Low-rank adaptation of large language models},
  author={Hu, Edward J and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
  journal={arXiv preprint arXiv:2106.09685},
  year={2021}
}
@article{dettmers2024qlora,
  title={Qlora: Efficient finetuning of quantized llms},
  author={Dettmers, Tim and Pagnoni, Artidoro and Holtzman, Ari and Zettlemoyer, Luke},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}
@article{shuttleworth2024lora,
  title={LoRA vs Full Fine-tuning: An Illusion of Equivalence},
  author={Shuttleworth, Reece and Andreas, Jacob and Torralba, Antonio and Sharma, Pratyusha},
  journal={arXiv preprint arXiv:2410.21228},
  year={2024}
}
@article{meng2024pissa,
  title={Pissa: Principal singular values and singular vectors adaptation of large language models},
  author={Meng, Fanxu and Wang, Zhaohui and Zhang, Muhan},
  journal={arXiv preprint arXiv:2404.02948},
  year={2024}
}
@article{lingam2024svft,
  title={SVFT: Parameter-Efficient Fine-Tuning with Singular Vectors},
  author={Lingam, Vijay and Tejaswi, Atula and Vavre, Aditya and Shetty, Aneesh and Gudur, Gautham Krishna and Ghosh, Joydeep and Dimakis, Alex and Choi, Eunsol and Bojchevski, Aleksandar and Sanghavi, Sujay},
  journal={arXiv preprint arXiv:2405.19597},
  year={2024}
}

@article{zhang2023adalora,
  title={AdaLoRA: Adaptive budget allocation for parameter-efficient fine-tuning},
  author={Zhang, Qingru and Chen, Minshuo and Bukharin, Alexander and Karampatziakis, Nikos and He, Pengcheng and Cheng, Yu and Chen, Weizhu and Zhao, Tuo},
  journal={arXiv preprint arXiv:2303.10512},
  year={2023}
}
@article{zi2023delta,
  title={Delta-lora: Fine-tuning high-rank parameters with the delta of low-rank matrices},
  author={Zi, Bojia and Qi, Xianbiao and Wang, Lingzhi and Wang, Jianan and Wong, Kam-Fai and Zhang, Lei},
  journal={arXiv preprint arXiv:2309.02411},
  year={2023}
}
@inproceedings{li2023losparse,
  title={Losparse: Structured compression of large language models based on low-rank and sparse approximation},
  author={Li, Yixiao and Yu, Yifan and Zhang, Qingru and Liang, Chen and He, Pengcheng and Chen, Weizhu and Zhao, Tuo},
  booktitle={International Conference on Machine Learning},
  pages={20336--20350},
  year={2023},
  organization={PMLR}
}
@article{liu2024dora,
  title={Dora: Weight-decomposed low-rank adaptation},
  author={Liu, Shih-Yang and Wang, Chien-Yi and Yin, Hongxu and Molchanov, Pavlo and Wang, Yu-Chiang Frank and Cheng, Kwang-Ting and Chen, Min-Hung},
  journal={arXiv preprint arXiv:2402.09353},
  year={2024}
}
@article{balazy2024lora,
  title={LoRA-XS: Low-Rank Adaptation with Extremely Small Number of Parameters},
  author={Ba{\l}azy, Klaudia and Banaei, Mohammadreza and Aberer, Karl and Tabor, Jacek},
  journal={arXiv preprint arXiv:2405.17604},
  year={2024}
}
@article{azizi2024lamda,
  title={LaMDA: Large Model Fine-Tuning via Spectrally Decomposed Low-Dimensional Adaptation},
  author={Azizi, Seyedarmin and Kundu, Souvik and Pedram, Massoud},
  journal={arXiv preprint arXiv:2406.12832},
  year={2024}
}
@article{buyukakyuz2024olora,
  title={OLoRA: Orthonormal Low-Rank Adaptation of Large Language Models},
  author={B{\"u}y{\"u}kaky{\"u}z, Kerim},
  journal={arXiv preprint arXiv:2406.01775},
  year={2024}
}
@article{yang2024corda,
  title={CorDA: Context-Oriented Decomposition Adaptation of Large Language Models},
  author={Yang, Yibo and Li, Xiaojie and Zhou, Zhongzhu and Song, Shuaiwen Leon and Wu, Jianlong and Nie, Liqiang and Ghanem, Bernard},
  journal={arXiv preprint arXiv:2406.05223},
  year={2024}
}
@article{wang2024milora,
  title={Milora: Harnessing minor singular components for parameter-efficient llm finetuning},
  author={Wang, Hanqing and Li, Yixia and Wang, Shuo and Chen, Guanhua and Chen, Yun},
  journal={arXiv preprint arXiv:2406.09044},
  year={2024}
}
@article{wang2024loraga,
  title={LoRA-GA: Low-Rank Adaptation with Gradient Approximation},
  author={Wang, Shaowen and Yu, Linxi and Li, Jian},
  journal={arXiv preprint arXiv:2407.05000},
  year={2024}
}
@article{wang2024lorapro,
  title={LoRA-Pro: Are Low-Rank Adapters Properly Optimized?},
  author={Wang, Zhengbo and Liang, Jian and He, Ran and Wang, Zilei and Tan, Tieniu},
  journal={arXiv preprint arXiv:2407.18242},
  year={2024}
}
@article{paischer2024one,
  title={One Initialization to Rule them All: Fine-tuning via Explained Variance Adaptation},
  author={Paischer, Fabian and Hauzenberger, Lukas and Schmied, Thomas and Alkin, Benedikt and Deisenroth, Marc Peter and Hochreiter, Sepp},
  journal={arXiv preprint arXiv:2410.07170},
  year={2024}
}
@article{li2024svdqunat,
  title={SVDQunat: Absorbing Outliers by Low-Rank Components for 4-Bit Diffusion Models},
  author={Li, Muyang and Lin, Yujun and Zhang, Zhekai and Cai, Tianle and Li, Xiuyu and Guo, Junxian and Xie, Enze and Meng, Chenlin and Zhu, Jun-Yan and Han, Song},
  journal={arXiv preprint arXiv:2411.05007},
  year={2024}
}
@inproceedings{radford2023robust,
  title={Robust speech recognition via large-scale weak supervision},
  author={Radford, Alec and Kim, Jong Wook and Xu, Tao and Brockman, Greg and McLeavey, Christine and Sutskever, Ilya},
  booktitle={International conference on machine learning},
  pages={28492--28518},
  year={2023},
  organization={PMLR}
}
@article{gandhi2023distil,
  title={Distil-whisper: Robust knowledge distillation via large-scale pseudo labelling},
  author={Gandhi, Sanchit and von Platen, Patrick and Rush, Alexander M},
  journal={arXiv preprint arXiv:2311.00430},
  year={2023}
}
@article{su2024roformer,
  title={Roformer: Enhanced transformer with rotary position embedding},
  author={Su, Jianlin and Ahmed, Murtadha and Lu, Yu and Pan, Shengfeng and Bo, Wen and Liu, Yunfeng},
  journal={Neurocomputing},
  volume={568},
  pages={127063},
  year={2024},
  publisher={Elsevier}
}
