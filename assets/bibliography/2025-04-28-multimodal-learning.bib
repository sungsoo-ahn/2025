@inproceedings{
dosovitskiy2021an,
title={An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
author={Alexey Dosovitskiy and Lucas Beyer and Alexander Kolesnikov and Dirk Weissenborn and Xiaohua Zhai and Thomas Unterthiner and Mostafa Dehghani and Matthias Minderer and Georg Heigold and Sylvain Gelly and Jakob Uszkoreit and Neil Houlsby},
booktitle={International Conference on Learning Representations (ICLR)},
year={2021},
}

@inproceedings{zeng2024rgb,
  title={Rgb↔ x: Image decomposition and synthesis using material-and lighting-aware diffusion models},
  author={Zeng, Zheng and Deschaintre, Valentin and Georgiev, Iliyan and Hold-Geoffroy, Yannick and Hu, Yiwei and Luan, Fujun and Yan, Ling-Qi and Ha{\v{s}}an, Milo{\v{s}}},
  booktitle={ACM SIGGRAPH},
  year={2024}
}

@article{dang2024tokenization,
  title={Tokenization and Morphology in Multilingual Language Models: A Comparative Analysis of mT5 and ByT5},
  author={Dang, Thao Anh and Raviv, Limor and Galke, Lukas},
  journal={arXiv preprint arXiv:2410.11627},
  year={2024}
}

@article{lee2017fully,
  title={Fully character-level neural machine translation without explicit segmentation},
  author={Lee, Jason and Cho, Kyunghyun and Hofmann, Thomas},
  journal={Transactions of the Association for Computational Linguistics},
  year={2017},
}

@inproceedings{
chirkova2023codebpe,
title={Code{BPE}: Investigating Subtokenization Options for Large Language Model Pretraining on Source Code},
author={Nadezhda Chirkova and Sergey Troshin},
booktitle={International Conference on Learning Representations (ICLR)},
year={2023},
}

@article{imrie2024automated,
  title={Automated Ensemble Multimodal Machine Learning for Healthcare},
  author={Imrie, Fergus and Denner, Stefan and Brunschwig, Lucas S and Maier-Hein, Klaus and van der Schaar, Mihaela},
  journal={arXiv preprint arXiv:2407.18227},
  year={2024}
}

@inproceedings{antol2015vqa,
  title={Vqa: Visual question answering},
  author={Antol, Stanislaw and Agrawal, Aishwarya and Lu, Jiasen and Mitchell, Margaret and Batra, Dhruv and Zitnick, C Lawrence and Parikh, Devi},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
  year={2015}
}

@inproceedings{goyal2017making,
  title={Making the v in vqa matter: Elevating the role of image understanding in visual question answering},
  author={Goyal, Yash and Khot, Tejas and Summers-Stay, Douglas and Batra, Dhruv and Parikh, Devi},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2017}
}

@inproceedings{agrawal2018don,
  title={Don't just assume; look and answer: Overcoming priors for visual question answering},
  author={Agrawal, Aishwarya and Batra, Dhruv and Parikh, Devi and Kembhavi, Aniruddha},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2018}
}

@inproceedings{dancette2021beyond,
  title={Beyond question-based biases: Assessing multimodal shortcut learning in visual question answering},
  author={Dancette, Corentin and Cadene, Remi and Teney, Damien and Cord, Matthieu},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
  year={2021}
}

@inproceedings{si2022language,
  title={Language Prior Is Not the Only Shortcut: A Benchmark for Shortcut Learning in VQA},
  author={Si, Qingyi and Meng, Fandong and Zheng, Mingyu and Lin, Zheng and Liu, Yuanxin and Fu, Peng and Cao, Yanan and Wang, Weiping and Zhou, Jie},
  booktitle={Findings of the Association for Computational Linguistics: EMNLP},
  year={2022}
}

@article{team2023gemini,
  title={Gemini: a family of highly capable multimodal models},
  author={Team, Gemini and Anil, Rohan and Borgeaud, Sebastian and Alayrac, Jean-Baptiste and Yu, Jiahui and Soricut, Radu and Schalkwyk, Johan and Dai, Andrew M and Hauth, Anja and Millican, Katie and others},
  journal={arXiv preprint arXiv:2312.11805},
  year={2023}
}

@article{liu2024visual,
  title={Visual instruction tuning},
  author={Liu, Haotian and Li, Chunyuan and Wu, Qingyang and Lee, Yong Jae},
  journal={Advances in Neural Information Processing Systems (NeurIPS)},
  year={2024}
}

@inproceedings{suhr2019corpus,
  title={A Corpus for Reasoning about Natural Language Grounded in Photographs},
  author={Suhr, Alane and Zhou, Stephanie and Zhang, Ally and Zhang, Iris and Bai, Huajun and Artzi, Yoav},
  booktitle={Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL)},
  year={2019}
}

@inproceedings{wang2020makes,
  title={What makes training multi-modal classification networks hard?},
  author={Wang, Weiyao and Tran, Du and Feiszli, Matt},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2020}
}

@article{baltruvsaitis2018multimodal,
  title={Multimodal machine learning: A survey and taxonomy},
  author={Baltru{\v{s}}aitis, Tadas and Ahuja, Chaitanya and Morency, Louis-Philippe},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year={2018},
  publisher={IEEE}
}

@article{barnum2020benefits,
  title={On the benefits of early fusion in multimodal representation learning},
  author={Barnum, George and Talukder, Sabera and Yue, Yisong},
  journal={arXiv preprint arXiv:2011.07191},
  year={2020}
}

@inproceedings{gadzicki2020early,
  title={Early vs late fusion in multimodal convolutional neural networks},
  author={Gadzicki, Konrad and Khamsehashari, Razieh and Zetzsche, Christoph},
  booktitle={IEEE international conference on information fusion (FUSION)},
  year={2020},
}

@inproceedings{hessel2020does,
  title={Does my multimodal model learn cross-modal interactions? It’s harder to tell than you might think!},
  author={Hessel, Jack and Lee, Lillian},
  booktitle={Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  year={2020}
}


@article{
likhosherstov2021polyvit,
title={PolyViT: Co-training Vision Transformers on Images, Videos and Audio},
author={Valerii Likhosherstov and Anurag Arnab and Krzysztof Marcin Choromanski and Mario Lucic and Yi Tay and Mostafa Dehghani},
journal={Transactions on Machine Learning Research (TMLR)},
year={2023},
}

@article{
liang2023highmodality,
title={High-Modality Multimodal Transformer: Quantifying Modality \& Interaction Heterogeneity for High-Modality Representation Learning},
author={Paul Pu Liang and Yiwei Lyu and Xiang Fan and Jeffrey Tsaw and Yudong Liu and Shentong Mo and Dani Yogatama and Louis-Philippe Morency and Russ Salakhutdinov},
journal={Transactions on Machine Learning Research (TMLR)},
year={2023},
}

@inproceedings{anderson2018bottom,
  title={Bottom-up and top-down attention for image captioning and visual question answering},
  author={Anderson, Peter and He, Xiaodong and Buehler, Chris and Teney, Damien and Johnson, Mark and Gould, Stephen and Zhang, Lei},
  booktitle={Proceedings of the IEEE International Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2018}
}

@inproceedings{joze2020mmtm,
  title={MMTM: Multimodal transfer module for CNN fusion},
  author={Joze, Hamid Reza Vaezi and Shaban, Amirreza and Iuzzolino, Michael L and Koishida, Kazuhito},
  booktitle={Proceedings of the IEEE International Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2020}
}

@inproceedings{wang2020deep,
  title={Deep multimodal fusion by channel exchanging},
  author={Wang, Yikai and Huang, Wenbing and Sun, Fuchun and Xu, Tingyang and Rong, Yu and Huang, Junzhou},
  booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
  year={2020}
}

@InProceedings{wu22characterizing,
  title = 	 {Characterizing and Overcoming the Greedy Nature of Learning in Multi-modal Deep Neural Networks},
  author = {Wu, Nan and Jastrzebski, Stanislaw and Cho, Kyunghyun and Geras, Krzysztof J},
  booktitle = {Proceedings of the International Conference on Machine Learning (ICML)},
  year = 	 {2022},

}


@inproceedings{morvant2014majority,
  title={Majority vote of diverse classifiers for late fusion},
  author={Morvant, Emilie and Habrard, Amaury and Ayache, St{\'e}phane},
  booktitle={Structural, Syntactic, and Statistical Pattern Recognition: Joint IAPR International Workshop},
  year={2014},
}

@inproceedings{kielaclark2015multi,
    title = "Multi- and Cross-Modal Semantics Beyond Vision: Grounding in Auditory Perception",
    author = "Kiela, Douwe  and
      Clark, Stephen",
    booktitle = {Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP)},
    year = "2015",
}

@inproceedings{shutova2016black,
    title = "Black Holes and White Rabbits: Metaphor Identification with Visual Features",
    author = "Shutova, Ekaterina  and
      Kiela, Douwe  and
      Maillard, Jean",
    booktitle = "Proceedings of the Conference of the North {A}merican Chapter of the Association for Computational Linguistics (NAACL)",
    year = "2016",
}


@inproceedings{li2021align,
  title={Align before fuse: Vision and language representation learning with momentum distillation},
  author={Li, Junnan and Selvaraju, Ramprasaath and Gotmare, Akhilesh and Joty, Shafiq and Xiong, Caiming and Hoi, Steven Chu Hong},
  booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
  year={2021}
}

@inproceedings{singh2022flava,
  title={Flava: A foundational language and vision alignment model},
  author={Singh, Amanpreet and Hu, Ronghang and Goswami, Vedanuj and Couairon, Guillaume and Galuba, Wojciech and Rohrbach, Marcus and Kiela, Douwe},
  booktitle={Proceedings of the IEEE International Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2022}
}

@inproceedings{tong2024cambrian,
  title={Cambrian-1: A Fully Open, Vision-Centric Exploration of Multimodal LLMs},
  author={Tong, Shengbang and Brown II, Ellis L and Wu, Penghao and Woo, Sanghyun and IYER, ADITHYA JAIRAM and Akula, Sai Charitha and Yang, Shusheng and Yang, Jihan and Middepogu, Manoj and Wang, Ziteng and others},
  booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
  year={2024},
}

@inproceedings{
madaan2024jointly,
title={Jointly Modeling Inter- \& Intra-Modality Dependencies for Multi-modal Learning},
author={Divyam Madaan and Taro Makino and Sumit Chopra and Kyunghyun Cho},
booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
year={2024},
}

@inproceedings{tong2024eyes,
  title={Eyes wide shut? exploring the visual shortcomings of multimodal llms},
  author={Tong, Shengbang and Liu, Zhuang and Zhai, Yuexiang and Ma, Yi and LeCun, Yann and Xie, Saining},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  year={2024}
}

@inproceedings{du2023uni,
  title={On uni-modal feature learning in supervised multi-modal learning},
  author={Du, Chenzhuang and Teng, Jiaye and Li, Tingle and Liu, Yichen and Yuan, Tianyuan and Wang, Yue and Yuan, Yang and Zhao, Hang},
  booktitle={International Conference on Machine Learning},
  year={2023},
}

@article{liang2022foundations,
  title={Foundations and Trends in Multimodal Machine Learning: Principles, Challenges, and Open Questions},
  author={Liang, Paul Pu and Zadeh, Amir and Morency, Louis-Philippe},
  journal={arXiv preprint arXiv:2209.03430},
  year={2022}
}

@article{baltruvsaitis2018multimodal,
  title={Multimodal machine learning: A survey and taxonomy},
  author={Baltru{\v{s}}aitis, Tadas and Ahuja, Chaitanya and Morency, Louis-Philippe},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  year={2018},
}
