
@article{ChatGPT-human,
  title={OpenAIs Big Reset},
  author={Wong, Matteo},
  journal={The Atlantic},
  year={2024}
}


@misc{metz2020riding, 
title={Riding Out Quarantine With a Chatbot Friend: ‘I Feel Very Connected’}, 
url={https://www.nytimes.com/2020/06/16/technology/chatbots-quarantine-coronavirus.html}, 
journal={The New York Times}, 
author={Cade Metz}, 
year={2020}, 
month={Jun}
}
@article{shteynberg2024does,
  title={Does it matter if empathic AI has no empathy?},
  author={Shteynberg, Garriy and Halpern, Jodi and Sadovnik, Amir and Garthoff, Jon and Perry, Anat and Hay, Jessica and Montemayor, Carlos and Olson, Michael A and Hulsey, Tim L and Fairweather, Abrol},
  journal={Nature Machine Intelligence},
  pages={1--2},
  year={2024},
  publisher={Nature Publishing Group UK London}
}
@inproceedings{zhou-etal-2023-navigating,
    title = "Navigating the Grey Area: How Expressions of Uncertainty and Overconfidence Affect Language Models",
    author = "Zhou, Kaitlyn  and
      Jurafsky, Dan  and
      Hashimoto, Tatsunori",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.emnlp-main.335",
    doi = "10.18653/v1/2023.emnlp-main.335",
    pages = "5506--5524",
    abstract = "The increased deployment of LMs for real-world tasks involving knowledge and facts makes it important to understand model epistemology: what LMs think they know, and how their attitudes toward that knowledge are affected by language use in their inputs. Here, we study an aspect of model epistemology: how epistemic markers of certainty, uncertainty, or evidentiality like {``}I{'}m sure it{'}s{''}, {``}I think it{'}s{''}, or {``}Wikipedia says it{'}s{''} affect models, and whether they contribute to model failures. We develop a typology of epistemic markers and inject 50 markers into prompts for question answering. We find that LMs are highly sensitive to epistemic markers in prompts, with accuracies varying more than 80{\%}. Surprisingly, we find that expressions of high certainty result in a 7{\%} decrease in accuracy as compared to low certainty expressions; similarly, factive verbs hurt performance, while evidentials benefit performance. Our analysis of a popular pretraining dataset shows that these markers of uncertainty are associated with answers on question-answering websites, while markers of certainty are associated with questions. These associations may suggest that the behavior of LMs is based on mimicking observed language use, rather than truly reflecting epistemic uncertainty.",
}

@misc{nishant2023this, 
title={@nishant_kj: ``This is even more interesting, 
someone put Bing into a depressive state''}, 
url={https://x.com/nishant_kj/status/1625353189091586048}, 
journal={X formerly Twitter}, 
author={Nishant}, 
year={2023}, 
month={Feb}
}

@article{zhou2024rel,
  title={Rel-AI: An Interaction-Centered Approach To Measuring Human-LM Reliance},
  author={Zhou, Kaitlyn and Hwang, Jena D and Ren, Xiang and Dziri, Nouha and Jurafsky, Dan and Sap, Maarten},
  journal={arXiv preprint arXiv:2407.07950},
  year={2024}
}

@misc{multi2023after, 
title={After using Claude 2 by Anthropic for 12 hours straight, here's what I found}, 
url={https://www.reddit.com/r/singularity/comments/14z1d8c/after_using_claude_2_by_anthropic_for_12_hours/}, 
journal={r/singularity on Reddit}, 
author={u/MultiMillionaire_}, 
year={2023}
}

@incollection{brynjolfsson2023turing,
  title={The turing trap: The promise & peril of human-like artificial intelligence},
  author={Brynjolfsson, Erik},
  booktitle={Augmented education in the global age},
  pages={103--116},
  year={2023},
  publisher={Routledge}
}

@article{friedman1992human,
  title={Human agency and responsible computing: Implications for computer system design},
  author={Friedman, Batya and Kahn Jr, Peter H},
  journal={Journal of Systems and Software},
  volume={17},
  number={1},
  pages={7--14},
  year={1992},
  publisher={Elsevier}
}

@inproceedings{chan2023harms,
  title={Harms from increasingly agentic algorithmic systems},
  author={Chan, Alan and Salganik, Rebecca and Markelius, Alva and Pang, Chris and Rajkumar, Nitarshan and Krasheninnikov, Dmitrii and Langosco, Lauro and He, Zhonghao and Duan, Yawen and Carroll, Micah and others},
  booktitle={Proceedings of the 2023 ACM Conference on Fairness, Accountability, and Transparency},
  pages={651--666},
  year={2023}
}


@article{shavit2023practices,
  title={Practices for governing agentic AI systems},
  author={Shavit, Yonadav and Agarwal, Sandhini and Brundage, Miles and Adler, Steven and OKeefe, Cullen and Campbell, Rosie and Lee, Teddy and Mishkin, Pamela and Eloundou, Tyna and Hickey, Alan and others},
  journal={Research Paper, OpenAI, December},
  year={2023}
}

@article{roose2023conversation,
  title={A Conversation With Bing's Chatbot Left Me Deeply Unsettled},
  author={Kevin Roose},
  journal={The New York Times},
  year={2023},
  month=feb,
  url={https://www.nytimes.com/2023/02/16/technology/bing-chatbot-microsoft-chatgpt.html}
}

@article{ruiz2024marshable,
  title={AI chatbots are being used for companionship. What to know before you try it},
  author={Rebecca Ruiz},
  journal={Marshable},
  year={2024},
  month=jun,
  url={https://mashable.com/article/ai-chatbot-companion}
}

@article{sentientGoogle,
  title={The Transcripts of an AI That a Google Engineer Claims Is Sentient Are Pretty Wild},
  author={Victor Tangermann},
  journal={Futurism},
  year={2022},
  month=jun,
  url={https://futurism.com/transcripts-ai-google-engineer-sentient}
}

@article{chatbot-self-awareness,
  title={Researchers say chatbot exhibits self-awareness},
  author={Peter Grad},
  journal={Tech Explore},
  year={2023},
  month=sep,
  url={https://techxplore.com/news/2023-09-chatbot-self-awareness.html}
}

@article{AI-feelings,
  title={This AI says it has feelings. Its wrong. Right?},
  author={Kelsey Piper},
  journal={Vox},
  year={2024},
  month=mar,
  url={https://www.vox.com/future-perfect/2024/3/15/24101088/anthropic-claude-opus-openai-chatgpt-artificial-intelligence-google-consciousness}
}

@article{dijkstra1985anthropomorphism,
  title={On anthropomorphism in science},
  author={Dijkstra, Edsger W},
  journal={EWD936, Sept},
  year={1985}
}
@article{hancock2020ai,
  title={AI-mediated communication: Definition, research agenda, and ethical considerations},
  author={Hancock, Jeffrey T and Naaman, Mor and Levy, Karen},
  journal={Journal of Computer-Mediated Communication},
  volume={25},
  number={1},
  pages={89--100},
  year={2020},
  publisher={Oxford University Press}
}
@book{malm2016fossil,
  title={Fossil capital: The rise of steam power and the roots of global warming},
  author={Malm, Andreas},
  year={2016},
  publisher={Verso books}
}
@article{shneidermandumpty,
	title = {A nonanthropomorphic style guide: overcoming the Humpty Dumpty syndrome},
	journal = {Sparks of innovation in human-computer interaction},
	year = {1993},
	pages = {331 - 331},
	author = {Shneiderman, Ben}
}
@article{lingel2020alexa,
  title={Alexa, tell me about your mother: The history of the secretary and the end of secrecy},
  author={Lingel, Jessa and Crawford, Kate},
  journal={Catalyst: Feminism, Theory, Technoscience},
  volume={6},
  number={1},
  year={2020}
}

@INPROCEEDINGS{Emnett2024-na,
  title     ={Using Robot Social Agency Theory to Understand Robots' Linguistic
               Anthropomorphism},
  author    ={Emnett, Cloe Z and Mott, Terran and Williams, Tom},
  booktitle ={Companion of the 2024 ACM/IEEE International Conference on
               Human-Robot Interaction},
  publisher ={Association for Computing Machinery},
  address   ={New York, NY, USA},
  pages     ={447--452},
  abstract  ={Robots' use of natural language is one of the key factors that
               leads humans to anthropomorphize them. But it is not yet well
               understood what types of language most lead to such
               language-based anthropomorphization (or, Linguistic
               Anthropomorphism). In this paper, we present a brief literature
               survey that suggests six broad categories of linguistic factors
               that lead humans to anthropomorphize robots: autonomy,
               adaptability, directness, politeness, proportionality, and humor.
               By contextualizing these six factors through the lens of Jackson
               and Williams' Theory of Social Agency for Human-Robot
               Interaction, we are able to show how and why these particular
               factors are those responsible for language-based robot
               anthropomorphism.},
  series    ={HRI '24},
  month     =  mar,
  year      =  2024,
  keywords  ={linguistic anthropomorphism, social agency}
}

@article{Roose.2024,
 author  = {Roose, Kevin},
 date    = {2024-10-23},
 title   = {Can A.I. Be Blamed for a Teens Suicide?},
 journal = {The New York Times},
 url     = {https://www.nytimes.com/2024/10/23/technology/characterai-lawsuit-teen-suicide.html},
year={2024},
}

@inproceedings{jones-bergen-2024-gpt,
    title ={Does {GPT}-4 pass the {T}uring test?},
    author ={Jones, Cameron  and
      Bergen, Ben},
    editor ={Duh, Kevin  and
      Gomez, Helena  and
      Bethard, Steven},
    booktitle ={Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)},
    month = jun,
    year ={2024},
    address ={Mexico City, Mexico},
    publisher ={Association for Computational Linguistics},
    url ={https://aclanthology.org/2024.naacl-long.290},
    doi ={10.18653/v1/2024.naacl-long.290},
    pages ={5183--5210},
}
@article{Porra2020-dq,
  title={Can computer based human-likeness endanger humanness? A philosophical and ethical perspective on digital assistants expressing feelings they cant have},
  author={Porra, Jaana and Lacity, Mary and Parks, Michael S},
  journal={Information Systems Frontiers},
  volume={22},
  pages={533--547},
  year={2020},
  publisher={Springer}
}

@ARTICLE{Chien2024-vl,
  title         ={Beyond Behaviorist Representational Harms: A Plan for
                   Measurement and Mitigation},
  author        ={Chien, Jennifer and Danks, David},
  journal       ={arXiv [cs.CY]},
  abstract      ={Algorithmic harms are commonly categorized as either
                   allocative or representational. This study specifically
                   addresses the latter, focusing on an examination of current
                   definitions of representational harms to discern what is
                   included and what is not. This analysis motivates our
                   expansion beyond behavioral definitions to encompass harms to
                   cognitive and affective states. The paper outlines high-level
                   requirements for measurement: identifying the necessary
                   expertise to implement this approach and illustrating it
                   through a case study. Our work highlights the unique
                   vulnerabilities of large language models to perpetrating
                   representational harms, particularly when these harms go
                   unmeasured and unmitigated. The work concludes by presenting
                   proposed mitigations and delineating when to employ them. The
                   overarching aim of this research is to establish a framework
                   for broadening the definition of representational harms and
                   to translate insights from fairness research into practical
                   measurement and mitigation praxis.},
  month         =  jan,
  year          =  2024,
  archivePrefix ={arXiv},
  primaryClass  ={cs.CY}
}
@article{payne.2024,
 author  = {Payne, Kate},
 date    = {2024-10-25},
year={2024},
 title   = {An AI chatbot pushed a teen to kill himself, a lawsuit against its creator alleges
},
 journal = {The Associated Press},
 url     = {https://apnews.com/article/chatbot-ai-lawsuit-suicide-teen-artificial-intelligence-9d48adc572100822fdbc3c90d1456bd0},
 urldate = {2024-10-26}
}
@article{khattab2023dspy,
  title={Dspy: Compiling declarative language model calls into self-improving pipelines},
  author={Khattab, Omar and Singhvi, Arnav and Maheshwari, Paridhi and Zhang, Zhiyuan and Santhanam, Keshav and Vardhamanan, Sri and Haq, Saiful and Sharma, Ashutosh and Joshi, Thomas T and Moazam, Hanna and others},
  journal={arXiv preprint arXiv:2310.03714},
  year={2023}
}
@article{zhang2022robustness,
  title={Robustness of demonstration-based learning under limited data scenario},
  author={Zhang, Hongxin and Zhang, Yanzhe and Zhang, Ruiyi and Yang, Diyi},
  journal={arXiv preprint arXiv:2210.10693},
  year={2022}
}
@article{zhi2024beyond,
  title={Beyond Preferences in AI Alignment},
  author={Zhi-Xuan, Tan and Carroll, Micah and Franklin, Matija and Ashton, Hal},
  journal={Philosophical Studies},
  pages={1--51},
  year={2024},
  publisher={Springer}
}
@book{berkeley1949giant,
  title={Giant brains; or, Machines that think},
  author={Berkeley, Edmund Callis},
  year={1949},
  publisher={Wiley}
}
@article{hewitt2024instruction,
  title={Instruction following without instruction tuning},
  author={Hewitt, John and Liu, Nelson F and Liang, Percy and Manning, Christopher D},
  journal={arXiv preprint arXiv:2409.14254},
  year={2024}
}
@article{gros2022robots,
  title={Robots-dont-cry: understanding falsely anthropomorphic utterances in dialog systems},
  author={Gros, David and Li, Yu and Yu, Zhou},
  journal={arXiv preprint arXiv:2210.12429},
  year={2022}
}
@article{west2023comparing,
  title={Comparing Google Bard with OpenAIs ChatGPT on political bias, facts, and morality},
  author={West, Darrell M},
  year={2023},
  journal={The Brookings Institution}
}
@book{lakoff2008metaphors,
  title={Metaphors we live by},
  author={Lakoff, George and Johnson, Mark},
  year={2008},
  publisher={University of Chicago press}
}
@article{laestadius2022too,
  title={Too human and not human enough: A grounded theory analysis of mental health harms from emotional dependence on the social chatbot Replika},
  author={Laestadius, Linnea and Bishop, Andrea and Gonzalez, Michael and Illencik, Diana and Campos-Castillo, Celeste},
  journal={New Media & Society},
  pages={14614448221142007},
  year={2022},
  publisher={SAGE Publications Sage UK: London, England}
}

@inproceedings{Contro2024-dr,
  title={Interaction Minimalism: Minimizing HRI to Reduce Emotional Dependency on Robots},
  author={Contro, Jack and Brandao, Martim},
  booktitle={Robophilosophy Conference 2024},
  year={2024}
}

@ARTICLE{Shteynberg2024-cg,
  title     ={Does it matter if empathic {AI} has no empathy?},
  author    ={Shteynberg, Garriy and Halpern, Jodi and Sadovnik, Amir and
               Garthoff, Jon and Perry, Anat and Hay, Jessica and Montemayor,
               Carlos and Olson, Michael A and Hulsey, Tim L and Fairweather,
               Abrol},
  journal   ={Nat. Mach. Intell.},
  publisher ={Springer Science and Business Media LLC},
  volume    =  6,
  number    =  5,
  pages     ={496--497},
  month     =  may,
  year      =  2024,
  language  ={en}
}


@INPROCEEDINGS{Maeda2024-cv,
  title     ={When Human-{AI} Interactions Become Parasocial: Agency and
               Anthropomorphism in Affective Design},
  author    ={Maeda, Takuya and Quan-Haase, Anabel},
  booktitle ={Proceedings of the 2024 ACM Conference on Fairness,
               Accountability, and Transparency},
  publisher ={Association for Computing Machinery},
  address   ={New York, NY, USA},
  pages     ={1068--1077},
  abstract  ={With the continuous improvement of large language models (LLMs),
               chatbots can produce coherent and continuous word sequences that
               mirror natural human language. While the use of natural language
               and human-like conversation styles enables the use of chatbots
               within a range of everyday settings, these usability-enhancing
               features can also have unintended consequences, such as making
               fallible information seem trustworthy by emphasizing friendliness
               and closeness. This can have serious implications for information
               retrieval tasks performed with chatbots. In this paper, we
               provide an overview of the literature on parasociality, social
               affordance, and trust to bridge these concepts within human-AI
               interactions. We critically examine how chatbot roleplaying and
               user role projection co-produce a pseudo-interactive,
               technologically-mediated space with imbalanced dynamics between
               users and chatbots. Based on the review of the literature, we
               develop a conceptual framework of parasociality in chatbots that
               describes interactions between humans and anthropomorphized
               chatbots. We dissect how chatbots use personal pronouns,
               conversational conventions, affirmations, and similar strategies
               to position the chatbots as users companions or assistants, and
               how these tactics induce trust-forming behaviors in users.
               Finally, based on the conceptual framework, we outline a set of
               ethical concerns that emerge from parasociality, including
               illusions of reciprocal engagement, task misalignment, and leaks
               of sensitive information. This paper argues that these possible
               consequences arise from a positive feedback cycle wherein
               anthropomorphized chatbot features encourage users to fill in the
               context around predictive outcomes.},
  series    ={FAccT '24},
  month     =  jun,
  year      =  2024,
  keywords  ={anthropomorphism, chatbots, design, ethics, human-AI
               interactions, parasociality, trust}
}

@article{madianou2021nonhuman,
  title={Nonhuman humanitarianism: when'AI for good'can be harmful},
  author={Madianou, Mirca},
  journal={Information, Communication & Society},
  volume={24},
  number={6},
  pages={850--868},
  year={2021},
  publisher={Taylor & Francis}
}
@inproceedings{akbulut2024all,
  title={All Too Human? Mapping and Mitigating the Risk from Anthropomorphic AI},
  author={Akbulut, Canfer and Weidinger, Laura and Manzini, Arianna and Gabriel, Iason and Rieser, Verena},
  booktitle={Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society},
  volume={7},
  pages={13--26},
  year={2024}
}
@inproceedings{cheng-etal-2024-anthroscore,
    title ={{A}nthro{S}core: A Computational Linguistic Measure of Anthropomorphism},
    author ={Cheng, Myra  and
      Gligoric, Kristina  and
      Piccardi, Tiziano  and
      Jurafsky, Dan},
    editor ={Graham, Yvette  and
      Purver, Matthew},
    booktitle ={Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics (Volume 1: Long Papers)},
    month = mar,
    year ={2024},
    address ={St. Julian{'}s, Malta},
    publisher ={Association for Computational Linguistics},
    url ={https://aclanthology.org/2024.eacl-long.49},
    pages ={807--825},
    abstract ={Anthropomorphism, or the attribution of human-like characteristics to non-human entities, has shaped conversations about the impacts and possibilities of technology. We present AnthroScore, an automatic metric of implicit anthropomorphism in language. We use a masked language model to quantify how non-human entities are implicitly framed as human by the surrounding context. We show that AnthroScore corresponds with human judgments of anthropomorphism and dimensions of anthropomorphism described in social science literature. Motivated by concerns of misleading anthropomorphism in computer science discourse, we use AnthroScore to analyze 15 years of research papers and downstream news articles. In research papers, we find that anthropomorphism has steadily increased over time, and that papers related to language models have the most anthropomorphism. Within ACL papers, temporal increases in anthropomorphism are correlated with key neural advancements. Building upon concerns of scientific misinformation in mass media, we identify higher levels of anthropomorphism in news headlines compared to the research papers they cite. Since AnthroScore is lexicon-free, it can be directly applied to a wide range of text sources.},
}

@incollection{friedman2007human,
  title={Human values, ethics, and design},
  author={Friedman, Batya and Kahn Jr, Peter H},
  booktitle={The human-computer interaction handbook},
  pages={1267--1292},
  year={2007},
  publisher={CRC press}
}

@inproceedings{el2024transparent,
  title={Transparent AI Disclosure Obligations: Who, What, When, Where, Why, How},
  author={El Ali, Abdallah and Venkatraj, Karthikeya Puttur and Morosoli, Sophie and Naudts, Laurens and Helberger, Natali and Cesar, Pablo},
  booktitle={Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},
  pages={1--11},
  year={2024}
}

@inproceedings{mozafari2020chatbot,
  title={The Chatbot Disclosure Dilemma: Desirable and Undesirable Effects of Disclosing the Non-Human Identity of Chatbots.},
  author={Mozafari, Nika and Weiger, Welf H and Hammerschmidt, Maik},
  booktitle={ICIS},
  pages={1--18},
  year={2020}
}


@article{bennett2020point,
  title={What is the point of fairness? Disability, AI and the complexity of justice},
  author={Bennett, Cynthia L and Keyes, Os},
  journal={ACM SIGACCESS accessibility and computing},
  pages={1--1},
  year={2020},
  publisher={ACM New York, NY, USA}
}

@article{olteanu2023responsible,
  title={Responsible AI Research Needs Impact Statements Too},
  author={Olteanu, Alexandra and Ekstrand, Michael and Castillo, Carlos and Suh, Jina},
  journal={arXiv preprint arXiv:2311.11776},
  year={2023}
}
@article{weinberg2022rethinking,
  title={Rethinking fairness: An interdisciplinary survey of critiques of hegemonic ML fairness approaches},
  author={Weinberg, Lindsay},
  journal={Journal of Artificial Intelligence Research},
  volume={74},
  pages={75--109},
  year={2022}
}

@article{van2024artificial,
  title={Artificial Intelligence: Panacea or Non-intentional Dehumanisation?},
  author={van der Gun, Luc and Guest, Olivia},
  journal={Journal of Human-Technology Relations},
  volume={2},
  year={2024}
}


@inproceedings{widder2022limits,
  title={Limits and possibilities for Ethical AI in open source: A study of deepfakes},
  author={Widder, David Gray and Nafus, Dawn and Dabbish, Laura and Herbsleb, James},
  booktitle={Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency},
  pages={2035--2046},
  year={2022}
}

@article{aizenberg2020designing,
  title={Designing for human rights in AI},
  author={Aizenberg, Evgeni and Van Den Hoven, Jeroen},
  journal={Big Data & Society},
  volume={7},
  number={2},
  pages={2053951720949566},
  year={2020},
  publisher={SAGE Publications Sage UK: London, England}
}

@inproceedings{fink2012anthropomorphism,
  title={Anthropomorphism and human likeness in the design of robots and human-robot interaction},
  author={Fink, Julia},
  booktitle={Social Robotics: 4th International Conference, ICSR 2012, Chengdu, China, October 29-31, 2012. Proceedings 4},
  pages={199--208},
  year={2012},
  organization={Springer}
}

@article{rehak2021language,
  title={The language labyrinth: Constructive critique on the terminology used in the AI discourse},
  author={Rehak, Rainer},
  journal={AI for Everyone},
  pages={87--102},
  year={2021}
}

@inproceedings{whitney2024real,
  title={Real Risks of Fake Data: Synthetic Data, Diversity-Washing and Consent Circumvention},
  author={Whitney, Cedric Deslandes and Norman, Justin},
  booktitle={The 2024 ACM Conference on Fairness, Accountability, and Transparency},
  pages={1733--1744},
  year={2024}
}

@article{bariach2024towards,
  title={Towards a Harms Taxonomy of AI Likeness Generation},
  author={Bariach, Ben and Hogan, Bernie and McBride, Keegan},
  journal={arXiv preprint arXiv:2407.12030},
  year={2024}
}

@book{winograd1986understanding,
  title={Understanding computers and cognition: A new foundation for design},
  author={Winograd, Terry and Flores, Fernando},
  volume={335},
  year={1986},
  publisher={Ablex publishing corporation Norwood, NJ}
}
@article{kirk2024prism,
  title={The PRISM Alignment Project: What Participatory, Representative and Individualised Human Feedback Reveals About the Subjective and Multicultural Alignment of Large Language Models},
  author={Kirk, Hannah Rose and Whitefield, Alexander and R{\o}ttger, Paul and Bean, Andrew and Margatina, Katerina and Ciro, Juan and Mosquera, Rafael and Bartolo, Max and Williams, Adina and He, He and others},
  journal={arXiv preprint arXiv:2404.16019},
  year={2024}
}
@article{glaese2022improving,
  title={Improving alignment of dialogue agents via targeted human judgements},
  author={Glaese, Amelia and McAleese, Nat and Tr{\k{e}}bacz, Maja and Aslanides, John and Firoiu, Vlad and Ewalds, Timo and Rauh, Maribeth and Weidinger, Laura and Chadwick, Martin and Thacker, Phoebe and others},
  journal={arXiv preprint arXiv:2209.14375},
  year={2022}
}

@incollection{lang2013computers,
  title={Are computers still social actors?},
  author={Lang, Helmut and Klepsch, Melina and Nothdurft, Florian and Seufert, Tina and Minker, Wolfgang},
  booktitle={CHI'13 extended abstracts on human factors in computing systems},
  pages={859--864},
  year={2013},
 publisher={Association for Computing Machinery}
}
@article{heyselaar2023casa,
  title={The CASA theory no longer applies to desktop computers},
  author={Heyselaar, Evelien},
  journal={Scientific Reports},
  volume={13},
  number={1},
  pages={19693},
  year={2023},
  publisher={Nature Publishing Group UK London}
}
@article{folk2023cultural,
  title={Cultural variation in attitudes towards social chatbots},
  author={Folk, Dunigan and Wu, Chenxi and Heine, Steven},
  year={2023},
  publisher={PsyArXiv}
}
@article{wynter2003unsettling,
  title={Unsettling the coloniality of being/power/truth/freedom: Towards the human, after man, its overrepresentationAn argument},
  author={Wynter, Sylvia},
  journal={CR: The new centennial review},
  volume={3},
  number={3},
  pages={257--337},
  year={2003},
  publisher={JSTOR}
}
@article{Zarouali2021-gy,
  title={Overcoming polarization with chatbot news? Investigating the impact of news content containing opposing views on agreement and credibility},
  author={Zarouali, Brahim and Makhortykh, Mykola and Bastian, Mariella and Araujo, Theo},
  journal={European journal of communication},
  volume={36},
  number={1},
  pages={53--68},
  year={2021},
  publisher={Sage Publications Sage UK: London, England}
}
@article{Watson2019-py,
  title={The rhetoric and reality of anthropomorphism in artificial intelligence},
  author={Watson, David},
  journal={Minds and Machines},
  volume={29},
  number={3},
  pages={417--440},
  year={2019},
  publisher={Springer}
}
@inproceedings{maeda2024human,
  title={When Human-AI Interactions Become Parasocial: Agency and Anthropomorphism in Affective Design},
  author={Maeda, Takuya and Quan-Haase, Anabel},
  booktitle={The 2024 ACM Conference on Fairness, Accountability, and Transparency},
  pages={1068--1077},
  year={2024}
}
@article{cotra2021ai,
  title={Why AI alignment could be hard with modern deep learning},
  author={Cotra, Ajeya},
  journal={Cold Takes},
  year={2021}
}
@article{sharma2023towards,
  title={Towards understanding sycophancy in language models},
  author={Sharma, Mrinank and Tong, Meg and Korbak, Tomasz and Duvenaud, David and Askell, Amanda and Bowman, Samuel R and Cheng, Newton and Durmus, Esin and Hatfield-Dodds, Zac and Johnston, Scott R and others},
  journal={arXiv preprint arXiv:2310.13548},
  year={2023}
}
@article{coates1998language,
  title={Language and gender},
  author={Coates, Jennifer and Pichler, Pia},
  journal={A Reader (2nd ed.) Oxford/Malden: Wiley},
  year={1998}
}
@inproceedings{kim2024m,
  title={ I'm Not Sure, But...: Examining the Impact of Large Language Models' Uncertainty Expression on User Reliance and Trust},
  author={Kim, Sunnie SY and Liao, Q Vera and Vorvoreanu, Mihaela and Ballard, Stephanie and Vaughan, Jennifer Wortman},
  booktitle={The 2024 ACM Conference on Fairness, Accountability, and Transparency},
  pages={822--835},
  year={2024}
}
@article{sui2024confabulation,
  title={Confabulation: The Surprising Value of Large Language Model Hallucinations},
  author={Sui, Peiqi and Duede, Eamon and Wu, Sophie and So, Richard Jean},
  journal={arXiv preprint arXiv:2406.04175},
  year={2024}
}
@article{perez2022discovering,
  title={Discovering language model behaviors with model-written evaluations},
  author={Perez, Ethan and Ringer, Sam and Luko{\v{s}}i{\=u}t{\.e}, Kamil{\.e} and Nguyen, Karina and Chen, Edwin and Heiner, Scott and Pettit, Craig and Olsson, Catherine and Kundu, Sandipan and Kadavath, Saurav and others},
  journal={arXiv preprint arXiv:2212.09251},
  year={2022}
}
@article{ibrahim2024characterizing,
  title={Characterizing and modeling harms from interactions with design patterns in AI interfaces},
  author={Ibrahim, Lujain and Rocher, Luc and Valdivia, Ana},
  journal={arXiv preprint arXiv:2404.11370},
  year={2024}
}
@inproceedings{nass1994computers,
  title={Computers are social actors},
  author={Nass, Clifford and Steuer, Jonathan and Tauber, Ellen R},
  booktitle={Proceedings of the SIGCHI conference on Human factors in computing systems},
  pages={72--78},
  year={1994}
}
@article{reeves1996media,
  title={The media equation: How people treat computers, television, and new media like real people},
  author={Reeves, Byron and Nass, Clifford},
  journal={Cambridge, UK},
  volume={10},
  number={10},
  pages={19--36},
  year={1996}
}
@article{gabriel2024ethics,
  title={The ethics of advanced AI assistants},
  author={Gabriel, Iason and Manzini, Arianna and Keeling, Geoff and Hendricks, Lisa Anne and Rieser, Verena and Iqbal, Hasan and Toma{\v{s}}ev, Nenad and Ktena, Ira and Kenton, Zachary and Rodriguez, Mikel and others},
  journal={arXiv preprint arXiv:2404.16244},
  year={2024}
}
@inproceedings{jakesch2022different,
  title={How different groups prioritize ethical values for responsible AI},
  author={Jakesch, Maurice and Bu{\c{c}}inca, Zana and Amershi, Saleema and Olteanu, Alexandra},
  booktitle={Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency},
  pages={310--323},
  year={2022}
}

@inproceedings{mcilroy2022mimetic,
  title={Mimetic models: Ethical implications of AI that acts like you},
  author={McIlroy-Young, Reid and Kleinberg, Jon and Sen, Siddhartha and Barocas, Solon and Anderson, Ashton},
  booktitle={Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society},
  pages={479--490},
  year={2022}
}

@inproceedings{agnew2024illusion,
  title={The illusion of artificial inclusion},
  author={Agnew, William and Bergman, A Stevie and Chien, Jennifer and D{\'\i}az, Mark and El-Sayed, Seliem and Pittman, Jaylen and Mohamed, Shakir and McKee, Kevin R},
  booktitle={Proceedings of the CHI Conference on Human Factors in Computing Systems},
  pages={1--12},
  year={2024}
}

@article{haque2024information,
  title={Information Seeking Using AI Assistants},
  author={Haque, Ebtesam Al and Brown, Chris and LaToza, Thomas D and Johnson, Brittany},
  journal={arXiv preprint arXiv:2408.04032},
  year={2024}
}
@article{tan2024far,
  title={How far are AI-powered programming assistants from meeting developers' needs?},
  author={Tan, Xin and Long, Xiao and Ni, Xianjun and Zhu, Yinghao and Jiang, Jing and Zhang, Li},
  journal={arXiv preprint arXiv:2404.12000},
  year={2024}
}
@article{tjuatja2024llms,
  title={Do LLMs exhibit human-like response biases? A case study in survey design},
  author={Tjuatja, Lindia and Chen, Valerie and Wu, Tongshuang and Talwalkwar, Ameet and Neubig, Graham},
  journal={Transactions of the Association for Computational Linguistics},
  volume={12},
  pages={1011--1026},
  year={2024},
  publisher={MIT Press 255 Main Street, 9th Floor, Cambridge, Massachusetts 02142, USA~}
}

@inproceedings{bender2021dangers,
  title={On the dangers of stochastic parrots: Can language models be too big?},
  author={Bender, Emily M and Gebru, Timnit and McMillan-Major, Angelina and Shmitchell, Shmargaret},
  booktitle={Proceedings of the 2021 ACM conference on fairness, accountability, and transparency},
  pages={610--623},
  year={2021}
}

@article{abercrombie2023mirages,
  title={Mirages: On anthropomorphism in dialogue systems},
  author={Abercrombie, Gavin and Curry, Amanda Cercas and Dinkar, Tanvi and Rieser, Verena and Talat, Zeerak},
  journal={arXiv preprint arXiv:2305.09800},
  year={2023}
}

@article{rosner2021ethics,
  title={The ethics of a deepfake Anthony Bourdain voice},
  author={Rosner, Helen},
  journal={New Yorker},
  volume={17},
  year={2021}
}

@article{vaccari2020deepfakes,
  title={Deepfakes and disinformation: Exploring the impact of synthetic political video on deception, uncertainty, and trust in news},
  author={Vaccari, Cristian and Chadwick, Andrew},
  journal={Social media+ society},
  volume={6},
  number={1},
  pages={2056305120903408},
  year={2020},
  publisher={SAGE Publications Sage UK: London, England}
}

@article{AI-romantic-partner,
  title={Replika lets you buy an AI girlfriend or boyfriend},
  author={Altchek, Ana},
  journal={Business Insider},
  year={2024}
}

@article{ChatGPT-human,
  title={OpenAIs Big Reset},
  author={Wong, Matteo},
  journal={The Atlantic},
  year={2024}
}


@incollection{brynjolfsson2023turing,
  title={The turing trap: The promise & peril of human-like artificial intelligence},
  author={Brynjolfsson, Erik},
  booktitle={Augmented education in the global age},
  pages={103--116},
  year={2023},
  publisher={Routledge}
}

@article{friedman1992human,
  title={Human agency and responsible computing: Implications for computer system design},
  author={Friedman, Batya and Kahn Jr, Peter H},
  journal={Journal of Systems and Software},
  volume={17},
  number={1},
  pages={7--14},
  year={1992},
  publisher={Elsevier}
}

@inproceedings{chan2023harms,
  title={Harms from increasingly agentic algorithmic systems},
  author={Chan, Alan and Salganik, Rebecca and Markelius, Alva and Pang, Chris and Rajkumar, Nitarshan and Krasheninnikov, Dmitrii and Langosco, Lauro and He, Zhonghao and Duan, Yawen and Carroll, Micah and others},
  booktitle={Proceedings of the 2023 ACM Conference on Fairness, Accountability, and Transparency},
  pages={651--666},
  year={2023}
}


@article{shavit2023practices,
  title={Practices for governing agentic AI systems},
  author={Shavit, Yonadav and Agarwal, Sandhini and Brundage, Miles and Adler, Steven and OKeefe, Cullen and Campbell, Rosie and Lee, Teddy and Mishkin, Pamela and Eloundou, Tyna and Hickey, Alan and others},
  journal={Research Paper, OpenAI, December},
  year={2023}
}

@article{roose2023conversation,
  title={A Conversation With Bing's Chatbot Left Me Deeply Unsettled},
  author={Kevin Roose},
  journal={The New York Times},
  year={2023},
  month=feb,
  url={https://www.nytimes.com/2023/02/16/technology/bing-chatbot-microsoft-chatgpt.html}
}

@article{ruiz2024marshable,
  title={AI chatbots are being used for companionship. What to know before you try it},
  author={Rebecca Ruiz},
  journal={Marshable},
  year={2024},
  month=jun,
  url={https://mashable.com/article/ai-chatbot-companion}
}

@article{sentientGoogle,
  title={The Transcripts of an AI That a Google Engineer Claims Is Sentient Are Pretty Wild},
  author={Victor Tangermann},
  journal={Futurism},
  year={2022},
  month=jun,
  url={https://futurism.com/transcripts-ai-google-engineer-sentient}
}

@article{chatbot-self-awareness,
  title={Researchers say chatbot exhibits self-awareness},
  author={Peter Grad},
  journal={Tech Explore},
  year={2023},
  month=sep,
  url={https://techxplore.com/news/2023-09-chatbot-self-awareness.html}
}

@article{AI-feelings,
  title={This AI says it has feelings. Its wrong. Right?},
  author={Kelsey Piper},
  journal={Vox},
  year={2024},
  month=mar,
  url={https://www.vox.com/future-perfect/2024/3/15/24101088/anthropic-claude-opus-openai-chatgpt-artificial-intelligence-google-consciousness}
}


@article{AI-self-awarness,
  title={Claude 3 Opus has stunned AI researchers with its intellect and 'self-awareness'  does this mean it can think for itself?},
  author={Roland Moore-Colyer},
  journal={Live Science},
  year={2024},
  month=apr,
  url={https://www.livescience.com/technology/artificial-intelligence/anthropic-claude-3-opus-stunned-ai-researchers-self-awareness-does-this-mean-it-can-think-for-itself}
}

@article{google-disclosure,
  title={Google's creepy AI phone call feature will disclose it's a robot, after backlash},
  author={Johnny Lieu},
  journal={Marshable},
  year={2018},
  month=may,
  url={https://mashable.com/article/google-duplex-disclosures-robot}
}

@article{abercrombie2021alexa,
  title={Alexa, Google, Siri: What are your pronouns? Gender and anthropomorphism in the design and perception of conversational assistants},
  author={Abercrombie, Gavin and Curry, Amanda Cercas and Pandya, Mugdha and Rieser, Verena},
  journal={arXiv preprint arXiv:2106.02578},
  year={2021}
}

@article{brandtzaeg2022my,
  title={My AI friend: How users of a social chatbot understand their human--AI friendship},
  author={Brandtzaeg, Petter Bae and Skjuve, Marita and Folstad, Asbjorn},
  journal={Human Communication Research},
  volume={48},
  number={3},
  pages={404--429},
  year={2022},
  publisher={Oxford University Press}
}

@misc{pizzatweet,
  title={Gemini loves the smell of pizza. That's how we can tell Gemini is human},
  author={Wyatt Walls},
  year={2024},
  month=mar,
  url={https://x.com/lefthanddraft/status/1772693138714362021}
}

@misc{haschildtweet,
  title={Meta AI claims to have a child in a NYC public school},
  author={Aleksandra Korolova},
  year={2024},
  month=apr,
  url={https://x.com/korolova/status/1780450925028548821}
}

@misc{PiClaimsToBeChatGPT,
  title={Why is Pi claiming to be ChatGPT?},
  author={salvationpumpfake},
  year={2023},
  url={{https://www.reddit.com/r/ChatGPT/comments/17c0wxb/why_is_pi_claiming_to_be_chatgpt}},
  note={Reddit post}
}


@inproceedings{park2023generative,
  title={Generative agents: Interactive simulacra of human behavior},
  author={Park, Joon Sung and O'Brien, Joseph and Cai, Carrie Jun and Morris, Meredith Ringel and Liang, Percy and Bernstein, Michael S},
  booktitle={Proceedings of the 36th annual acm symposium on user interface software and technology},
  pages={1--22},
  year={2023}
}

@article{shanahan2023role,
  title={Role play with large language models},
  author={Shanahan, Murray and McDonell, Kyle and Reynolds, Laria},
  journal={Nature},
  volume={623},
  number={7987},
  pages={493--498},
  year={2023},
  publisher={Nature Publishing Group UK London}
}

@inproceedings{park2022social,
  title={Social simulacra: Creating populated prototypes for social computing systems},
  author={Park, Joon Sung and Popowski, Lindsay and Cai, Carrie and Morris, Meredith Ringel and Liang, Percy and Bernstein, Michael S},
  booktitle={Proceedings of the 35th Annual ACM Symposium on User Interface Software and Technology},
  pages={1--18},
  year={2022}
}
@book{quintanar1982interactive,
  title={The interactive computer as a social stimulus in computer-managed instruction: a theoretical and empirical analysis of the social psychological processes evoked during human-computer interaction},
  author={Quintanar, Leo Raul},
  year={1982},
  publisher={University of Notre Dame}
}
@article{decosmo2022google,
  title={Google Engineer Claims AI Chatbot Is Sentient: Why That Matters},
  author={Leonardo De Cosmo},
  journal={Scientific American},
  year={2022},
  month=jul,
  url={https://www.scientificamerican.com/article/google-engineer-claims-ai-chatbot-is-sentient-why-that-matters/}
}

@article{fiesler2024ai,
  title={AI Chatbots Are Intruding Into Online Communities},
  author={Casey Fiesler},
  journal={Discover Magazine},
  year={2024},
  month=may,
  url={https://www.discovermagazine.com/technology/ai-chatbots-are-intruding-into-online-communities}
}

@inproceedings{rogers2024position,
  title={Position: Key Claims in LLM Research Have a Long Tail of Footnotes},
  author={Rogers, Anna and Luccioni, Sasha},
  booktitle={Forty-first International Conference on Machine Learning},
  year={2024}
}

@book{benjamin2019race,
  title={Race after technology: Abolitionist tools for the new Jim code},
  author={Benjamin, Ruha},
  year={2019},
  publisher={John Wiley & Sons}
}

@inproceedings{jacobs_measurement_2021,
	address = {New York, NY, USA},
	series = {{FAccT}},
	title = {Measurement and {Fairness}},
	isbn = {978-1-4503-8309-7},
	url = {https://dl.acm.org/doi/10.1145/3442188.3445901},
	doi = {10.1145/3442188.3445901},
	urldate = {2023-04-06},
	booktitle = {Proceedings of the 2021 {ACM} {Conference} on {Fairness}, {Accountability}, and {Transparency}},
	publisher = {Association for Computing Machinery},
	author = {Jacobs, Abigail Z. and Wallach, Hanna},
	month = mar,
	year = {2021},
	pages = {375--385}
}

@inproceedings{blodgett-etal-2021-stereotyping,
    title ={Stereotyping {N}orwegian Salmon: An Inventory of Pitfalls in Fairness Benchmark Datasets},
    author ={Blodgett, Su Lin  and
      Lopez, Gilsinia  and
      Olteanu, Alexandra  and
      Sim, Robert  and
      Wallach, Hanna},
    editor ={Zong, Chengqing  and
      Xia, Fei  and
      Li, Wenjie  and
      Navigli, Roberto},
    booktitle ={Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)},
    month = aug,
    year ={2021},
    address ={Online},
    publisher ={Association for Computational Linguistics},
    url ={https://aclanthology.org/2021.acl-long.81},
    doi ={10.18653/v1/2021.acl-long.81},
    pages ={1004--1015}
}

@article{hoffmann2019fairness,
  title={Where fairness fails: data, algorithms, and the limits of antidiscrimination discourse},
  author={Hoffmann, Anna Lauren},
  journal={Information, Communication & Society},
  volume={22},
  number={7},
  pages={900--915},
  year={2019},
  publisher={Taylor & Francis}
}

@book{barocas-hardt-narayanan,
  title = {Fairness and Machine Learning: Limitations and Opportunities},
  author = {Solon Barocas and Moritz Hardt and Arvind Narayanan},
  publisher = {MIT Press},
  year = {2023}
}

@inproceedings{keyes2019mulching,
  title={A mulching proposal: Analysing and improving an algorithmic system for turning the elderly into high-nutrient slurry},
  author={Keyes, Os and Hutson, Jevan and Durbin, Meredith},
  booktitle={Extended abstracts of the 2019 CHI conference on human factors in computing systems},
  pages={1--11},
  year={2019}
}

@misc{crawford2017neurips,
  title={The Trouble with Bias},
  author={Crawford, Kate},
  year={2017},
  note={NeurIPS Keynote}
}

@article{roach2023want,
  title={`I want to be human.' My intense, unnerving chat with Microsoft's AI chatbot},
  author={Jacob Roach},
  journal={Digital Trends},
  year={2023},
  month=feb,
  url={https://www.digitaltrends.com/computing/chatgpt-bing-hands-on/}
}

@article{piper2024this,
  title={This AI says it has feelings. Its wrong. Right?},
  author={Kelsey Piper},
  journal={Vox},
  year={2024},
  month=mar,
  url={https://www.vox.com/future-perfect/2024/3/15/24101088/anthropic-claude-opus-openai-chatgpt-artificial-intelligence-google-consciousness}
}

@inproceedings{inie2024from,
author = {Inie, Nanna and Druga, Stefania and Zukerman, Peter and Bender, Emily M.},
title = {From AI to Probabilistic Automation: How Does Anthropomorphization of Technical Systems Descriptions Influence Trust?},
year = {2024},
isbn = {9798400704505},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3630106.3659040},
doi = {10.1145/3630106.3659040},
abstract = {In this paper we investigate how peoples level of trust (as reported through self-assessment) in so-called AI (artificial intelligence) is influenced by anthropomorphizing language in system descriptions. Building on prior work, we define four categories of anthropomorphization (1. Properties of a cognizer, 2. Agency, 3. Biological metaphors, and 4. Properties of a communicator). We use a survey-based approach (n=954) to investigate whether participants are likely to trust one of two (fictitious) AI systems by randomly assigning people to see either an anthropomorphized or a de-anthropomorphized description of the systems. We find that participants are no more likely to trust anthropomorphized over de-anthropmorphized product descriptions overall. The type of product or system in combination with different anthropomorphic categories appears to exert greater influence on trust than anthropomorphizing language alone, and age is the only demographic factor that significantly correlates with peoples preference for anthropomorphized or de-anthropomorphized descriptions. When elaborating on their choices, participants highlight factors such as lesser of two evils, lower or higher stakes contexts, and human favoritism as driving motivations when choosing between product A and B, irrespective of whether they saw an anthropomorphized or a de-anthropomorphized description of the product. Our results suggest that anthropomorphism in AI descriptions is an aggregate concept that may influence different groups differently, and provide nuance to the discussion of whether anthropomorphization leads to higher trust and over-reliance by the general public in systems sold as AI.},
booktitle = {Proceedings of the 2024 ACM Conference on Fairness, Accountability, and Transparency},
pages = {23222347},
numpages = {26},
keywords = {AI, anthropomorphism, probabilistic automation, semantics, trust},
location = {Rio de Janeiro, Brazil},
series = {FAccT '24}
}

