@misc{alammar2018illustrated,
  author       = {Jay Alammar},
  title        = {The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning)},
  year         = {2018},
  url          = {https://jalammar.github.io/illustrated-bert/}
}

@article{banse2024federated,
  author       = {Adrien Banse and Jan Kreischer and Xavier Oliva i Jürgens},
  title        = {Federated Learning with Differential Privacy},
  year         = {2024},
  journal      = {arXiv preprint},
  url          = {https://arxiv.org/abs/2402.02230}
}

@article{bonawitz2016secure,
  author       = {Keith Bonawitz et al.},
  title        = {Practical Secure Aggregation for Federated Learning on User-Held Data},
  year         = {2016},
  journal      = {arXiv preprint},
  url          = {https://arxiv.org/abs/1611.04482}
}

@misc{byrnes2022agi,
  author       = {Steven Byrnes},
  title        = {My work on Artificial General Intelligence (AGI) safety},
  year         = {2022},
  url          = {https://sjbyrnes.com/agi.html}
}

@article{devlin2018bert,
  author       = {Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
  title        = {BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  year         = {2018},
  journal      = {arXiv preprint},
  url          = {https://arxiv.org/abs/1810.04805}
}

@article{dosovitskiy2021image,
  author       = {Alexey Dosovitskiy et al.},
  title        = {An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
  year         = {2021},
  journal      = {arXiv preprint},
  url          = {https://arxiv.org/abs/2010.11929}
}

@article{hu2022standard,
  author       = {Zhiting Hu and Eric P. Xing},
  title        = {Toward a 'Standard Model' of Machine Learning},
  year         = {2022},
  journal      = {Harvard Data Science Review},
  volume       = {4},
  number       = {4},
  url          = {https://hdsr.mitpress.mit.edu/pub/zkib7xth/release/2}
}

@article{jiang2019biggestlosers,
  author       = {Angela H. Jiang et al.},
  title        = {Accelerating Deep Learning by Focusing on the Biggest Losers},
  year         = {2019},
  journal      = {arXiv preprint},
  url          = {https://arxiv.org/abs/1910.00762}
}

@article{kairouz2021federated,
  author       = {Peter Kairouz et al.},
  title        = {Advances and Open Problems in Federated Learning},
  year         = {2021},
  journal      = {arXiv preprint},
  url          = {https://arxiv.org/abs/1912.04977}
}

@inproceedings{katharopoulos2018importance,
  author       = {Angelos Katharopoulos and Francois Fleuret},
  title        = {Not All Samples Are Created Equal: Deep Learning with Importance Sampling},
  year         = {2018},
  booktitle    = {Proceedings of Machine Learning Research},
  volume       = {80},
  pages        = {2525--2534},
  url          = {https://proceedings.mlr.press/v80/katharopoulos18a.html}
}

@article{khetarpal2022continual,
  author       = {Khimya Khetarpal and Matthew Riemer and Irina Rish and Doina Precup},
  title        = {Towards Continual Reinforcement Learning: A Review and Perspectives},
  year         = {2022},
  journal      = {arXiv preprint},
  url          = {https://arxiv.org/abs/2012.13490}
}

@article{kudithipudi2022lifelong,
  author       = {Dhireesha Kudithipudi et al.},
  title        = {Biological underpinnings for lifelong learning machines},
  year         = {2022},
  journal      = {Nature Machine Intelligence},
  volume       = {4},
  pages        = {196--210},
  url          = {https://www.nature.com/articles/s42256-022-00452-0}
}

@misc{levin2024space,
  author       = {Michael Levin},
  title        = {The Space Of Possible Minds},
  year         = {2024},
  journal      = {Noema},
  url          = {https://www.noemamag.com/ai-could-be-a-bridge-toward-diverse-intelligence/}
}

@article{loshchilov2015batch,
  author       = {Ilya Loshchilov and Frank Hutter},
  title        = {Online Batch Selection for Faster Training of Neural Networks},
  year         = {2015},
  journal      = {arXiv preprint},
  url          = {https://arxiv.org/abs/1511.06343}
}

@misc{mcmahan2022federated,
  author       = {Brendan McMahan and Abhradeep Thakurta},
  title        = {Federated Learning with Formal Differential Privacy Guarantees},
  year         = {2022},
  url          = {https://research.google/blog/federated-learning-with-formal-differential-privacy-guarantees/}
}

@article{mikolov2013wordvecs,
  author       = {Tomas Mikolov and Ilya Sutskever and Kai Chen and Greg Corrado and Jeffrey Dean},
  title        = {Distributed Representations of Words and Phrases and their Compositionality},
  year         = {2013},
  journal      = {arXiv preprint},
  url          = {https://arxiv.org/abs/1310.4546}
}

@inproceedings{mindermann2022prioritized,
  author       = {Sören Mindermann et al.},
  title        = {Prioritized Training on Points that are Learnable, Worth Learning, and not yet Learnt},
  year         = {2022},
  booktitle    = {Proceedings of Machine Learning Research},
  volume       = {162},
  pages        = {15630--15649},
  url          = {https://proceedings.mlr.press/v162/mindermann22a.html}
}

@techreport{mitchell2006discipline,
  author       = {Tom Mitchell},
  title        = {The Discipline of Machine Learning},
  year         = {2006},
  institution  = {CMU},
  number       = {CMU-ML-06-108},
  url          = {https://www.cs.cmu.edu/~tom/pubs/MachineLearning.pdf}
}

@article{Parisi2019,
  author = {Parisi, German I. and Kemker, Ronald and Part, Jose L. and Kanan, Christopher and Wermter, Stefan},
  title = {Continual lifelong learning with neural networks: A review},
  journal = {Neural Networks},
  volume = {113},
  pages = {54--71},
  year = {2019},
  url = {https://www.sciencedirect.com/science/article/pii/S0893608019300231}
}

@article{Parr2021,
  author = {Parr, Thomas and Pezzulo, Giovanni},
  title = {Understanding, Explanation, and Active Inference},
  journal = {Frontiers in Systems Neuroscience},
  volume = {15},
  year = {2021},
  url = {https://www.frontiersin.org/journals/systems-neuroscience/articles/10.3389/fnsys.2021.772641/full}
}

@misc{Pyro2020,
  title = {Primitives},
  year = {2020},
  howpublished = {\url{https://docs.pyro.ai/en/stable/primitives.html}}
}

@article{Radford2019,
  author = {Radford, Alec and Wu, Jeffrey and Amodei, Dario and Sutskever, Ilya and others},
  title = {Language Models are Unsupervised Multitask Learners},
  year = {2019},
  url = {https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf}
}

@article{Radford2021,
  author = {Radford, Alec and others},
  title = {Learning Transferable Visual Models From Natural Language Supervision},
  year = {2021},
  journal = {arXiv preprint},
  volume = {arXiv:2103.00020},
  url = {https://arxiv.org/abs/2103.00020}
}

@article{Radford2022,
  author = {Radford, Alec and others},
  title = {Robust Speech Recognition via Large-Scale Weak Supervision},
  year = {2022},
  journal = {arXiv preprint},
  volume = {arXiv:2212.04356},
  url = {https://arxiv.org/abs/2212.04356}
}

@article{Raffel2019,
  author = {Raffel, Colin and Shazeer, Noam and Roberts, Adam and others},
  title = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},
  year = {2019},
  journal = {arXiv preprint},
  volume = {arXiv:1910.10683},
  url = {https://arxiv.org/abs/1910.10683}
}

@article{Shteingart2014,
  author = {Shteingart, Hanan and Loewenstein, Yonatan},
  title = {Reinforcement learning and human behavior},
  journal = {Current Opinion in Neurobiology},
  volume = {25},
  pages = {93--98},
  year = {2014},
  url = {https://www.sciencedirect.com/science/article/abs/pii/S0959438813002286}
}

@article{Tsai2019,
  author = {Tsai, Yao-Hung Hubert and Bai, Shaojie and Yamada, Makoto and Morency, Louis-Philippe and Salakhutdinov, Ruslan},
  title = {Transformer Dissection: A Unified Understanding of Transformer's Attention via the Lens of Kernel},
  year = {2019},
  journal = {arXiv preprint},
  volume = {arXiv:1908.11775},
  url = {https://arxiv.org/abs/1908.11775}
}

@article{Wang2024,
  author = {Wang, Liyuan and Zhang, Xingxing and Su, Hang and Zhu, Jun},
  title = {A Comprehensive Survey of Continual Learning: Theory, Method and Application},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {46},
  number = {8},
  pages = {5362--5383},
  year = {2024},
  url = {https://ieeexplore.ieee.org/document/10444954}
}

@article{Weiss2021,
  author = {Weiss, Gail and Goldberg, Yoav and Yahav, Eran},
  title = {Thinking Like Transformers},
  year = {2021},
  journal = {arXiv preprint},
  volume = {arXiv:2106.06981},
  url = {https://arxiv.org/abs/2106.06981}
}

@article{Zhuang2019,
  author = {Zhuang, Fuzhen and others},
  title = {A Comprehensive Survey on Transfer Learning},
  year = {2019},
  journal = {arXiv preprint},
  volume = {arXiv:1911.02685},
  url = {https://arxiv.org/abs/1911.02685}
}

@article{Zia2024,
  author = {Zia, Tehseen},
  title = {Unveiling of Large Multimodal Models: Shaping the Landscape of Language Models in 2024},
  journal = {Unite.ai},
  year = {2024},
  url = {https://www.unite.ai/unveiling-of-large-multimodal-models-shaping-the-landscape-of-language-models-in-2024/}
}
