@article{schulman2017proximal,
  title={Proximal policy optimization algorithms},
  author={Schulman, John and Wolski, Filip and Dhariwal, Prafulla and et al},
  journal={arXiv preprint arXiv:1707.06347},
  year={2017}
}

@misc{huggingface-blog-rlhf,
  title = {Illustrating Reinforcement Learning from Human Feedback (RLHF)},
  author = {The Hugging Face Team},
  howpublished = {\url{https://huggingface.co/blog/rlhf}},
  year = {2023},
  note = {Accessed: 2024-06-26}
}

@misc{newfacade-notes-on-reinforcement-learning,
  title = {Reinforcement Learning From Human Feedback Notes},
  author = {Yunhui Xia and Wei Shen},
  howpublished = {\url{https://newfacade.github.io/notes-on-reinforcement-learning/17-ppo-trl.html}},
  year = {2023},
  note = {Accessed: 2024-06-26}
}

@article{cobbe2021training,
  title={Training verifiers to solve math word problems},
  author={Cobbe, Karl and Kosaraju, Vineet and Bavarian, Mohammad and et al},
  journal={arXiv preprint arXiv:2110.14168},
  year={2021}
}

@inproceedings{gao2023scaling,
  title={Scaling laws for reward model overoptimization},
  author={Gao, Leo and Schulman, John and Hilton, Jacob},
  booktitle={International Conference on Machine Learning},
  pages={10835--10866},
  year={2023},
  organization={PMLR}
}

@article{zhang2024generative,
  title={Generative Verifiers: Reward Modeling as Next-Token Prediction},
  author={Zhang, Liang and Hosseini, Ashwin and Bansal, Hannaneh and et al},
  journal={arXiv preprint arXiv:2408.15240},
  year={2024}
}

@article{zheng2023judging,
  title={Judging llm-as-a-judge with mt-bench and chatbot arena},
  author={Zheng, Lianhui and Chiang, Wei-Lin and Sheng, Ying and et al},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  pages={46595--46623},
  year={2023}
}

@article{rafailov2024direct,
  title={Direct preference optimization: Your language model is secretly a reward model},
  author={Rafailov, Rafael and Sharma, Archit and Mitchell, Eric and et al},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@inproceedings{wang2024math,
  title={Math-shepherd: Verify and reinforce llms step-by-step without human annotations},
  author={Wang, Peifeng and Li, Lei and Shao, Zhaopeng and et al},
  booktitle={Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={9426--9439},
  year={2024}
}

@article{setlur2024rl,
  title={RL on Incorrect Synthetic Data Scales the Efficiency of LLM Math Reasoning by Eight-Fold},
  author={Setlur, Aman and Garg, Shikhar and Geng, Xinyang and et al},
  journal={arXiv preprint arXiv:2406.14532},
  year={2024}
}

@inproceedings{liu2022dont,
  title={Don't throw away your value model! Generating more preferable text with Value-Guided Monte-Carlo Tree Search decoding},
  author={Liu, Junxian and Cohen, Ari and Pasunuru, Ramakanth and et al},
  booktitle={First Conference on Language Modeling},
  year={2022}
}

@article{shen2024policy,
  title={Policy Filtration in RLHF to Fine-Tune LLM for Code Generation},
  author={Shen, Wenkang and Zhang, Chunyuan},
  journal={arXiv preprint arXiv:2409.06957},
  year={2024}
}