@article{baurComplexityPartialDerivatives1983,
  title = {The complexity of partial derivatives},
  author = {Baur, Walter and Strassen, Volker},
  year = {1983},
  month = feb,
  journal = {Theoretical Computer Science},
  volume = {22},
  number = {3},
  pages = {317--330},
  issn = {0304-3975},
  doi = {10.1016/0304-3975(83)90110-X},
  url = {https://www.sciencedirect.com/science/article/pii/030439758390110X},
  urldate = {2024-05-07},
  abstract = {Let L denote the nonscalar complexity in k(x1,…, xn). We prove L(ƒ,∂ƒ/∂x1,…,∂ƒ/∂xn)⩽3L(ƒ). Using this we determine the complexity of single power sums, single elementary symmetric functions, the resultant and the discriminant as root functions, up to order of magnitude. Also we linearly reduce matrix inversion to computing the determinant.},
  keywords = {sparse_ad_blog,tracer},
  file = {/home/guillaume/Zotero/storage/YKEAGMGI/030439758390110X.html}
}

@article{bezansonJuliaFreshApproach2017,
  title = {Julia: A Fresh Approach to Numerical Computing},
  shorttitle = {Julia},
  author = {Bezanson, Jeff and Edelman, Alan and Karpinski, Stefan and Shah, Viral B.},
  year = {2017},
  month = jan,
  journal = {SIAM Review},
  volume = {59},
  number = {1},
  pages = {65--98},
  issn = {0036-1445, 1095-7200},
  doi = {10.1137/141000671},
  url = {https://epubs.siam.org/doi/10.1137/141000671},
  urldate = {2022-12-03},
  langid = {english},
  language = {en},
  keywords = {bootstrap,gnn,hmm,inferopt,povar,sparse_ad_blog,thesis,tracer,viva},
  file = {/home/guillaume/Zotero/storage/YWLISSFK/Bezanson et al_2017_Julia.pdf}
}

@article{bischofEfficientComputationGradients1996,
  title = {Efficient computation of gradients and Jacobians by dynamic exploitation of sparsity in automatic differentiation},
  author = {Bischof, Christian H. and Khademi, Peyvand M. and Buaricha, Ali and Alan, Carle},
  year = {1996},
  month = jan,
  journal = {Optimization Methods and Software},
  volume = {7},
  number = {1},
  pages = {1--39},
  publisher = {Taylor & Francis},
  issn = {1055-6788},
  doi = {10.1080/10556789608805642},
  url = {https://doi.org/10.1080/10556789608805642},
  urldate = {2024-03-28},
  abstract = {Automatic differentiation (AD) is a technique that augments computer codes with statements for the computation of derivatives. The computational workhorse of AD-generated codes for first-order derivatives is the linear combination of vectors. For many large-scale problems, the vectors involved in this operation are inherently sparse. If the underlying function is a partially separable one (e.g., if its Hessian is sparse), many of the intermediate gradient vectors computed by AD will also be sparse, even though the final gradient is likely to be dense. For large Jacobians computations, every intermediate derivative vector is usually at least as sparse as the least sparse row of the final Jacobian. In this paper, we show that dynamic exploitation of the sparsity inherent in derivative computation can result in dramatic gains in runtime and memory savings. For a set of gradient problems exhibiting implicit sparsity, we report on the runtime and memory requirements of computing the gradients with the ADIFOR (Automatic Differentiation of FORtran) tool, both with and without employing the SparsLinC (Sparse Linear Combinations) library, and show that SparsLinC can reduce runtime and memory costs by orders of magnitude. We also compute sparse Jacobians using the SparsLinC-based approach — in the process, automatically detecting the sparsity structure of the Jacobian — and show that these Jacobian results compare favorably with those of previous techniques that require a priori knowledge of the sparsity structure of the Jacobian},
  keywords = {semidone,sparse_ad_blog,tracer},
  file = {/home/guillaume/Zotero/storage/2C6PGEL4/Bischof et al. - 1996 - Efficient computation of gradients and Jacobians b.pdf}
}

@misc{blondelElementsDifferentiableProgramming2024,
  title = {The Elements of Differentiable Programming},
  author = {Blondel, Mathieu and Roulet, Vincent},
  year = {2024},
  month = jul,
  number = {arXiv:2403.14606},
  eprint = {2403.14606},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2403.14606},
  url = {http://arxiv.org/abs/2403.14606},
  urldate = {2024-08-06},
  abstract = {Artificial intelligence has recently experienced remarkable advances, fueled by large models, vast datasets, accelerated hardware, and, last but not least, the transformative power of differentiable programming. This new programming paradigm enables end-to-end differentiation of complex computer programs (including those with control flows and data structures), making gradient-based optimization of program parameters possible. As an emerging paradigm, differentiable programming builds upon several areas of computer science and applied mathematics, including automatic differentiation, graphical models, optimization and statistics. This book presents a comprehensive review of the fundamental concepts useful for differentiable programming. We adopt two main perspectives, that of optimization and that of probability, with clear analogies between the two. Differentiable programming is not merely the differentiation of programs, but also the thoughtful design of programs intended for differentiation. By making programs differentiable, we inherently introduce probability distributions over their execution, providing a means to quantify the uncertainty associated with program outputs.},
  archiveprefix = {arXiv},
  keywords = {autodiff,done,sparse_ad_blog,tracer},
  file = {/home/guillaume/Zotero/storage/6LFEKUZA/Blondel_Roulet_2024_The Elements of Differentiable Programming.pdf;/home/guillaume/Zotero/storage/7VQWVTKF/2403.html}
}

@article{bouchet-valatDataFramesjlFlexibleFast2023,
  title = {DataFrames.jl: Flexible and Fast Tabular Data in Julia},
  shorttitle = {DataFrames.jl},
  author = {Bouchet-Valat, Milan and Kamiński, Bogumił},
  year = {2023},
  month = sep,
  journal = {Journal of Statistical Software},
  volume = {107},
  pages = {1--32},
  issn = {1548-7660},
  doi = {10.18637/jss.v107.i04},
  url = {https://doi.org/10.18637/jss.v107.i04},
  urldate = {2023-09-25},
  abstract = {DataFrames.jl is a package written for and in the Julia language offering flexible and efficient handling of tabular data sets in memory. Thanks to Julia's unique strengths, it provides an appealing set of features: Rich support for standard data processing tasks and excellent flexibility and efficiency for more advanced and non-standard operations. We present the fundamental design of the package and how it compares with implementations of data frames in other languages, its main features, performance, and possible extensions. We conclude with a practical illustration of typical data processing operations.},
  copyright = {Copyright (c) 2023 Milan  Bouchet-Valat, Bogumił Kamiński},
  langid = {english},
  language = {en},
  keywords = {sparse_ad_blog,tracer},
  file = {/home/guillaume/Zotero/storage/9ZDJS8JM/Bouchet-Valat and Kamiński - 2023 - DataFrames.jl Flexible and Fast Tabular Data in J.pdf}
}

@misc{bradburyJAXComposableTransformations2018,
  title = {JAX: composable transformations of Python+NumPy programs},
  author = {Bradbury, James and Frostig, Roy and Hawkins, Peter and Johnson, Matthew James and Leary, Chris and Maclaurin, Dougal and Necula, George and Paszke, Adam and VanderPlas, Jake and Wanderman-Milne, Skye and Zhang, Qiao},
  year = {2018},
  url = {http://github.com/google/jax},
  keywords = {sparse_ad_blog,tracer}
}

@article{colemanEfficientComputationSparse1998,
  title = {The Efficient Computation of Sparse Jacobian Matrices Using Automatic Differentiation},
  author = {Coleman, Thomas F. and Verma, Arun},
  year = {1998},
  month = jan,
  journal = {SIAM Journal on Scientific Computing},
  volume = {19},
  number = {4},
  pages = {1210--1233},
  publisher = {Society for Industrial and Applied Mathematics},
  issn = {1064-8275},
  doi = {10.1137/S1064827595295349},
  url = {https://epubs.siam.org/doi/abs/10.1137/S1064827595295349},
  urldate = {2024-05-07},
  abstract = {The computation of large sparse Jacobian matrices is required in many important large-scale scientific problems. Three approaches to computing such matrices are considered: hand-coding, difference approximations, and automatic differentiation using the ADIFOR (automatic differentiation in Fortran) tool. The authors compare the numerical reliability and computational efficiency of these approaches on applications from the MINPACK-2 test problem collection. The conclusion is that ADIFOR is the method of choice, leading to results that are as accurate as hand-coded derivatives, while at the same time outperforming difference approximations in both accuracy and speed.},
  keywords = {sparse_ad_blog,todo,tracer},
  file = {/home/guillaume/Zotero/storage/8CUCBYGA/Coleman and Verma - 1998 - The Efficient Computation of Sparse Jacobian Matri.pdf}
}

@article{colemanEstimationSparseHessian1984,
  title = {Estimation of sparse hessian matrices and graph coloring problems},
  author = {Coleman, Thomas F. and Moré, Jorge J.},
  year = {1984},
  month = oct,
  journal = {Mathematical Programming},
  volume = {28},
  number = {3},
  pages = {243--270},
  issn = {1436-4646},
  doi = {10.1007/BF02612334},
  url = {https://doi.org/10.1007/BF02612334},
  urldate = {2024-05-07},
  abstract = {Large scale optimization problems often require an approximation to the Hessian matrix. If the Hessian matrix is sparse then estimation by differences of gradients is attractive because the number of required differences is usually small compared to the dimension of the problem. The problem of estimating Hessian matrices by differences can be phrased as follows: Given the sparsity structure of a symmetric matrixA, obtain vectorsd1,d2, …dp such thatAd1,Ad2, …Adp determineA uniquely withp as small as possible. We approach this problem from a graph theoretic point of view and show that both direct and indirect approaches to this problem have a natural graph coloring interpretation. The complexity of the problem is analyzed and efficient practical heuristic procedures are developed. Numerical results illustrate the differences between the various approaches.},
  langid = {english},
  language = {en},
  keywords = {semidone,sparse_ad_blog,tracer},
  file = {/home/guillaume/Zotero/storage/BKQV52ET/Coleman and Moré - 1984 - Estimation of sparse hessian matrices and graph coloring problems.pdf}
}

@article{curtisEstimationSparseJacobian1974,
  title = {On the Estimation of Sparse Jacobian Matrices},
  author = {Curtis, A. R. and Powell, M. J. D. and Reid, J. K.},
  year = {1974},
  month = feb,
  journal = {IMA Journal of Applied Mathematics},
  volume = {13},
  number = {1},
  pages = {117--119},
  issn = {0272-4960},
  doi = {10.1093/imamat/13.1.117},
  url = {https://doi.org/10.1093/imamat/13.1.117},
  urldate = {2024-05-08},
  abstract = {We show how to use known constant elements in a Jacobian matrix to reduce the work required to estimate the remaining elements by finite differences.},
  keywords = {sparse_ad_blog,tracer},
  file = {/home/guillaume/Zotero/storage/KKIQT6J4/curtis1974.pdf.pdf;/home/guillaume/Zotero/storage/VRL7WZAH/CURTIS et al. - 1974 - On the Estimation of Sparse Jacobian Matrices.pdf;/home/guillaume/Zotero/storage/NBQFP9I8/721987.html}
}

@inproceedings{dagreouHowComputeHessianvector2024,
  title = {How to compute Hessian-vector products?},
  booktitle = {The Third Blogpost Track at ICLR 2024},
  author = {Dagréou, Mathieu and Ablin, Pierre and Vaiter, Samuel and Moreau, Thomas},
  year = {2024},
  month = feb,
  url = {https://openreview.net/forum?id=rTgjQtGP3O},
  urldate = {2024-03-16},
  abstract = {The products between the Hessian of a function and a vector, so-called Hessian-vector product (HVPs) is a quantity that appears in optimization and machine learning. However, the computation of HVPs is often considered prohibitive, preventing practitioners from using algorithms that rely on these quantities. Standard automatic differentiation theory predicts that computing a HVP has a cost of the same order of magnitude as computing a gradient. The goal of this blog post is to provide a practical counterpart to this theoretical result, showing that modern automatic differentiation frameworks, Jax and Pytorch, allow for efficient computation of these HVPs in standard deep learning cost functions.},
  langid = {english},
  language = {en},
  keywords = {done,sparse_ad_blog,tracer},
  file = {/home/guillaume/Zotero/storage/FE3QWTK5/forum.html}
}

@misc{dalleGdalleSparseMatrixColoringsjlV04102024,
  title = {gdalle/SparseMatrixColorings.jl: v0.4.10},
  shorttitle = {gdalle/SparseMatrixColorings.jl},
  author = {Dalle, Guillaume and Montoison, Alexis and Hill, Adrian},
  year = {2024},
  month = nov,
  doi = {10.5281/zenodo.14077901},
  url = {https://zenodo.org/records/14077901},
  urldate = {2024-11-21},
  abstract = {SparseMatrixColorings v0.4.10 Diff since v0.4.9 Merged pull requests: Add compressed visualization (\#157) (@gdalle) Closed issues: Bicoloring (\#23)},
  howpublished = {Zenodo},
  keywords = {sparse_ad_blog,tracer},
  file = {/home/guillaume/Zotero/storage/4C37IAZB/14077901.html}
}

@misc{dalleJuliaDiffDifferentiationInterfacejlDifferentiationInterfacev06232024,
  title = {JuliaDiff/DifferentiationInterface.jl: DifferentiationInterface-v0.6.23},
  shorttitle = {JuliaDiff/DifferentiationInterface.jl},
  author = {Dalle, Guillaume and Hill, Adrian and Tebbutt, Will and Marcotte, Alain and Scheidegger, Andreas and ExpandingMan and Wechsler, Felix and Gerlero, Gabriel and Schmitz, Niklas and Yong, Penelope and Dixit, Vaibhav Kumar},
  year = {2024},
  month = nov,
  doi = {10.5281/zenodo.14191253},
  url = {https://zenodo.org/records/14191253},
  urldate = {2024-11-21},
  abstract = {DifferentiationInterface DifferentiationInterface-v0.6.23 Diff since DifferentiationInterface-v0.6.22 Merged pull requests: perf: check mutability of array before preallocating dual buffer (\#619) (@gdalle) CompatHelper: bump compat for JLArrays in [weakdeps] to 0.2 for package DifferentiationInterfaceTest, (keep existing compat) (\#621) (@github-actions[bot]) refactor!: remove randomness in scenario creation (\#623) (@gdalle) Fix tutorial typo (\#625) (@penelopeysm) chore: Bump codecov/codecov-action from 4 to 5 (\#626) (@dependabot[bot]) test: don't test Diffractor (currently broken) (\#627) (@gdalle) fix: replace use of undocumented AutoForwardDiff constructor (\#629) (@gdalle) fix: disable ForwardDiff tag checking with custom backend tags (\#631) (@gdalle) Closed issues: Error in ForwardDiff tagging (\#594) Remove undocumented ADTypes constructors (\#628) ForwardDiff Tag from prepare\_jacobian not being applied to function in jacobian! (\#630)},
  howpublished = {Zenodo},
  keywords = {sparse_ad_blog,tracer},
  file = {/home/guillaume/Zotero/storage/M2VAQQRW/14191253.html}
}

@article{danischMakiejlFlexibleHighperformance2021,
  title = {Makie.jl: Flexible high-performance data visualization for Julia},
  shorttitle = {Makie.jl},
  author = {Danisch, Simon and Krumbiegel, Julius},
  year = {2021},
  month = sep,
  journal = {Journal of Open Source Software},
  volume = {6},
  number = {65},
  pages = {3349},
  issn = {2475-9066},
  doi = {10.21105/joss.03349},
  url = {https://joss.theoj.org/papers/10.21105/joss.03349},
  urldate = {2024-02-29},
  abstract = {Danisch et al., (2021). Makie.jl: Flexible high-performance data visualization for Julia. Journal of Open Source Software, 6(65), 3349, https://doi.org/10.21105/joss.03349},
  langid = {english},
  language = {en},
  keywords = {gnn,hmm,sparse_ad_blog,tracer},
  file = {/home/guillaume/Zotero/storage/T4W5S92V/Danisch and Krumbiegel - 2021 - Makie.jl Flexible high-performance data visualiza.pdf}
}

@article{dixonAutomaticDifferentiationLarge1990,
  title = {Automatic differentiation of large sparse systems},
  author = {Dixon, L. C. W. and Maany, Z. and Mohseninia, M.},
  year = {1990},
  month = may,
  journal = {Journal of Economic Dynamics and Control},
  series = {Special Issue on Computer Science and Economics},
  volume = {14},
  number = {2},
  pages = {299--311},
  issn = {0165-1889},
  doi = {10.1016/0165-1889(90)90023-A},
  url = {https://www.sciencedirect.com/science/article/pii/016518899090023A},
  urldate = {2024-05-08},
  abstract = {With the advent of computer languages such as ADA and PASCAL-SC that allow the definition of new data types and the overwriting of operators it has become feasible to implement new algebras on a computer relatively simply. In the field of optimization when the objective function to be minimised F(x), xϵRN, can easily involve hundreds of arithmetic operations m, the task of deriving its gradient ∇F and Hessian ∇2F can be daunting and involve many man-days of effort. The classical alternative of estimating the value of the derivatives by difference formulae can lead to numerical limitation on the performance of the code and be computationally expensive. In this paper we discuss five new algebras that make both tasks redundant as with each the computer can accurately evaluate ∇F and ∇2F in far less time than the numerical difference formulae would imply. The interface between these algebras and the Truncated Newton Algorithm for unconstrained optimisation is also described.},
  keywords = {semidone,sparse_ad_blog,tracer},
  file = {/home/guillaume/Zotero/storage/XKL8WUSB/dixon1990.pdf.pdf;/home/guillaume/Zotero/storage/C83LAY9K/016518899090023A.html}
}

@article{gebremedhinColPackSoftwareGraph2013,
  title = {ColPack: Software for graph coloring and related problems in scientific computing},
  shorttitle = {ColPack},
  author = {Gebremedhin, Assefaw H. and Nguyen, Duc and Patwary, Md. Mostofa Ali and Pothen, Alex},
  year = {2013},
  month = oct,
  journal = {ACM Transactions on Mathematical Software},
  volume = {40},
  number = {1},
  pages = {1:1--1:31},
  issn = {0098-3500},
  doi = {10.1145/2513109.2513110},
  url = {https://dl.acm.org/doi/10.1145/2513109.2513110},
  urldate = {2024-05-07},
  abstract = {We present a suite of fast and effective algorithms, encapsulated in a software package called ColPack, for a variety of graph coloring and related problems. Many of the coloring problems model partitioning needs arising in compression-based computation of Jacobian and Hessian matrices using Algorithmic Differentiation. Several of the coloring problems also find important applications in many areas outside derivative computation, including frequency assignment in wireless networks, scheduling, facility location, and concurrency discovery and data movement operations in parallel and distributed computing. The presentation in this article includes a high-level description of the various coloring algorithms within a common design framework, a detailed treatment of the theory and efficient implementation of known as well as new vertex ordering techniques upon which the coloring algorithms rely, a discussion of the package's software design, and an illustration of its usage. The article also includes an extensive experimental study of the major algorithms in the package using real-world as well as synthetically generated graphs.},
  keywords = {sparse_ad_blog,todo,tracer},
  file = {/home/guillaume/Zotero/storage/6KAR2CMV/Gebremedhin et al. - 2013 - ColPack Software for graph coloring and related p.pdf}
}

@article{gebremedhinWhatColorYour2005,
  title = {What Color Is Your Jacobian? Graph Coloring for Computing Derivatives},
  shorttitle = {What Color Is Your Jacobian?},
  author = {Gebremedhin, Assefaw Hadish and Manne, Fredrik and Pothen, Alex},
  year = {2005},
  month = jan,
  journal = {SIAM Review},
  volume = {47},
  number = {4},
  pages = {629--705},
  publisher = {Society for Industrial and Applied Mathematics},
  issn = {0036-1445},
  doi = {10/cmwds4},
  url = {https://epubs.siam.org/doi/abs/10.1137/S0036144504444711},
  urldate = {2022-02-03},
  abstract = {Graph coloring has been employed since the 1980s to efficiently compute sparse Jacobian and Hessian matrices using either finite differences or automatic differentiation. Several coloring problems occur in this context, depending on whether the matrix is a Jacobian or a Hessian, and on the specifics of the computational techniques employed. We consider eight variant vertex coloring problems here. This article begins with a gentle introduction to the problem of computing a sparse Jacobian, followed by an overview of the historical development of the research area. Then we present a unifying framework for the graph models of the variant matrix estimation problems. The framework is based upon the viewpoint that a partition of a matrix into structurally orthogonal groups of columns corresponds to distance-2 coloring an appropriate graph representation. The unified framework helps integrate earlier work and leads to fresh insights; enables the design of more efficient algorithms for many problems; leads to new algorithms for others; and eases the task of building graph models for new problems. We report computational results on two of the coloring problems to support our claims. Most of the methods for these problems treat a column or a row of a matrix as an atomic entity, and partition the columns or rows (or both). A brief review of methods that do not fit these criteria is provided. We also discuss results in discrete mathematics and theoretical computer science that intersect with the topics considered here.},
  keywords = {autodiff,semidone,sparse_ad_blog,tracer},
  file = {/home/guillaume/Zotero/storage/AGLB9RM8/Gebremedhin et al_2005_What Color Is Your Jacobian.pdf}
}

@inproceedings{gowdaSparsityProgrammingAutomated2019,
  title = {Sparsity Programming: Automated Sparsity-Aware Optimizations in Differentiable Programming},
  shorttitle = {Sparsity Programming},
  booktitle = {Program Transformations for ML Workshop at NeurIPS 2019},
  author = {Gowda, Shashi and Ma, Yingbo and Churavy, Valentin and Edelman, Alan and Rackauckas, Christopher},
  year = {2019},
  month = sep,
  url = {https://openreview.net/forum?id=rJlPdcY38B},
  urldate = {2024-03-27},
  abstract = {Previous studies in numerical analysis have shown how the calculation of a Jacobian, Hessian, and their factorizations can be accelerated when their sparsity pattern is known. However, accurate Jacobian and Hessian sparsity patterns cannot be computed numerically, leaving the burden on the user to provide them. As scientific simulations have grown in complexity, the application of differentiable programming for calculating the derivatives of arbitrary programs with respect to parameters has been on the rise, but current methodologies do not automatically detect and make use of sparsity-related accelerations. In this manuscript we develop a method for the accurate and efficient construction of sparsity patterns by transforming an input program into one that computes the sparsity pattern of its Jacobian or Hessian. Our implementation, which we demonstrate on partial differential equations, is a scalable technique for acceleration of automatic differentiation on arbitrarily complex multivariate programs. This demonstrates that dynamic program analysis can be effective in more scenarios than are currently well known in differentiable programming.},
  langid = {english},
  language = {en},
  keywords = {done,sparse_ad_blog,tracer},
  file = {/home/guillaume/Zotero/storage/4N3SHJ4L/Gowda et al. - 2019 - Sparsity Programming Automated Sparsity-Aware Opt.pdf}
}

@article{gowerComputingSparsityPattern2014,
  title = {Computing the sparsity pattern of Hessians using automatic differentiation},
  author = {Gower, Robert Mansel and Mello, Margarida Pinheiro},
  year = {2014},
  month = mar,
  journal = {ACM Transactions on Mathematical Software},
  volume = {40},
  number = {2},
  pages = {10:1--10:15},
  issn = {0098-3500},
  doi = {10.1145/2490254},
  url = {https://dl.acm.org/doi/10.1145/2490254},
  urldate = {2024-03-27},
  abstract = {We compare two methods that calculate the sparsity pattern of Hessian matrices using the computational framework of automatic differentiation. The first method is a forward-mode algorithm by Andrea Walther in 2008 which has been implemented as the driver called hess\_pat in the automatic differentiation package ADOL-C. The second is edge\_push\_sp, a new reverse mode algorithm descended from the edge\_pushing algorithm for calculating Hessians by Gower and Mello in 2012. We present complexity analysis and perform numerical tests for both algorithms. The results show that the new reverse algorithm is very promising.},
  keywords = {sparse_ad_blog,todo,tracer},
  file = {/home/guillaume/Zotero/storage/P4QHBTVR/Gower and Mello - 2014 - Computing the sparsity pattern of Hessians using a.pdf}
}

@article{gowerNewFrameworkComputation2012,
  title = {A new framework for the computation of Hessians},
  author = {Gower, R. M. and Mello, M. P.},
  year = {2012},
  month = apr,
  journal = {Optimization Methods and Software},
  volume = {27},
  number = {2},
  pages = {251--273},
  publisher = {Taylor & Francis},
  issn = {1055-6788},
  doi = {10.1080/10556788.2011.580098},
  url = {https://doi.org/10.1080/10556788.2011.580098},
  urldate = {2024-05-07},
  abstract = {We investigate the computation of Hessian matrices via Automatic Differentiation, using a graph model and an algebraic model. The graph model reveals the inherent symmetries involved in calculating the Hessian. The algebraic model, based on Griewank and Walther's [Evaluating derivatives, in Principles and Techniques of Algorithmic Differentiation, 2nd ed., Society for Industrial and Applied Mathematics (SIAM), Philadelphia, PA, 2008] state transformations synthesizes the calculation of the Hessian as a formula. These dual points of view, graphical and algebraic, lead to a new framework for Hessian computation. This is illustrated by developing edge\_pushing, a new truly reverse Hessian computation algorithm that fully exploits the Hessian's symmetry. Computational experiments compare the performance of edge\_pushing on 16 functions from the CUTE collection [I. Bongartz et al. Cute: constrained and unconstrained testing environment, ACM Trans. Math. Softw. 21(1) (1995), pp. 123–160] against two algorithms available as drivers of the software ADOL-C [A. Griewank et al. ADOL-C: A package for the automatic differentiation of algorithms written in C/C++, Technical report, Institute of Scientific Computing, Technical University Dresden, 1999. Updated version of the paper published in ACM Trans. Math. Softw. 22, 1996, pp. 131–167; A. Walther, Computing sparse Hessians with automatic differentiation, ACM Trans. Math. Softw. 34(1) (2008), pp. 1–15; A.H. Gebremedhin et al. Efficient computation of sparse Hessians using coloring and automatic differentiation, INFORMS J. Comput. 21(2) (2009), pp. 209–223], and the results are very promising.},
  keywords = {sparse_ad_blog,todo,tracer},
  file = {/home/guillaume/Zotero/storage/HWT7JHGW/Gower and Mello - 2012 - A new framework for the computation of Hessians.pdf}
}

@article{griewankDetectingJacobianSparsity2002,
  title = {Detecting Jacobian sparsity patterns by Bayesian probing},
  author = {Griewank, Andreas and Mitev, Christo},
  year = {2002},
  month = jun,
  journal = {Mathematical Programming},
  volume = {93},
  number = {1},
  pages = {1--25},
  issn = {1436-4646},
  doi = {10.1007/s101070100281},
  url = {https://doi.org/10.1007/s101070100281},
  urldate = {2024-09-06},
  abstract = {In this paper we describe an automatic procedure for successively reducing the set of possible nonzeros in a Jacobian matrix until eventually the exact sparsity pattern is obtained. The dependence information needed in this probing process consist of “Boolean” Jacobian-vector products and possibly also vector-Jacobian products, which can be evaluated exactly by automatic differentiation or approximated by divided differences. The latter approach yields correct sparsity patterns, provided there is no exact cancellation at the current argument.¶Starting from a user specified, or by default initialized, probability distribution the procedure suggests a sequence of probing vectors. The resulting information is then used to update the probabilities that certain elements are nonzero according to Bayes’ law. The proposed probing procedure is found to require only O(logn) probing vectors on randomly generated matrices of dimension n, with a fixed number of nonzeros per row or column. This result has been proven for (block-) banded matrices, and for general sparsity pattern finite termination of the probing procedure can be guaranteed.},
  langid = {english},
  language = {en},
  keywords = {sparse_ad_blog,todo,tracer},
  file = {/home/guillaume/Zotero/storage/84W4FPNL/Griewank_Mitev_2002_Detecting Jacobian sparsity patterns by Bayesian probing.pdf}
}

@book{griewankEvaluatingDerivativesPrinciples2008,
  title = {Evaluating derivatives: principles and techniques of algorithmic differentiation},
  shorttitle = {Evaluating derivatives},
  author = {Griewank, Andreas and Walther, Andrea},
  year = {2008},
  edition = {2nd ed},
  publisher = {Society for Industrial and Applied Mathematics},
  address = {Philadelphia, PA},
  abstract = {This title is a comprehensive treatment of algorithmic, or automatic, differentiation. The second edition covers recent developments in applications and theory, including an elegant NP completeness argument and an introduction to scarcity},
  isbn = {978-0-89871-659-7},
  lccn = {QA304 .G76 2008},
  keywords = {autodiff,inferopt,semidone,sparse_ad_blog,thesis,tracer},
  annotation = {OCLC: ocn227574816},
  file = {/home/guillaume/Zotero/storage/ZBNZIFGC/Griewank_Walther_2008_Evaluating Derivatives.pdf},
  url = {https://epubs.siam.org/doi/book/10.1137/1.9780898717761},
}

@misc{hillSparseConnectivityTracerjl2024,
  title = {SparseConnectivityTracer.jl},
  author = {Hill, Adrian and Dalle, Guillaume},
  year = {2024},
  month = oct,
  doi = {10.5281/zenodo.13961066},
  url = {https://zenodo.org/records/13961066},
  urldate = {2024-11-04},
  abstract = {SparseConnectivityTracer v0.6.8 Diff since v0.6.7 Merged pull requests: Increase code coverage (\#206) (@adrhill) Support clamp and clamp! (\#208) (@adrhill) Remove DuplicateVector (\#209) (@adrhill) Closed issues: Fix DuplicateVector performance (\#63) Overload lu on matrices of tracers (\#138) Specialize array overloads (\#192) clamp gives TypeError from julia 1.11 (\#207)},
  howpublished = {Zenodo},
  keywords = {sparse_ad_blog,tracer},
  file = {/home/guillaume/Zotero/storage/BFFBYZ3M/13961066.html}
}

@article{hossainComputingSparseJacobian1998,
  title = {Computing a sparse Jacobian matrix by rows and columns},
  author = {Hossain, A. K. M. Shahadat and Steihaug, Trond},
  year = {1998},
  month = jan,
  journal = {Optimization Methods and Software},
  volume = {10},
  number = {1},
  pages = {33--48},
  publisher = {Taylor & Francis},
  issn = {1055-6788},
  doi = {10.1080/10556789808805700},
  url = {https://doi.org/10.1080/10556789808805700},
  urldate = {2024-11-09},
  abstract = {Efficient estimation of large sparse Jacobian matrices has been studied extensively in the last couple of years. It has been observed that the estimation of Jacobian matrix can be posed as a graph coloring problem. Elements of the matrix are estimated by taking divided difference in several directions corresponding to a group of structurally independent columns. Another possibility is to obtain the nonzero elements by means of the so called Automatic differentiation, which gives the estimates free of truncation error that one encounters in a divided difference scheme. In this paper we show that it is possible to exploit sparsity both in columns and rows by employing the forward and the reverse mode of Automatic differentiation. A graph-theoretic characterization of the problem is given.},
  keywords = {sparse_ad_blog,todo,tracer},
  file = {/home/guillaume/Zotero/storage/YP2BT6HT/Hossain and Steihaug - 1998 - Computing a sparse Jacobian matrix by rows and columns.pdf}
}

@inproceedings{paszkePyTorchImperativeStyle2019,
  title = {PyTorch: An Imperative Style, High-Performance Deep Learning Library},
  shorttitle = {PyTorch},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
  year = {2019},
  volume = {32},
  publisher = {Curran Associates, Inc.},
  url = {https://proceedings.neurips.cc/paper/2019/hash/bdbca288fee7f92f2bfa9f7012727740-Abstract.html},
  urldate = {2024-05-06},
  abstract = {Deep learning frameworks have often focused on either usability or speed, but not both. PyTorch is a machine learning library that shows that these two goals are in fact compatible: it was designed from first principles to support an imperative and Pythonic programming style that supports code as a model, makes debugging easy and is consistent with other popular scientific computing libraries, while remaining efficient and supporting hardware accelerators such as GPUs. In this paper, we detail the principles that drove the implementation of PyTorch and how they are reflected in its architecture. We emphasize that every aspect of PyTorch is a regular Python program under the full control of its user. We also explain how the careful and pragmatic implementation of the key components of its runtime enables them to work together to achieve compelling performance. We demonstrate the efficiency of individual subsystems, as well as the overall speed of PyTorch on several commonly used benchmarks.},
  keywords = {sparse_ad_blog,tracer},
  file = {/home/guillaume/Zotero/storage/Z6BMB8KI/Paszke et al_2019_PyTorch.pdf}
}

@article{pearlmutterFastExactMultiplication1994,
  title = {Fast Exact Multiplication by the Hessian},
  author = {Pearlmutter, Barak A.},
  year = {1994},
  month = jan,
  journal = {Neural Computation},
  volume = {6},
  number = {1},
  pages = {147--160},
  issn = {0899-7667},
  doi = {10.1162/neco.1994.6.1.147},
  url = {https://ieeexplore.ieee.org/abstract/document/6796137},
  urldate = {2024-03-16},
  abstract = {Just storing the Hessian H (the matrix of second derivatives δ2E/δwiδwj of the error E with respect to each pair of weights) of a large neural network is difficult. Since a common use of a large matrix like H is to compute its product with various vectors, we derive a technique that directly calculates Hv, where v is an arbitrary vector. To calculate Hv, we first define a differential operator Rvf(w) = (δ/δr)f(w + rv)|r=0, note that Rv▽w = Hv and Rvw = v, and then apply Rv· to the equations used to compute ▽w. The result is an exact and numerically stable procedure for computing Hv, which takes about as much computation, and is about as local, as a gradient evaluation. We then apply the technique to a one pass gradient calculation algorithm (backpropagation), a relaxation gradient calculation algorithm (recurrent backpropagation), and two stochastic gradient calculation algorithms (Boltzmann machines and weight perturbation). Finally, we show that this technique can be used at the heart of many iterative techniques for computing various properties of H, obviating any need to calculate the full Hessian.},
  keywords = {sparse_ad_blog,tracer},
  file = {/home/guillaume/Zotero/storage/QEM3B8NK/Pearlmutter_1994_Fast Exact Multiplication by the Hessian.pdf;/home/guillaume/Zotero/storage/SLGZT5PU/6796137.html}
}

@article{powellEstimationSparseHessian1979,
  title = {On the Estimation of Sparse Hessian Matrices},
  author = {Powell, M. J. D. and Toint, Ph. L.},
  year = {1979},
  month = dec,
  journal = {SIAM Journal on Numerical Analysis},
  volume = {16},
  number = {6},
  pages = {1060--1074},
  publisher = {Society for Industrial and Applied Mathematics},
  issn = {0036-1429},
  doi = {10.1137/0716078},
  url = {https://epubs.siam.org/doi/abs/10.1137/0716078},
  urldate = {2024-11-05},
  abstract = {Numerical optimization algorithms often require the (symmetric) matrix of second derivatives, \$\textbackslash nabla \textasciicircum 2 f( x )\$. If the Hessian matrix is large and sparse, then estimation by finite differences can be quite attractive since several schemes allow for estimation in much fewer than n gradient evaluations.The purpose of this paper is to analyze, from a combinatorial point of view, a class of methods known as substitution methods. We present a concise characterization of such methods in graph-theoretic terms. Using this characterization, we develop a complexity analysis of the general problem and derive a roundoff error bound on the Hessian approximation. Moreover, the graph model immediately reveals procedures to effect the substitution process optimally (i.e. using fewest possible substitutions given the differencing directions) in space proportional to the number of nonzeros in the Hessian matrix.},
  keywords = {sparse_ad_blog,tracer},
  file = {/home/guillaume/Zotero/storage/7IQPUZPZ/Powell and Toint - 1979 - On the Estimation of Sparse Hessian Matrices.pdf}
}

@misc{revelsForwardModeAutomaticDifferentiation2016,
  title = {Forward-Mode Automatic Differentiation in Julia},
  author = {Revels, Jarrett and Lubin, Miles and Papamarkou, Theodore},
  year = {2016},
  month = jul,
  number = {arXiv:1607.07892},
  eprint = {1607.07892},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1607.07892},
  url = {http://arxiv.org/abs/1607.07892},
  urldate = {2022-08-11},
  abstract = {We present ForwardDiff, a Julia package for forward-mode automatic differentiation (AD) featuring performance competitive with low-level languages like C++. Unlike recently developed AD tools in other popular high-level languages such as Python and MATLAB, ForwardDiff takes advantage of just-in-time (JIT) compilation to transparently recompile AD-unaware user code, enabling efficient support for higher-order differentiation and differentiation using custom number types (including complex numbers). For gradient and Jacobian calculations, ForwardDiff provides a variant of vector-forward mode that avoids expensive heap allocation and makes better use of memory bandwidth than traditional vector mode. In our numerical experiments, we demonstrate that for nontrivially large dimensions, ForwardDiff's gradient computations can be faster than a reverse-mode implementation from the Python-based autograd package. We also illustrate how ForwardDiff is used effectively within JuMP, a modeling language for optimization. According to our usage statistics, 41 unique repositories on GitHub depend on ForwardDiff, with users from diverse fields such as astronomy, optimization, finite element analysis, and statistics. This document is an extended abstract that has been accepted for presentation at the AD2016 7th International Conference on Algorithmic Differentiation.},
  archiveprefix = {arXiv},
  keywords = {done,sparse_ad_blog,thesis,tracer},
  file = {/home/guillaume/Zotero/storage/ZXUSBVZK/Revels et al_2016_Forward-Mode Automatic Differentiation in Julia.pdf;/home/guillaume/Zotero/storage/SP7X68BB/1607.html}
}

@misc{sapienzaDifferentiableProgrammingDifferential2024,
  title = {Differentiable Programming for Differential Equations: A Review},
  shorttitle = {Differentiable Programming for Differential Equations},
  author = {Sapienza, Facundo and Bolibar, Jordi and Schäfer, Frank and Groenke, Brian and Pal, Avik and Boussange, Victor and Heimbach, Patrick and Hooker, Giles and Pérez, Fernando and Persson, Per-Olof and Rackauckas, Christopher},
  year = {2024},
  month = jun,
  number = {arXiv:2406.09699},
  eprint = {2406.09699},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2406.09699},
  url = {http://arxiv.org/abs/2406.09699},
  urldate = {2024-11-22},
  abstract = {The differentiable programming paradigm is a cornerstone of modern scientific computing. It refers to numerical methods for computing the gradient of a numerical model's output. Many scientific models are based on differential equations, where differentiable programming plays a crucial role in calculating model sensitivities, inverting model parameters, and training hybrid models that combine differential equations with data-driven approaches. Furthermore, recognizing the strong synergies between inverse methods and machine learning offers the opportunity to establish a coherent framework applicable to both fields. Differentiating functions based on the numerical solution of differential equations is non-trivial. Numerous methods based on a wide variety of paradigms have been proposed in the literature, each with pros and cons specific to the type of problem investigated. Here, we provide a comprehensive review of existing techniques to compute derivatives of numerical solutions of differential equations. We first discuss the importance of gradients of solutions of differential equations in a variety of scientific domains. Second, we lay out the mathematical foundations of the various approaches and compare them with each other. Third, we cover the computational considerations and explore the solutions available in modern scientific software. Last but not least, we provide best-practices and recommendations for practitioners. We hope that this work accelerates the fusion of scientific models and data, and fosters a modern approach to scientific modelling.},
  archiveprefix = {arXiv},
  keywords = {sparse_ad_blog,tracer},
  file = {/home/guillaume/Zotero/storage/IRWQE3LN/Sapienza et al. - 2024 - Differentiable Programming for Differential Equations A Review.pdf;/home/guillaume/Zotero/storage/WKFD9DSU/2406.html}
}

@misc{schaferAbstractDifferentiationjlBackendAgnosticDifferentiable2022,
  title = {AbstractDifferentiation.jl: Backend-Agnostic Differentiable Programming in Julia},
  shorttitle = {AbstractDifferentiation.jl},
  author = {Schäfer, Frank and Tarek, Mohamed and White, Lyndon and Rackauckas, Chris},
  year = {2022},
  month = feb,
  number = {arXiv:2109.12449},
  eprint = {2109.12449},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2109.12449},
  url = {http://arxiv.org/abs/2109.12449},
  urldate = {2024-11-22},
  abstract = {No single Automatic Differentiation (AD) system is the optimal choice for all problems. This means informed selection of an AD system and combinations can be a problem-specific variable that can greatly impact performance. In the Julia programming language, the major AD systems target the same input and thus in theory can compose. Hitherto, switching between AD packages in the Julia Language required end-users to familiarize themselves with the user-facing API of the respective packages. Furthermore, implementing a new, usable AD package required AD package developers to write boilerplate code to define convenience API functions for end-users. As a response to these issues, we present AbstractDifferentiation.jl for the automatized generation of an extensive, unified, user-facing API for any AD package. By splitting the complexity between AD users and AD developers, AD package developers only need to implement one or two primitive definitions to support various utilities for AD users like Jacobians, Hessians and lazy product operators from native primitives such as pullbacks or pushforwards, thus removing tedious -- but so far inevitable -- boilerplate code, and enabling the easy switching and composing between AD implementations for end-users.},
  archiveprefix = {arXiv},
  keywords = {sparse_ad_blog},
  file = {/home/guillaume/Zotero/storage/WUXC3LHK/Schäfer et al. - 2022 - AbstractDifferentiation.jl Backend-Agnostic Differentiable Programming in Julia.pdf;/home/guillaume/Zotero/storage/F32SJPWN/2109.html}
}

@article{waltherComputingSparseHessians2008,
  title = {Computing sparse Hessians with automatic differentiation},
  author = {Walther, Andrea},
  year = {2008},
  month = jan,
  journal = {ACM Transactions on Mathematical Software},
  volume = {34},
  number = {1},
  pages = {3:1--3:15},
  issn = {0098-3500},
  doi = {10.1145/1322436.1322439},
  url = {https://dl.acm.org/doi/10.1145/1322436.1322439},
  urldate = {2024-03-27},
  abstract = {A new approach for computing a sparsity pattern for a Hessian is presented: nonlinearity information is propagated through the function evaluation yielding the nonzero structure. A complexity analysis of the proposed algorithm is given. Once the sparsity pattern is available, coloring algorithms can be applied to compute a seed matrix. To evaluate the product of the Hessian and the seed matrix, a vector version for evaluating second order adjoints is analysed. New drivers of ADOL-C are provided implementing the presented algorithms. Runtime analyses are given for some problems of the CUTE collection.},
  keywords = {done,sparse_ad_blog,tracer},
  file = {/home/guillaume/Zotero/storage/788MQVJV/Walther - 2008 - Computing sparse Hessians with automatic different.pdf}
}

@article{waltherGettingStartedADOLC2009,
  title = {Getting Started with ADOL-C},
  author = {Walther, Andrea},
  year = {2009},
  journal = {DROPS-IDN/v2/document/10.4230/DagSemProc.09061.10},
  publisher = {Schloss Dagstuhl – Leibniz-Zentrum für Informatik},
  doi = {10.4230/DagSemProc.09061.10},
  url = {https://drops.dagstuhl.de/entities/document/10.4230/DagSemProc.09061.10},
  urldate = {2024-06-05},
  abstract = {The C++ package ADOL-C described in this paper facilitates the evaluation of first and higher derivatives of vector functions that are defined by computer programs written in C or C++. The numerical values of derivative vectors are obtained free of truncation errors at mostly a small multiple of the run time and a fix small multiple random access memory required by the given function evaluation program. Derivative matrices are obtained by columns, by rows or in sparse format. This tutorial describes the source code modification required for the application of ADOL-C, the most frequently used drivers to evaluate derivatives and some recent developments.},
  copyright = {https://creativecommons.org/licenses/by/4.0/legalcode},
  langid = {english},
  language = {en},
  keywords = {done,sparse_ad_blog,tracer},
  file = {/home/guillaume/Zotero/storage/MAV7WCRX/Walther - 2009 - Getting Started with ADOL-C.pdf}
}
