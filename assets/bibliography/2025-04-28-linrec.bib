@article{ladner1980parallelprefix,
  title={Parallel prefix computation},
  author={Ladner, Richard E and Fischer, Michael J},
  journal={Journal of the ACM (JACM)},
  volume={27},
  number={4},
  pages={831--838},
  year={1980},
  publisher={ACM New York, NY, USA},
  url={https://dl.acm.org/doi/10.1145/322217.322232}
}

@article{blelloch1990prefixapplication,
  title={Prefix sums and their applications},
  author={Blelloch, Guy E},
  year={1990},
  publisher={School of Computer Science, Carnegie Mellon University Pittsburgh, PA, USA},
  url={https://www.cs.cmu.edu/~guyb/papers/Ble93.pdf}
}

@inproceedings{martin2018parallelizing,
title={Parallelizing Linear Recurrent Neural Nets Over Sequence Length},
author={Eric Martin and Chris Cundy},
booktitle={International Conference on Learning Representations},
url={https://arxiv.org/abs/1709.04057},
year={2018}
}


@inproceedings{gu2022s4,
  title={Efficiently Modeling Long Sequences with Structured State Spaces},
  author={Gu, Albert and Goel, Karan and R{\'e}, Christopher},
  booktitle={International Conference on Learning Representations (ICLR 2022)},
  url={https://arxiv.org/abs/2111.00396},
  year={2022}
}

@inproceedings{smith2023s5,
  title={Simplified State Space Layers for Sequence Modeling},
  author={Jimmy T.H. Smith and Andrew Warrington and Scott Linderman},
  booktitle={The Eleventh International Conference on Learning Representations },
  url={https://arxiv.org/abs/2208.04933},
  year={2023}
}


@inproceedings{orvieto2023lru,
  title={Resurrecting recurrent neural networks for long sequences},
  author={Orvieto, Antonio and Smith, Samuel L and Gu, Albert and Fernando, Anushan and Gulcehre, Caglar and Pascanu, Razvan and De, Soham},
  booktitle={International Conference on Machine Learning},
  url={https://arxiv.org/abs/2303.06349},
  year={2023}
}

@misc{rusch2024ossm,
      title={Oscillatory State-Space Models}, 
      author={T. Konstantin Rusch and Daniela Rus},
      url={https://arxiv.org/abs/2410.03943}, 
      year={2024}
}

@article{de2024griffin,
  title={Griffin: Mixing gated linear recurrences with local attention for efficient language models},
  author={De, Soham and Smith, Samuel L and Fernando, Anushan and Botev, Aleksandar and Cristian-Muraru, George and Gu, Albert and Haroun, Ruba and Berrada, Leonard and Chen, Yutian and Srinivasan, Srivatsan and others},
  url={https://arxiv.org/abs/2402.19427}, 
  year={2024}
}

@article{gu2024mamba,
      title={Mamba: Linear-Time Sequence Modeling with Selective State Spaces}, 
      author={Albert Gu and Tri Dao},
      booktitle={First Conference on Language Modeling},
      url={https://arxiv.org/abs/2312.00752}, 
      year={2024}
}

@misc{beck2024xlstm,
      title={xLSTM: Extended Long Short-Term Memory}, 
      author={Maximilian Beck and Korbinian Pöppel and Markus Spanring and Andreas Auer and Oleksandra Prudnikova and Michael Kopp and Günter Klambauer and Johannes Brandstetter and Sepp Hochreiter},
      booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},
      url={https://arxiv.org/abs/2405.04517}, 
      year={2024}
}

@inproceedings{dao2024mamba2,
  title={Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality},
  author={Dao, Tri and Gu, Albert},
  booktitle={Forty-first International Conference on Machine Learning},
  url={https://arxiv.org/abs/2405.21060},
  year={2024}
}

@inproceedings{rush2022annotatedS4,
  title = {The Annotated S4},
  author = {Rush, Alexander and Karamcheti, Sidd},
  booktitle = {ICLR Blog Track},
  url  = {https://iclr-blog-track.github.io/2022/03/25/annotated-s4/},
  year = {2022}
}

@article{chen2024mamba,
  title = {Mamba No. 5 (A Little Bit Of...)},
  author = {Chen, James},
  year = {2024},
  url  = {https://jameschen.io/jekyll/update/2024/02/12/mamba.html}
}

@article{grootendorst2024mamba,
  title = {A Visual Guide to Mamba and State Space Models},
  author = {Grootendorst, Maarten},
  year = {2024},
  url  = {https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-mamba-and-state}
}

@article{rush2024mamba,
  title = {Mamba: The Hard Way},
  author = {Rush, Alexander},
  year = {2024},
  url  = {https://srush.github.io/annotated-mamba/hard.html}
}


@article{merrill2016cubscan,
  title={Single-pass parallel prefix scan with decoupled look-back},
  author={Merrill, Duane and Garland, Michael},
  journal={NVIDIA, Tech. Rep. NVR-2016-002},
  url={https://research.nvidia.com/publication/2016-03_single-pass-parallel-prefix-scan-decoupled-look-back},
  year={2016}
}


@software{mambapy,
  author = {Torres--Leguet, Alexandre},
  title = {mamba.py: A simple, hackable and efficient Mamba implementation in pure PyTorch and MLX.},
  url = {https://github.com/alxndrTL/mamba.py},
  version = {1.0},
  year = {2024},
}

@software{johnryan265pscan,
  author = {Ryan, John},
  title = {FastPSCAN: 5 different algorithms for the parallel scan problem in pytorch.},
  url = {https://github.com/johnryan465/pscan},
  version = {1.0},
  year = {2024},
}

@software{acceleratedscan,
  author = {Kyrylov, Volodymyr},
  title = {Accelerated Scan},
  url = {https://github.com/proger/accelerated-scan},
  version = {1.0},
  year = {2024},
  publisher = {Zenodo}
}

@inproceedings{katharopoulos2021linearattention,
  title={{Transformers are RNNs: Fast autoregressive transformers with linear attention}},
  author={Katharopoulos, Angelos and Vyas, Apoorv and Pappas, Nikolaos and Fleuret, Fran{\c{c}}ois},
  booktitle={International conference on machine learning},
  pages={5156--5165},
  year={2020},
  organization={PMLR}
}

@software{yang2024fla,
  title  = {FLA: A Triton-Based Library for Hardware-Efficient Implementations of Linear Attention Mechanism},
  author = {Yang, Songlin and Zhang, Yu},
  url    = {https://github.com/sustcsonglin/flash-linear-attention},
  year   = {2024}
}

@inproceedings{yang2024gla,
  title     = {Gated Linear Attention Transformers with Hardware-Efficient Training},
  author    = {Yang, Songlin and Wang, Bailin and Shen, Yikang and Panda, Rameswar and Kim, Yoon},
  booktitle = {Forty-first International Conference on Machine Learning},
  url       = {https://arxiv.org/abs/2312.06635},
  year      = {2024}
}


@InProceedings{schlag2021fwp,
  title = 	 {Linear Transformers Are Secretly Fast Weight Programmers},
  author =       {Schlag, Imanol and Irie, Kazuki and Schmidhuber, J{\"u}rgen},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  year = 	 {2021},
  url={https://arxiv.org/abs/2102.11174}, 
}

@inproceedings{yang2024deltanet,
  title     = {Parallelizing Linear Transformers with the Delta Rule over Sequence Length},
  author    = {Yang, Songlin and Wang, Bailin and Zhang, Yu and Shen, Yikang and Kim, Yoon},
  booktitle = {The Thirty-eighth Annual Conference on Neural Information Processing Systems},
  url       = {https://arxiv.org/abs/2406.06484},
  year      = {2024}
}

@misc{yang2024gateddeltanet,
      title={Gated Delta Networks: Improving Mamba2 with Delta Rule}, 
      author={Songlin Yang and Jan Kautz and Ali Hatamizadeh},
      url={https://arxiv.org/abs/2412.06464}, 
      year={2024}
}

@article{gholami2020memorywall,
  title={AI and Memory Wall},
  author={Gholami, Amir and Yao, Zhewei and Kim, Sehoon and Hooper, Coleman and Mahoney, Michael W, and Keutzer, Kurt},
  journal={IEEE Micro Journal},
  url = {https://arxiv.org/abs/2403.14123},
  year={2024}
}

@misc{cirone2024theoreticalfoundations,
      title={Theoretical Foundations of Deep Selective State-Space Models}, 
      author={Nicola Muca Cirone and Antonio Orvieto and Benjamin Walker and Cristopher Salvi and Terry Lyons},
      booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},
      year={2024},
      eprint={2402.19047},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2402.19047}, 
}

@misc{merrill2024illusionstate,
      title={The Illusion of State in State-Space Models}, 
      author={William Merrill and Jackson Petty and Ashish Sabharwal},
      booktitle={The Twelfth International Conference on Learning Representations},
      year={2024},
      url={https://arxiv.org/abs/2404.08819}, 
}

@misc{grazzi2024unlockingstate,
      title={Unlocking State-Tracking in Linear RNNs Through Negative Eigenvalues}, 
      author={Riccardo Grazzi and Julien Siems and Jörg K. H. Franke and Arber Zela and Frank Hutter and Massimiliano Pontil},
      year={2024},
      eprint={2411.12537},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2411.12537}, 
}

@misc{lim2024elk,
      title={Parallelizing non-linear sequential models over the sequence length}, 
      author={Yi Heng Lim and Qi Zhu and Joshua Selfridge and Muhammad Firmansyah Kasim},
      booktitle={The Twelfth International Conference on Learning Representations},
      year={2024},
      eprint={2309.12252},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2309.12252}, 
}

@InProceedings{gonzalez2024deer,
      title={Towards Scalable and Stable Parallelization of Nonlinear RNNs}, 
      author={Xavier Gonzalez and Andrew Warrington and Jimmy T. H. Smith and Scott W. Linderman},
      booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},
      year={2024},
      eprint={2407.19115},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2407.19115}, 
}

@misc{pöppel2024flashrnn,
      title={FlashRNN: Optimizing Traditional RNNs on Modern Hardware}, 
      author={Korbinian Pöppel and Maximilian Beck and Sepp Hochreiter},
      year={2024},
      eprint={2412.07752},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2412.07752}, 
}

@InProceedings{pascanu2012vanishing,
  title = 	 {On the difficulty of training recurrent neural networks},
  author = 	 {Pascanu, Razvan and Mikolov, Tomas and Bengio, Yoshua},
  booktitle = 	 {Proceedings of the 30th International Conference on Machine Learning},
  year = 	 {2013},
  url = 	 {https://arxiv.org/abs/1211.5063},
}

@inproceedings{zucchet2024rnnoptimization,
title={Recurrent neural networks: vanishing and exploding gradients are not the end of the story},
author={Nicolas Zucchet and Antonio Orvieto},
booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},
year={2024},
url={https://arxiv.org/abs/2405.21064}
}

