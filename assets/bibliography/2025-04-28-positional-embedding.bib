@inproceedings{46201, title={Attention is All You Need}, author={Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin}, year={2017}, URL={https://arxiv.org/pdf/1706.03762.pdf}}

@inproceedings{50650, title={An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale}, author={Alexander Kolesnikov and Alexey Dosovitskiy and Dirk Weissenborn and Georg Heigold and Jakob Uszkoreit and Lucas Beyer and Matthias Minderer and Mostafa Dehghani and Neil Houlsby and Sylvain Gelly and Thomas Unterthiner and Xiaohua Zhai}, year={2021}}

@inproceedings{46989, title={Self-Attention with Relative Position Representations}, author={Peter Shaw and Jakob Uszkoreit and Ashish Vaswani}, year={2018}}

@article{press2021train, title={Train short, test long: Attention with linear biases enables input length extrapolation}, author={Press, Ofir and Smith, Noah A and Lewis, Mike}, journal={arXiv preprint arXiv:2108.12409}, year={2021}}

@article{su2021roformer, title={RoFormer: Enhanced Transformer with Rotary Position Embedding}, author={Su, Jianlin and Lu, Yu and Pan, Shengfeng and Murtadha, Ahmed and Wen, Bo and Liu, Yunfeng}, journal={arXiv preprint arXiv:2104.09864}, year={2021}}

@article{heo2024rotary, title={Rotary position embedding for vision transformer}, author={Heo, Byeongho and Park, Song and Han, Dongyoon and Yun, Sangdoo}, journal={arXiv preprint arXiv:2403.13298}, year={2024}}

@article{fuller2024croma, title={CROMA: Remote sensing representations with contrastive radar-optical masked autoencoders}, author={Fuller, Anthony and Millard, Koreen and Green, James}, journal={Advances in Neural Information Processing Systems}, volume={36}, year={2024}}

@article{chen2023extending, title={Extending context window of large language models via positional interpolation}, author={Chen, Shouyuan and Wong, Sherman and Chen, Liangjian and Tian, Yuandong}, journal={arXiv preprint arXiv:2306.15595}, year={2023}}

@article{al2023position, title={Position Interpolation Improves ALiBi Extrapolation}, author={Al-Khateeb, Faisal and Dey, Nolan and Soboleva, Daria and Hestness, Joel}, journal={arXiv preprint arXiv:2310.13017}, year={2023}}

@inproceedings{cubuk2020randaugment, title={Randaugment: Practical automated data augmentation with a reduced search space}, author={Cubuk, Ekin D and Zoph, Barret and Shlens, Jonathon and Le, Quoc V}, booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition workshops}, pages={702--703}, year={2020}}

@inproceedings{zhang2018mixup, title={mixup: Beyond empirical risk management}, author={Zhang, H and Cisse, M and Dauphin, Y and Lopez-Paz, D}, booktitle={6th Int. Conf. Learning Representations (ICLR)}, pages={1--13}, year={2018}}

@inproceedings{yun2019cutmix, title={Cutmix: Regularization strategy to train strong classifiers with localizable features}, author={Yun, Sangdoo and Han, Dongyoon and Oh, Seong Joon and Chun, Sanghyuk and Choe, Junsuk and Yoo, Youngjoon}, booktitle={Proceedings of the IEEE/CVF international conference on computer vision}, pages={6023--6032}, year={2019}}

@inproceedings{zhong2020random, title={Random erasing data augmentation}, author={Zhong, Zhun and Zheng, Liang and Kang, Guoliang and Li, Shaozi and Yang, Yi}, booktitle={Proceedings of the AAAI conference on artificial intelligence}, volume={34}, number={07}, pages={13001--13008}, year={2020}}

@inproceedings{szegedy2016rethinking, title={Rethinking the inception architecture for computer vision}, author={Szegedy, Christian and Vanhoucke, Vincent and Ioffe, Sergey and Shlens, Jon and Wojna, Zbigniew}, booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition}, pages={2818--2826}, year={2016}}

@article{wu2023bloomberggpt, title={Bloomberggpt: A large language model for finance}, author={Wu, Shijie and Irsoy, Ozan and Lu, Steven and Dabravolski, Vadim and Dredze, Mark and Gehrmann, Sebastian and Kambadur, Prabhanjan and Rosenberg, David and Mann, Gideon}, journal={arXiv preprint arXiv:2303.17564}, year={2023}}

@article{touvron2023llama, title={Llama: Open and efficient foundation language models}, author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others}, journal={arXiv preprint arXiv:2302.13971}, year={2023}}

@article{zhang2022opt, title={Opt: Open pre-trained transformer language models}, author={Zhang, Susan and Roller, Stephen and Goyal, Naman and Artetxe, Mikel and Chen, Moya and Chen, Shuohui and Dewan, Christopher and Diab, Mona and Li, Xian and Lin, Xi Victoria and others}, journal={arXiv preprint arXiv:2205.01068}, year={2022}}

@article{devlin2018bert, title={Bert: Pre-training of deep bidirectional transformers for language understanding}, author={Devlin, Jacob}, journal={arXiv preprint arXiv:1810.04805}, year={2018}}

@article{radford2019language, title={Language models are unsupervised multitask learners}, author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others}}

@inproceedings{islam2019much, title={How much Position Information Do Convolutional Neural Networks Encode?}, author={Islam, Md Amirul and Jia, Sen and Bruce, Neil DB}, booktitle={International Conference on Learning Representations}, year={2019}}

@misc{mosaicml2023introducing, title={Introducing mpt-7b: A new standard for open-source, commercially usable llms}, author={MosaicML NLP Team and others}, year={2023}, publisher={Accessed}}

@article{le2023bloom, title={Bloom: A 176b-parameter open-access multilingual language model}, author={Le Scao, Teven and Fan, Angela and Akiki, Christopher and Pavlick, Ellie and Ili{\'c}, Suzana and Hesslow, Daniel and Castagn{\'e}, Roman and Luccioni, Alexandra Sasha and Yvon, Fran{\c{c}}ois and Gall{\'e}, Matthias and others}, year={2023}}