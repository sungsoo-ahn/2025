@article{zhao2023survey,
  title   = {A survey of large language models},
  author  = {Zhao, Wayne Xin and Zhou, Kun and Li, Junyi and Tang, Tianyi and Wang, Xiaolei and Hou, Yupeng and Min, Yingqian and Zhang, Beichen and Zhang, Junjie and Dong, Zican and others},
  journal = {arXiv preprint arXiv:2303.18223},
  year    = {2023}
}

@article{ouyang2022training,
  title   = {Training language models to follow instructions with human feedback},
  author  = {Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},
  journal = {Advances in neural information processing systems},
  volume  = {35},
  pages   = {27730--27744},
  year    = {2022}
}


@article{stiennon2020learning,
  title   = {Learning to summarize with human feedback},
  author  = {Stiennon, Nisan and Ouyang, Long and Wu, Jeffrey and Ziegler, Daniel and Lowe, Ryan and Voss, Chelsea and Radford, Alec and Amodei, Dario and Christiano, Paul F},
  journal = {Advances in Neural Information Processing Systems},
  volume  = {33},
  pages   = {3008--3021},
  year    = {2020}
}

@article{rafailov2024direct,
  title   = {Direct preference optimization: Your language model is secretly a reward model},
  author  = {Rafailov, Rafael and Sharma, Archit and Mitchell, Eric and Manning, Christopher D and Ermon, Stefano and Finn, Chelsea},
  journal = {Advances in Neural Information Processing Systems},
  volume  = {36},
  year    = {2024}
}

@article{lewis2020retrieval,
  title   = {Retrieval-augmented generation for knowledge-intensive nlp tasks},
  author  = {Lewis, Patrick and Perez, Ethan and Piktus, Aleksandra and Petroni, Fabio and Karpukhin, Vladimir and Goyal, Naman and K{\"u}ttler, Heinrich and Lewis, Mike and Yih, Wen-tau and Rockt{\"a}schel, Tim and others},
  journal = {Advances in Neural Information Processing Systems},
  volume  = {33},
  pages   = {9459--9474},
  year    = {2020}
}

@article{hu2021lora,
  title   = {Lora: Low-rank adaptation of large language models},
  author  = {Hu, Edward J and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
  journal = {arXiv preprint arXiv:2106.09685},
  year    = {2021}
}

@article{de2021editing,
  title   = {Editing factual knowledge in language models},
  author  = {De Cao, Nicola and Aziz, Wilker and Titov, Ivan},
  journal = {arXiv preprint arXiv:2104.08164},
  year    = {2021}
}

@inproceedings{yang2022vision,
  title     = {Vision-language pre-training with triple contrastive learning},
  author    = {Yang, Jinyu and Duan, Jiali and Tran, Son and Xu, Yi and Chanda, Sampath and Chen, Liqun and Zeng, Belinda and Chilimbi, Trishul and Huang, Junzhou},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages     = {15671--15680},
  year      = {2022}
}

@article{yang2023survey,
  title     = {A survey on ensemble learning under the era of deep learning},
  author    = {Yang, Yongquan and Lv, Haijun and Chen, Ning},
  journal   = {Artificial Intelligence Review},
  volume    = {56},
  number    = {6},
  pages     = {5545--5589},
  year      = {2023},
  publisher = {Springer}
}

@article{kumar2024scaling,
  title   = {Scaling Laws for Precision},
  author  = {Kumar, Tanishq and Ankner, Zachary and Spector, Benjamin F and Bordelon, Blake and Muennighoff, Niklas and Paul, Mansheej and Pehlevan, Cengiz and R{\'e}, Christopher and Raghunathan, Aditi},
  journal = {arXiv preprint arXiv:2411.04330},
  year    = {2024}
}

@article{Gui2023ASO,
  title   = {A Survey on Self-Supervised Learning: Algorithms, Applications, and Future Trends},
  author  = {Jie Gui and Tuo Chen and Jing Zhang and Qiong Cao and Zhe Sun and Haoran Luo and Dacheng Tao},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year    = {2023},
  volume  = {46},
  pages   = {9052-9071},
  url     = {https://api.semanticscholar.org/CorpusID:261046875}
}

@inproceedings{Vaswani2017AttentionIA,
  title     = {Attention is All you Need},
  author    = {Ashish Vaswani and Noam M. Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
  booktitle = {Neural Information Processing Systems},
  year      = {2017},
  url       = {https://api.semanticscholar.org/CorpusID:13756489}
}

@article{An2024TrainingFreeLS,
  title   = {Training-Free Long-Context Scaling of Large Language Models},
  author  = {Chen An and Fei Huang and Jun Zhang and Shansan Gong and Xipeng Qiu and Chang Zhou and Lingpeng Kong},
  journal = {ArXiv},
  year    = {2024},
  volume  = {abs/2402.17463},
  url     = {https://api.semanticscholar.org/CorpusID:268032518}
}

@inproceedings{hqtd,
  author    = {Yu, Xiao and Zhang, Zexian and Niu, Feifei and Hu, Xing and Xia, Xin and Grundy, John},
  title     = {What Makes a High-Quality Training Dataset for Large Language Models: A Practitioners' Perspective},
  year      = {2024},
  isbn      = {9798400712487},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3691620.3695061},
  doi       = {10.1145/3691620.3695061},
  abstract  = {Large Language Models (LLMs) have demonstrated remarkable performance in various application domains, largely due to their self-supervised pre-training on extensive high-quality text datasets. However, despite the importance of constructing such datasets, many leading LLMs lack documentation of their dataset construction and training procedures, leaving LLM practitioners with a limited understanding of what makes a high-quality training dataset for LLMs. To fill this gap, we initially identified 18 characteristics of high-quality LLM training datasets, as well as 10 potential data pre-processing methods and 6 data quality assessment methods, through detailed interviews with 13 experienced LLM professionals. We then surveyed 219 LLM practitioners from 23 countries across 5 continents. We asked our survey respondents to rate the importance of these characteristics, provide a rationale for their ratings, specify the key data pre-processing and data quality assessment methods they used, and highlight the challenges encountered during these processes. From our analysis, we identified 13 crucial characteristics of high-quality LLM datasets that receive a high rating, accompanied by key rationale provided by respondents. We also identified some widely-used data pre-processing and data quality assessment methods, along with 7 challenges encountered during these processes. Based on our findings, we discuss the implications for researchers and practitioners aiming to construct high-quality training datasets for optimizing LLMs.},
  booktitle = {Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering},
  pages     = {656â€“668},
  numpages  = {13},
  keywords  = {large language models, high-quality data, practitioners' perspective, empirical study},
  location  = {Sacramento, CA, USA},
  series    = {ASE '24}
}

@article{Adler2024Nemotron43T,
  title   = {Nemotron-4 340B Technical Report},
  author  = {Nvidia Bo Adler and Niket Agarwal and Ashwath Aithal and Dong H. Anh and Pallab Bhattacharya and Annika Brundyn and Jared Casper and Bryan Catanzaro and Sharon Clay and Jonathan Cohen and Sirshak Das and Ayush Dattagupta and Olivier Delalleau and Leon Derczynski and Yi Dong and Daniel Egert and Ellie Evans and Aleksander Ficek and Denys Fridman and Shaona Ghosh and Boris Ginsburg and Igor Gitman and Tomasz Grzegorzek and Robert Hero and Jining Huang and Vibhu Jawa and Joseph Jennings and Aastha Jhunjhunwala and John Kamalu and Sadaf Khan and Oleksii Kuchaiev and Patrick LeGresley and Hui Li and Jiwei Liu and Zihan Liu and Eileen Peters Long and Ameya Mahabaleshwarkar and Somshubra Majumdar and James Maki and Miguel Martinez and Maer Rodrigues de Melo and Ivan Moshkov and Deepak Narayanan and Sean Narenthiran and Jesus Navarro and Phong Nguyen and Osvald Nitski and Vahid Noroozi and Guruprasad Nutheti and Christopher Parisien and Jupinder Parmar and Mostofa Patwary and Krzysztof Pawelec and Wei Ping and Shrimai Prabhumoye and Rajarshi Roy and Trisha Saar and Vasanth Rao Naik Sabavat and Sanjeev Satheesh and Jane Polak Scowcroft and Jason D. Sewall and Pavel Shamis and Gerald Shen and Mohammad Shoeybi and Dave Sizer and Misha Smelyanskiy and Felipe Soares and Makesh Narsimhan Sreedhar and Dan Su and Sandeep Subramanian and Shengyang Sun and Shubham Toshniwal and Hao Wang and Zhilin Wang and Jiaxuan You and Jiaqi Zeng and Jimmy Zhang and Jing Zhang and Vivienne Zhang and Yian Zhang and Chen Zhu},
  journal = {ArXiv},
  year    = {2024},
  volume  = {abs/2406.11704},
  url     = {https://api.semanticscholar.org/CorpusID:270493785}
}

@misc{o1_model_analysis,
  author       = {Artificial Analysis},
  title        = {OpenAI o1 Model Analysis},
  howpublished = {\url{https://artificialanalysis.ai/models/o1}},
  year         = {2024},
  note         = {Accessed: 2024-11-19}
}

@article{kumar2024scaling,
  title   = {Scaling Laws for Precision},
  author  = {Kumar, Tanishq and Ankner, Zachary and Spector, Benjamin F and Bordelon, Blake and Muennighoff, Niklas and Paul, Mansheej and Pehlevan, Cengiz and R{\'e}, Christopher and Raghunathan, Aditi},
  journal = {arXiv preprint arXiv:2411.04330},
  year    = {2024}
}
